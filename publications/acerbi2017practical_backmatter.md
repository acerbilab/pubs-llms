# Practical Bayesian Optimization for Model Fitting with Bayesian Adaptive Direct Search - Backmatter

---

#### Page 10

# Acknowledgments 

We thank Will Adler, Robbe Goris, Andra Mihali, Bas van Opheusden, and Aspen Yoo for sharing data and model evaluation code that we used in the CCN17 benchmark set; Maija Honig, Andra Mihali, Bas van Opheusden, and Aspen Yoo for providing user feedback on earlier versions of the badx package for MATLAB; Will Adler, Andra Mihali, Bas van Opheusden, and Aspen Yoo for helpful feedback on a previous version of this manuscript; John Wixted and colleagues for allowing us to reuse their data for the CCN17 'word recognition memory' problem set; and three anonymous reviewers for useful feedback. This work has utilized the NYU IT High Performance Computing resources and services.

## References

[1] Rios, L. M. \& Sahinidis, N. V. (2013) Derivative-free optimization: A review of algorithms and comparison of software implementations. Journal of Global Optimization 56, 1247-1293.
[2] Jones, D. R., Schonlau, M., \& Welch, W. J. (1998) Efficient global optimization of expensive black-box functions. Journal of Global Optimization 13, 455-492.
[3] Brochu, E., Cora, V. M., \& De Freitas, N. (2010) A tutorial on Bayesian optimization of expensive cost functions, with application to active user modeling and hierarchical reinforcement learning. arXiv preprint arXiv:1012.2599.
[4] Shahriari, B., Swersky, K., Wang, Z., Adams, R. P., \& de Freitas, N. (2016) Taking the human out of the loop: A review of Bayesian optimization. Proceedings of the IEEE 104, 148-175.
[5] Snoek, J., Larochelle, H., \& Adams, R. P. (2012) Practical Bayesian optimization of machine learning algorithms. Advances in Neural Information Processing Systems 24, 2951-2959.
[6] Audet, C. \& Dennis Jr, J. E. (2006) Mesh adaptive direct search algorithms for constrained optimization. SIAM Journal on optimization 17, 188-217.
[7] Taddy, M. A., Lee, H. K., Gray, G. A., \& Griffin, J. D. (2009) Bayesian guided pattern search for robust local optimization. Technometrics 51, 389-401.
[8] Picheny, V. \& Ginsbourger, D. (2014) Noisy kriging-based optimization methods: A unified implementation within the DiceOptim package. Computational Statistics \& Data Analysis 71, 1035-1053.
[9] Gramacy, R. B. \& Le Digabel, S. (2015) The mesh adaptive direct search algorithm with treed Gaussian process surrogates. Pacific Journal of Optimization 11, 419-447.
[10] Hutter, F., Hoos, H. H., \& Leyton-Brown, K. (2011) Sequential model-based optimization for general algorithm configuration. LION 5, 507-523.
[11] Bergstra, J. S., Bardenet, R., Bengio, Y., \& Kégl, B. (2011) Algorithms for hyper-parameter optimization. pp. 2546-2554.
[12] Talgorn, B., Le Digabel, S., \& Kokkolaras, M. (2015) Statistical surrogate formulations for simulationbased design optimization. Journal of Mechanical Design 137, 021405-1-021405-18.
[13] Audet, C., Custódio, A., \& Dennis Jr, J. E. (2008) Erratum: Mesh adaptive direct search algorithms for constrained optimization. SIAM Journal on Optimization 18, 1501-1503.
[14] Clarke, F. H. (1983) Optimization and Nonsmooth Analysis. (John Wiley \& Sons, New York).
[15] Rasmussen, C. \& Williams, C. K. I. (2006) Gaussian Processes for Machine Learning. (MIT Press).
[16] Gramacy, R. B. \& Lee, H. K. (2012) Cases for the nugget in modeling computer experiments. Statistics and Computing 22, 713-722.
[17] Srinivas, N., Krause, A., Seeger, M., \& Kakade, S. M. (2010) Gaussian process optimization in the bandit setting: No regret and experimental design. ICML-10 pp. 1015-1022.
[18] Mockus, J., Tiesis, V., \& Zilinskas, A. (1978) in Towards Global Optimisation. (North-Holland Amsterdam), pp. 117-129.
[19] Kushner, H. J. (1964) A new method of locating the maximum point of an arbitrary multipeak curve in the presence of noise. Journal of Basic Engineering 86, 97-106.
[20] Bratley, P. \& Fox, B. L. (1988) Algorithm 659: Implementing Sobol's quasirandom sequence generator. ACM Transactions on Mathematical Software (TOMS) 14, 88-100.
[21] Hansen, N., Müller, S. D., \& Koumoutsakos, P. (2003) Reducing the time complexity of the derandomized evolution strategy with covariance matrix adaptation (CMA-ES). Evolutionary Computation 11, 1-18.
[22] Hoffman, M. D., Brochu, E., \& de Freitas, N. (2011) Portfolio allocation for Bayesian optimization. Proceedings of the Twenty-Seventh Conference on Uncertainty in Artificial Intelligence pp. 327-336.

---

#### Page 11

[23] Picheny, V., Ginsbourger, D., Richet, Y., \& Caplin, G. (2013) Quantile-based optimization of noisy computer experiments with tunable precision. Technometrics 55, 2-13.
[24] Picheny, V., Wagner, T., \& Ginsbourger, D. (2013) A benchmark of kriging-based infill criteria for noisy optimization. Structural and Multidisciplinary Optimization 48, 607-626.
[25] Lagarias, J. C., Reeds, J. A., Wright, M. H., \& Wright, P. E. (1998) Convergence properties of the Nelder-Mead simplex method in low dimensions. SIAM Journal on Optimization 9, 112-147.
[26] Waltz, R. A., Morales, J. L., Nocedal, J., \& Orban, D. (2006) An interior algorithm for nonlinear optimization that combines line search and trust region steps. Mathematical Programming 107, 391-408.
[27] Nocedal, J. \& Wright, S. (2006) Numerical Optimization, Springer Series in Operations Research. (Springer Verlag), 2nd edition.
[28] Gill, P. E., Murray, W., \& Wright, M. H. (1981) Practical Optimization. (Academic press).
[29] Goldberg, D. E. (1989) Genetic Algorithms in Search, Optimization \& Machine Learning. (AddisonWesley).
[30] Bergstra, J. \& Bengio, Y. (2012) Random search for hyper-parameter optimization. Journal of Machine Learning Research 13, 281-305.
[31] Huyer, W. \& Neumaier, A. (1999) Global optimization by multilevel coordinate search. Journal of Global Optimization 14, 331-355.
[32] Jastrebski, G. A. \& Arnold, D. V. (2006) Improving evolution strategies through active covariance matrix adaptation. IEEE Congress on Evolutionary Computation (CEC 2006). pp. 2814-2821.
[33] Csendes, T., Pál, L., Sendin, J. O. H., \& Banga, J. R. (2008) The GLOBAL optimization method revisited. Optimization Letters 2, 445-454.
[34] Hansen, N., Niederberger, A. S., Guzzella, L., \& Koumoutsakos, P. (2009) A method for handling uncertainty in evolutionary optimization with an application to feedback control of combustion. IEEE Transactions on Evolutionary Computation 13, 180-197.
[35] Hansen, N., Finck, S., Ros, R., \& Auger, A. (2009) Real-parameter black-box optimization benchmarking 2009: Noiseless functions definitions.
[36] Acerbi, L., Dokka, K., Angelaki, D. E., \& Ma, W. J. (2017) Bayesian comparison of explicit and implicit causal inference strategies in multisensory heading perception. bioRxiv preprint bioRxiv:150052.
[37] Adler, W. T. \& Ma, W. J. (2017) Human confidence reports account for sensory uncertainty but in a non-Bayesian way. bioRxiv preprint bioRxiv:093203.
[38] Goris, R. L., Simoncelli, E. P., \& Movshon, J. A. (2015) Origin and function of tuning diversity in macaque visual cortex. Neuron 88, 819-831.
[39] Körding, K. P., Beierholm, U., Ma, W. J., Quartz, S., Tenenbaum, J. B., \& Shams, L. (2007) Causal inference in multisensory perception. PLoS One 2, e943.
[40] van den Berg, R., Yoo, A. H., \& Ma, W. J. (2017) Fechner's law in metacognition: A quantitative model of visual working memory confidence. Psychological Review 124, 197-214.
[41] Mickes, L., Wixted, J. T., \& Wais, P. E. (2007) A direct test of the unequal-variance signal detection model of recognition memory. Psychonomic Bulletin \& Review 14, 858-865.
[42] Shiffrin, R. M. \& Steyvers, M. (1997) A model for recognition memory: REM-retrieving effectively from memory. Psychonomic Bulletin \& Review 4, 145-166.
[43] Ma, W. J., Navalpakkam, V., Beck, J. M., van Den Berg, R., \& Pouget, A. (2011) Behavior and neural basis of near-optimal visual search. Nature Neuroscience 14, 783-790.
[44] Mazyar, H., van den Berg, R., \& Ma, W. J. (2012) Does precision decrease with set size? J Vis 12, 1-10.
[45] van den Berg, R., Shin, H., Chou, W.-C., George, R., \& Ma, W. J. (2012) Variability in encoding precision accounts for visual short-term memory limitations. Proc Natl Acad Sci U S A 109, 8780-8785.
[46] van Opheusden, B., Bnaya, Z., Galbiati, G., \& Ma, W. J. (2016) Do people think like computers? International Conference on Computers and Games pp. 212-224.
[47] Snoek, J., Swersky, K., Zemel, R., \& Adams, R. (2014) Input warping for Bayesian optimization of non-stationary functions. pp. 1674-1682.
[48] Martinez-Cantin, R. (2014) BayesOpt: A Bayesian optimization library for nonlinear optimization, experimental design and bandits. Journal of Machine Learning Research 15, 3735-3739.
[49] Hennig, P., Osborne, M. A., \& Girolami, M. (2015) Probabilistic numerics and uncertainty in computations. Proceedings of the Royal Society A 471, 20150142.
[50] Quiñonero Candela, J., Rasmussen, C. E., \& Williams, C. K. (2007) Approximation methods for Gaussian process regression. Large-scale kernel machines pp. 203-224.

---

#### Page 12

[51] Royston, J. (1982) An extension of Shapiro and Wilk's W test for normality to large samples. Applied Statistics pp. 115-124.
[52] Lizotte, D. J. (2008) Ph.D. thesis (University of Alberta).
[53] Huyer, W. \& Neumaier, A. (2008) SNOBFIT-stable noisy optimization by branch and fit. ACM Transactions on Mathematical Software (TOMS) 35, 9.
[54] Kolda, T. G., Lewis, R. M., \& Torczon, V. (2003) Optimization by direct search: New perspectives on some classical and modern methods. SIAM Review 45, 385-482.
[55] Eberhart, R. \& Kennedy, J. (1995) A new optimizer using particle swarm theory. Proceedings of the Sixth International Symposium on Micro Machine and Human Science, 1995 (MHS'95). pp. 39-43.
[56] Kirkpatrick, S., Gelatt, C. D., Vecchi, M. P., et al. (1983) Optimization by simulated annealing. Science 220, 671-680.
[57] Liang, J., Qu, B., \& Suganthan, P. (2013) Problem definitions and evaluation criteria for the CEC 2014 special session and competition on single objective real-parameter numerical optimization.
[58] Rasmussen, C. E. \& Nickisch, H. (2010) Gaussian processes for machine learning (GPML) toolbox. Journal of Machine Learning Research 11, 3011-3015.