# Learning Robust Statistics for Simulation-based Inference under Model Misspecification - Backmatter

---

# Acknowledgements

The authors would like to thank Dr. Carl Gustafson and Prof. Fredrik Tufvesson (Lund University) for providing the measurement data. We also thank Masha Naslidnyk, Dr. François-Xavier Briol, and Dr. Markus Heinonen for their useful comments and discussion. We acknowledge the computational resources provided by the Aalto Science-IT Project from Computer Science IT. This work was supported by the Academy of Finland Flagship programme: Finnish Center for Artificial Intelligence FCAI. SK was supported by the UKRI Turing AI World-Leading Researcher Fellowship, [EP/W002973/1].

## References

[1] Albert, C., Ulzega, S., Ozdemir, F., Perez-Cruz, F., and Mira, A. (2022). Learning Summary Statistics for Bayesian Inference with Autoencoders. SciPost Phys. Core, 5:043.
[2] Alquier, P. and Gerber, M. (2021). Universal robust regression via maximum mean discrepancy. arXiv:2006.00840.
[3] Beaumont, M. A. (2010). Approximate Bayesian computation in evolution and ecology. Annual Review of Ecology, Evolution, and Systematics, 41(1):379-406.
[4] Beaumont, M. A. (2019). Approximate Bayesian computation. Annual Review of Statistics and Its Application, 6(1):379-403.
[5] Beaumont, M. A., Zhang, W., and Balding, D. J. (2002). Approximate Bayesian computation in population genetics. Genetics, 162(4):2025-2035.
[6] Berlinet, A. and Thomas-Agnan, C. (2004). Reproducing Kernel Hilbert Spaces in Probability and Statistics. Springer Science+Business Media, New York.
[7] Bernardo, S. (2000). Bayesian Theory. John Wiley \& Sons.
[8] Bharti, A., Briol, F.-X., and Pedersen, T. (2022a). A general method for calibrating stochastic radio channel models with kernels. IEEE Transactions on Antennas and Propagation, 70(6):39864001.
[9] Bharti, A., Filstroff, L., and Kaski, S. (2022b). Approximate Bayesian computation with domain expert in the loop. In International Conference on Machine Learning, volume 162, pages 18931905.
[10] Bharti, A., Naslidnyk, M., Key, O., Kaski, S., and Briol, F.-X. (2023). Optimally-weighted estimators of the maximum mean discrepancy for likelihood-free inference. arXiv:2301.11674.
[11] Bi, J., Shen, W., and Zhu, W. (2021). Random forest adjustment for approximate Bayesian computation. Journal of Computational and Graphical Statistics, 0(0):1-10.
[12] Bissiri, P. G., Holmes, C. C., and Walker, S. G. (2016). A general framework for updating belief distributions. Journal of the Royal Statistical Society. Series B (Statistical Methodology), 78(5):1103-1130.
[13] Blum, M. G. B. and François, O. (2010). Non-linear regression models for approximate Bayesian computation. Statistics and Computing, 20(1):63-73.

---

#### Page 11

[14] Blum, M. G. B., Nunes, M. A., Prangle, D., and Sisson, S. A. (2013). A comparative review of dimension reduction methods in approximate Bayesian computation. Statistical Science, 28(2).
[15] Briol, F.-X., Barp, A., Duncan, A. B., and Girolami, M. (2019). Statistical inference for generative models with maximum mean discrepancy. arXiv:1906.05944.
[16] Cannon, P., Ward, D., and Schmon, S. M. (2022). Investigating the impact of model misspecification in neural simulation-based inference. arXiv preprint arXiv:2209.01845.
[17] Chan, J., Perrone, V., Spence, J., Jenkins, P., Mathieson, S., and Song, Y. (2018). A likelihoodfree inference framework for population genetic data using exchangeable neural networks. In Advances in Neural Information Processing Systems, volume 31.
[18] Chen, Y., Zhang, D., Gutmann, M. U., Courville, A., and Zhu, Z. (2021). Neural approximate sufficient statistics for implicit models. In International Conference on Learning Representations.
[19] Chérief-Abdellatif, B.-E. and Alquier, P. (2020). MMD-Bayes: Robust Bayesian estimation via maximum mean discrepancy. In Proceesings of the 2nd Symposium on Advances in Approximate Bayesian Inference, pages 1-21.
[20] Chérief-Abdellatif, B.-E. and Alquier, P. (2021). Finite sample properties of parametric MMD estimation: robustness to misspecification and dependence. arXiv:1912.05737.
[21] Cranmer, K., Brehmer, J., and Louppe, G. (2020). The frontier of simulation-based inference. Proceedings of the National Academy of Sciences, 117(48):30055-30062.
[22] Dax, M., Green, S. R., Gair, J., Deistler, M., Schölkopf, B., and Macke, J. H. (2022). Group equivariant neural posterior estimation. In International Conference on Learning Representations.
[23] Delaunoy, A., Hermans, J., Rozet, F., Wehenkel, A., and Louppe, G. (2022). Towards reliable simulation-based inference with balanced neural ratio estimation. arXiv preprint arXiv:2208.13624.
[24] Dellaporta, C., Knoblauch, J., Damoulas, T., and Briol, F.-X. (2022). Robust bayesian inference for simulator-based models via the mmd posterior bootstrap. In International Conference on Artificial Intelligence and Statistics, volume 151, pages 943-970.
[25] Diggle, P. J. and Gratton, R. J. (1984). Monte carlo methods of inference for implicit statistical models. Journal of the Royal Statistical Society: Series B (Methodological), 46(2):193-212.
[26] Dinev, T. and Gutmann, M. U. (2018). Dynamic likelihood-free inference via ratio estimation (DIRE). arXiv preprint arXiv:1810.09899.
[27] Dingeldein, L., Cossio, P., and Covino, R. (2023). Simulation-based inference of singlemolecule force spectroscopy. Machine Learning: Science and Technology, 4(2):025009.
[28] Durkan, C., Murray, I., and Papamakarios, G. (2020). On contrastive learning for likelihood-free inference. In International Conference on Machine Learning, volume 119, pages 2771-2781.
[29] Dyer, J., Cannon, P., Farmer, J. D., and Schmon, S. (2022a). Black-box bayesian inference for economic agent-based models. arXiv preprint arXiv:2202.00625.
[30] Dyer, J., Cannon, P. W., and Schmon, S. M. (2022b). Amortised likelihood-free inference for expensive time-series simulators with signatured ratio estimation. In International Conference on Artificial Intelligence and Statistics, volume 151, pages 11131-11144.
[31] Fearnhead, P. and Prangle, D. (2012). Constructing summary statistics for approximate Bayesian computation: semi-automatic approximate bayesian computation. Journal of the Royal Statistical Society. Series B (Statistical Methodology), 74(3):419-474.
[32] Frazier, D. T. and Drovandi, C. (2021). Robust approximate bayesian inference with synthetic likelihood. Journal of Computational and Graphical Statistics, 30(4):958-976.
[33] Frazier, D. T., Drovandi, C., and Loaiza-Maya, R. (2020a). Robust approximate bayesian computation: An adjustment approach. arXiv preprint arXiv:2008.04099.

---

#### Page 12

[34] Frazier, D. T., Drovandi, C., and Nott, D. J. (2021). Synthetic likelihood in misspecified models: Consequences and corrections. arXiv preprint arXiv:2104.03436.
[35] Frazier, D. T., Robert, C. P., and Rousseau, J. (2020b). Model misspecification in approximate bayesian computation: consequences and diagnostics. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 82(2):421-444.
[36] Fujisawa, M., Teshima, T., Sato, I., and Sugiyama, M. (2021). $\gamma$-abc: Outlier-robust approximate bayesian computation based on a robust divergence estimator. In International Conference on Artificial Intelligence and Statistics, volume 130, pages 1783-1791.
[37] Furia, C. S. and Churchill, R. M. (2022). Normalizing flows for likelihood-free inference with fusion simulations. Plasma Physics and Controlled Fusion, 64(10):104003.
[38] Geffner, T., Papamakarios, G., and Mnih, A. (2022). Score modeling for simulation-based inference. arXiv preprint arXiv:2209.14249.
[39] Glöckler, M., Deistler, M., and Macke, J. H. (2022). Variational methods for simulation-based inference. In International Conference on Learning Representations.
[40] Green, S. R. and Gair, J. (2021). Complete parameter inference for GW150914 using deep learning. Machine Learning: Science and Technology, 2(3):03LT01.
[41] Greenberg, D., Nonnenmacher, M., and Macke, J. (2019). Automatic posterior transformation for likelihood-free inference. In International Conference on Machine Learning, volume 97, pages $2404-2414$.
[42] Gretton, A., Borgwardt, K., Rasch, M. J., and Scholkopf, B. (2012). A kernel two-sample test. Journal of Machine Learning Research, 13:723-773.
[43] Grünwald, P. (2012). The Safe Bayesian. In Lecture Notes in Computer Science, pages 169-183. Springer Berlin Heidelberg.
[44] Gustafson, C., Bolin, D., and Tufvesson, F. (2016). Modeling the polarimetric mm-wave propagation channel using censored measurements. In 2016 Global Communications Conference. IEEE.
[45] Haneda, K., Järveläinen, J., Karttunen, A., Kyrö, M., and Putkonen, J. (2015). A statistical spatio-temporal radio channel model for large indoor environments at 60 and 70 ghz. IEEE Transactions on Antennas and Propagation, 63(6):2694-2704.
[46] Hermans, J., Begy, V., and Louppe, G. (2020). Likelihood-free MCMC with amortized approximate ratio estimators. In International Conference on Machine Learning, volume 119, pages 4239-4248.
[47] Jiang, B., yu Wu, T., Zheng, C., and Wong, W. (2017). Learning summary statistic for approximate Bayesian computation via deep neural network. Statistica Sinica, page 1595-1618.
[48] Kelly, R. P., Nott, D. J., Frazier, D. T., Warne, D. J., and Drovandi, C. (2023). Misspecificationrobust sequential neural likelihood. arXiv preprint arXiv:2301.13368.
[49] Knoblauch, J., Jewson, J., and Damoulas, T. (2019). Generalized variational inference: Three arguments for deriving new posteriors. arXiv:1904.02063.
[50] Legramanti, S., Durante, D., and Alquier, P. (2022). Concentration and robustness of discrepancy-based ABC via Rademacher complexity. arXiv:2206.06991.
[51] Lintusaari, J., Gutmann, M. U., Dutta, R., Kaski, S., and Corander, J. (2017). Fundamentals and recent developments in approximate Bayesian computation. Systematic Biology, 66:66-82.
[52] Lueckmann, J.-M., Bassetto, G., Karaletsos, T., and Macke, J. H. (2019). Likelihood-free inference with emulator networks. In Proceedings of The 1st Symposium on Advances in Approximate Bayesian Inference, volume 96, pages 32-53.

---

#### Page 13

[53] Lueckmann, J.-M., Goncalves, P. J., Bassetto, G., Öcal, K., Nonnenmacher, M., and Macke, J. H. (2017). Flexible statistical inference for mechanistic models of neural dynamics. In Advances in Neural Information Processing Systems, volume 30.
[54] Mitrovic, J., Sejdinovic, D., and Teh, Y.-W. (2016). DR-ABC: Approximate Bayesian computation with kernel-based distribution regression. In Proceedings of the International Conference on Machine Learning, pages 1482-1491.
[55] Muandet, K., Fukumizu, K., Sriperumbudur, B., and Schölkopf, B. (2017). Kernel mean embedding of distributions: A review and beyond. Foundations and Trends ${ }^{\circledR}$ in Machine Learning, 10(1-2):1-141.
[56] Niu, Z., Meier, J., and Briol, F.-X. (2021). Discrepancy-based inference for intractable generative models using quasi-Monte Carlo. arXiv:2106.11561.
[57] Pacchiardi, L. and Dutta, R. (2021). Generalized bayesian likelihood-free inference using scoring rules estimators. arXiv preprint arXiv:2104.03889.
[58] Pacchiardi, L. and Dutta, R. (2022). Likelihood-free inference with generative neural networks via scoring rule minimization. arXiv preprint arXiv:2205.15784.
[59] Papamakarios, G. and Murray, I. (2016). Fast $\epsilon$-free inference of simulation models with bayesian conditional density estimation. In International Conference on Neural Information Processing Systems, page 1036-1044.
[60] Papamakarios, G., Pavlakou, T., and Murray, I. (2017). Masked autoregressive flow for density estimation. Advances in neural information processing systems, 30.
[61] Papamakarios, G., Sterratt, D., and Murray, I. (2019). Sequential neural likelihood: Fast likelihood-free inference with autoregressive flows. In International Conference on Artificial Intelligence and Statistics, volume 89, pages 837-848.
[62] Park, M., Jitkrittum, W., and Sejdinovic, D. (2015). K2-ABC: approximate Bayesian computation with kernel embeddings. International Conference on Artificial Intelligence and Statistics, $51: 398-407$.
[63] Pedersen, T. (2019). Stochastic multipath model for the in-room radio channel based on room electromagnetics. IEEE Transactions on Antennas and Propagation, 67(4):2591-2603.
[64] Pesonen, H., Simola, U., Köhn-Luque, A., Vuollekoski, H., Lai, X., Frigessi, A., Kaski, S., Frazier, D. T., Maneesoonthorn, W., Martin, G. M., and Corander, J. (2022). ABC of the future. International Statistical Review.
[65] Peter J. Huber, E. M. R. (2009). Robust Statistics. WILEY.
[66] Pomponi, J., Scardapane, S., and Uncini, A. (2021). Bayesian neural networks with maximum mean discrepancy regularization. Neurocomputing, 453:428-437.
[67] Price, L. F., Drovandi, C. C., Lee, A., and Nott, D. J. (2018). Bayesian synthetic likelihood. Journal of Computational and Graphical Statistics, 27(1):1-11.
[68] Pudlo, P., Marin, J.-M., Estoup, A., Cornuet, J.-M., Gautier, M., and Robert, C. P. (2015). Reliable ABC model choice via random forests. Bioinformatics, 32(6):859-866.
[69] Radev, S. T., Mertens, U. K., Voss, A., Ardizzone, L., and Köthe, U. (2022). Bayesflow: Learning complex stochastic models with invertible neural networks. IEEE Transactions on Neural Networks and Learning Systems, 33(4):1452-1466.
[70] Ramesh, P., Lueckmann, J.-M., Boelts, J., Tejero-Cantero, Á., Greenberg, D. S., Goncalves, P. J., and Macke, J. H. (2022). GATSBI: Generative adversarial training for simulation-based inference. In International Conference on Learning Representations.
[71] Riesselman, A. J., Ingraham, J. B., and Marks, D. S. (2018). Deep generative models of genetic variation capture the effects of mutations. Nature Methods, 15(10):816-822.

---

#### Page 14

[72] Rothfuss, J., Ferreira, F., Walther, S., and Ulrich, M. (2019). Conditional density estimation with neural networks: Best practices and benchmarks. arXiv preprint arXiv:1903.00954.
[73] Schmitt, M., Bürkner, P.-C., Köthe, U., and Radev, S. T. (2021). Detecting model misspecification in amortized bayesian inference with neural networks. arXiv e-prints.
[74] Schmon, S. M., Cannon, P. W., and Knoblauch, J. (2020). Generalized posteriors in approximate bayesian computation. arXiv preprint arXiv:2011.08644.
[75] Sharrock, L., Simons, J., Liu, S., and Beaumont, M. (2022). Sequential neural score estimation: Likelihood-free inference with conditional score based diffusion models. arXiv preprint arXiv:2210.04872.
[76] Sisson, S. A. (2018). Handbook of Approximate Bayesian Computation. Chapman and Hall/CRC.
[77] Thomas, O., Dutta, R., Corander, J., Kaski, S., and Gutmann, M. U. (2022). Likelihood-free inference by ratio estimation. Bayesian Analysis, 17(1).
[78] Turin, G. L., Clapp, F. D., Johnston, T. L., Fine, S. B., and Lavry, D. (1972). A statistical model of urban multipath propagation. IEEE Transactions on Vehicular Technology, 21(1):1-9.
[79] van Opheusden, B., Acerbi, L., and Ma, W. J. (2020). Unbiased and efficient log-likelihood estimation with inverse binomial sampling. PLoS Computational Biology, 16(12):e1008483.
[80] Vasist, M., Rozet, F., Absil, O., Mollière, P., Nasedkin, E., and Louppe, G. (2023). Neural posterior estimation for exoplanetary atmospheric retrieval. Astronomy \& Astrophysics, 672:A147.
[81] Ward, D., Cannon, P., Beaumont, M., Fasiolo, M., and Schmon, S. M. (2022). Robust neural posterior estimation and statistical model criticism. In Advances in Neural Information Processing Systems.
[82] Weinstein, E. N. and Miller, J. W. (2023). Bayesian data selection. Journal of Machine Learning Research, 24(23):1-72.
[83] Wiqvist, S., Frellsen, J., and Picchini, U. (2021). Sequential neural posterior and likelihood approximation. arXiv preprint arXiv:2102.06522.
[84] Wiqvist, S., Mattei, P.-A., Picchini, U., and Frellsen, J. (2019). Partially exchangeable networks and architectures for learning summary statistics in approximate Bayesian computation. In International Conference on Machine Learning, volume 97, pages 6798-6807.
[85] Wood, S. N. (2010). Statistical inference for noisy nonlinear ecological dynamic systems. Nature, 466(7310):1102-1104.
[86] Zaheer, M., Kottur, S., Ravanbakhsh, S., Poczos, B., Salakhutdinov, R. R., and Smola, A. J. (2017). Deep sets. Advances in neural information processing systems, 30.
[87] Zbair, M., Qaffou, A., and Hilal, K. (2022). Approximate bayesian estimation of parameters of an agent-based model in epidemiology. In Lecture Notes in Networks and Systems, pages 302-314. Springer International Publishing.
