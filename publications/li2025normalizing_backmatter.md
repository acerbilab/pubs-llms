# Normalizing Flow Regression for Bayesian Inference with Offline Likelihood Evaluations - Backmatter

---

#### Page 13

# Acknowledgments 

This work was supported by Research Council of Finland (grants 358980 and 356498), and by the Flagship programme: Finnish Center for Artificial Intelligence FCAI. The authors wish to thank the Finnish Computing Competence Infrastructure (FCCI) for supporting this project with computational and data storage resources.

## References

Luigi Acerbi. Variational Bayesian Monte Carlo. Advances in Neural Information Processing Systems, 31:8222-8232, 2018.

Luigi Acerbi. An exploration of acquisition and mean functions in Variational Bayesian Monte Carlo. Proceedings of The 1st Symposium on Advances in Approximate Bayesian Inference (PMLR), 96:1-10, 2019.

Luigi Acerbi. Variational Bayesian Monte Carlo with noisy likelihoods. Advances in Neural Information Processing Systems, 33:8211-8222, 2020.

Luigi Acerbi and Wei Ji Ma. Practical Bayesian optimization for model fitting with Bayesian adaptive direct search. Advances in Neural Information Processing Systems, 30:18341844, 2017.

Luigi Acerbi, Daniel M Wolpert, and Sethu Vijayakumar. Internal representations of temporal statistics and feedback calibrate motor-sensory interval timing. PLoS Computational Biology, 8(11):e1002771, 2012.

Luigi Acerbi, Kalpana Dokka, Dora E Angelaki, and Wei Ji Ma. Bayesian comparison of explicit and implicit causal inference strategies in multisensory heading perception. PLoS Computational Biology, 14(7):e1006110, 2018.

Masaki Adachi, Satoshi Hayakawa, Martin Jørgensen, Harald Oberhauser, and Michael A Osborne. Fast Bayesian inference with batch Bayesian quadrature via kernel recombination. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems, volume 35, pages 16533-16547. Curran Associates, Inc., 2022.

Abhinav Agrawal, Daniel R Sheldon, and Justin Domke. Advances in black-box VI: Normalizing flows, importance weighting, and optimization. In Advances in Neural Information Processing Systems, volume 33, pages 17358-17369. Curran Associates, Inc., 2020.

Takeshi Amemiya. Tobit models: A survey. Journal of Econometrics, 24(1):3-61, January 1984. ISSN 0304-4076. doi: 10.1016/0304-4076(84)90074-5.

Saba Amiri, Eric Nalisnick, Adam Belloum, Sander Klous, and Leon Gommans. Practical synthesis of mixed-tailed data with normalizing flows. Transactions on Machine Learning Research, July 2024. ISSN 2835-8856.

David M Blei, Alp Kucukelbir, and Jon D McAuliffe. Variational inference: A review for statisticians. Journal of the American Statistical Association, 112(518):859-877, 2017.

---

#### Page 14

Richard P. Brent. Algorithms for Minimization without Derivatives. Prentice-Hall, Englewood Cliffs, New Jersey, 1st edition, 1973.

Per A Brodtkorb and John D'Errico. numdifftools 0.9.41. https://github.com/pbrod/ numdifftools, 2022.

Steve Brooks, editor. Handbook for Markov Chain Monte Carlo. Taylor \& Francis, Boca Raton, 2011. ISBN 978-1-4200-7941-8.

Kenneth P Burnham and David R Anderson. Model selection and multimodel inference: a practical information-theoretic approach. Springer Science \& Business Media, 2003.

Bob Carpenter. Predator-Prey Population Dynamics: the Lotka-Volterra model in Stan. https://mc-stan.org/users/documentation/case-studies/ lotka-volterra-predator-prey.html, 2018.

Bob Carpenter, Andrew Gelman, Matthew D Hoffman, Daniel Lee, Ben Goodrich, Michael Betancourt, Marcus Brubaker, Jiqiang Guo, Peter Li, and Allen Riddell. Stan: A probabilistic programming language. Journal of Statistical Software, 76(1), 2017.

Rob Cornish, Anthony Caterini, George Deligiannidis, and Arnaud Doucet. Relaxing bijectivity constraints with continuously indexed normalising flows. In Proceedings of the 37th International Conference on Machine Learning, pages 2133-2143. PMLR, November 2020 .

Daniel A De Souza, Diego Mesquita, Samuel Kaski, and Luigi Acerbi. Parallel MCMC without embarrassing failures. International Conference on Artificial Intelligence and Statistics, pages 1786-1804, 2022.

Akash Kumar Dhaka, Alejandro Catalina, Manushi Welandawe, Michael R Andersen, Jonathan Huggins, and Aki Vehtari. Challenges and opportunities in high dimensional variational inference. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan, editors, Advances in Neural Information Processing Systems, volume 34, pages 7787-7798. Curran Associates, Inc., 2021.

Laurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio. Density estimation using Real NVP. In International Conference on Learning Representations, February 2017.

Conor Durkan, Artur Bekasov, Iain Murray, and George Papamakarios. nflows: normalizing flows in PyTorch, November 2020. URL https://doi.org/10.5281/zenodo.4296287.

Jonas El Gammal, Nils Schöneberg, Jesús Torrado, and Christian Fidler. Fast and robust Bayesian inference using Gaussian processes with GPry. Journal of Cosmology and Astroparticle Physics, 2023(10):021, October 2023. ISSN 1475-7516. doi: $10.1088 / 1475-7516 / 2023 / 10 / 021$.

Daniel Foreman-Mackey. Corner.py: Scatterplot matrices in Python. Journal of Open Source Software, 1(2):24, June 2016. ISSN 2475-9066. doi: 10.21105/joss. 00024.

---

#### Page 15

Vincent Fortuin. Priors in Bayesian Deep Learning: A Review. International Statistical Review, 90(3):563-591, 2022. ISSN 1751-5823. doi: 10.1111/insr. 12502.

Andrew Gelman, John B Carlin, Hal S Stern, David B Dunson, Aki Vehtari, and Donald B Rubin. Bayesian Data Analysis (3rd edition). CRC Press, 2013.

Mathieu Germain, Karol Gregor, Iain Murray, and Hugo Larochelle. MADE: Masked autoencoder for distribution estimation. In Francis Bach and David Blei, editors, Proceedings of the 32nd International Conference on Machine Learning, volume 37 of Proceedings of Machine Learning Research, pages 881-889, Lille, France, July 2015. PMLR.

Charles J Geyer. Estimating normalizing constants and reweighting mixtures. (Technical report). 1994.

David Greenberg, Marcel Nonnenmacher, and Jakob Macke. Automatic posterior transformation for likelihood-free inference. In Proceedings of the 36th International Conference on Machine Learning, volume 97 of Proceedings of Machine Learning Research, pages 2404-2414. PMLR, 2019.

Quentin F. Gronau, Alexandra Sarafoglou, Dora Matzke, Alexander Ly, Udo Boehm, Maarten Marsman, David S. Leslie, Jonathan J. Forster, Eric-Jan Wagenmakers, and Helen Steingroever. A tutorial on bridge sampling. Journal of Mathematical Psychology, 81:80-97, December 2017. ISSN 0022-2496. doi: 10.1016/j.jmp.2017.09.005.

Tom Gunter, Michael A Osborne, Roman Garnett, Philipp Hennig, and Stephen J Roberts. Sampling for inference in probabilistic models with fast Bayesian quadrature. Advances in Neural Information Processing Systems, 27:2789-2797, 2014.

Michael Gutmann and Aapo Hyvärinen. Noise-contrastive estimation: A new estimation principle for unnormalized statistical models. In Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics, pages 297-304. JMLR Workshop and Conference Proceedings, March 2010.

Nikolaus Hansen. The CMA evolution strategy: A tutorial. arXiv:1604.00772 [cs, stat], April 2016.

Peter Howard. Modeling basics. Lecture Notes for Math, 442, 2009.
Bobby Huggins, Chengkun Li, Marlon Tobaben, Mikko J. Aarnos, and Luigi Acerbi. PyVBMC: Efficient Bayesian inference in Python. Journal of Open Source Software, 8(86):5428, 2023. doi: 10.21105/joss.05428. URL https://doi.org/10.21105/joss. 05428 .

Marko Järvenpää, Michael U Gutmann, Aki Vehtari, and Pekka Marttinen. Parallel Gaussian process surrogate Bayesian inference with noisy likelihood evaluations. Bayesian Analysis, 16(1):147-178, 2021.

Mehrdad Jazayeri and Michael N Shadlen. Temporal context calibrates interval timing. Nature Neuroscience, 13(8):1020-1026, 2010.

---

#### Page 16

Marc C. Kennedy and Anthony O'Hagan. Bayesian calibration of computer models. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 63(3):425-464, 2001. ISSN 1467-9868. doi: 10.1111/1467-9868.00294.

Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. Proceedings of the 3rd International Conference on Learning Representations, 2014.

Konrad P Körding, Ulrik Beierholm, Wei Ji Ma, Steven Quartz, Joshua B Tenenbaum, and Ladan Shams. Causal inference in multisensory perception. PLoS One, 2(9):e943, 2007.

Alp Kucukelbir, Dustin Tran, Rajesh Ranganath, Andrew Gelman, and David M. Blei. Automatic differentiation variational inference. Journal of Machine Learning Research, 18(1):430-474, January 2017. ISSN 1532-4435.

Chengkun Li, Grégoire Clarté, Martin Jørgensen, and Luigi Acerbi. Fast post-process Bayesian inference with variational sparse Bayesian quadrature, 2024. URL https:// arxiv.org/abs/2303.05263.

Dong C. Liu and Jorge Nocedal. On the limited memory BFGS method for large scale optimization. Mathematical Programming, 45(1):503-528, August 1989. ISSN 1436-4646. doi: $10.1007 /$ BF01589116.

Jan-Matthis Lueckmann, Jan Boelts, David Greenberg, Pedro Goncalves, and Jakob Macke. Benchmarking simulation-based inference. In Proceedings of The 24th International Conference on Artificial Intelligence and Statistics, pages 343-351. PMLR, March 2021.

Wei Ji Ma, Konrad Paul Kording, and Daniel Goldreich. Bayesian models of perception and action: An introduction. MIT press, 2023.

David J.C. MacKay. Choice of basis for Laplace approximation. Machine Learning, 33(1): 77-86, October 1998. ISSN 1573-0565. doi: 10.1023/A:1007558615313.

David JC MacKay. Information theory, inference and learning algorithms. Cambridge University Press, 2003.

Petrus Mikkola, Osvaldo A. Martin, Suyog Chandramouli, Marcelo Hartmann, Oriol Abril Pla, Owen Thomas, Henri Pesonen, Jukka Corander, Aki Vehtari, Samuel Kaski, PaulChristian Bürkner, and Arto Klami. Prior knowledge elicitation: The past, present, and future. Bayesian Analysis, 19(4):1129-1161, December 2024. ISSN 1936-0975, 1931-6690. doi: $10.1214 / 23-\mathrm{BA} 1381$.

Radford M. Neal. MCMC using Hamiltonian dynamics. In Handbook of Markov Chain Monte Carlo. Chapman and Hall/CRC, 2011. ISBN 978-0-429-13850-8.

George Papamakarios, Theo Pavlakou, and Iain Murray. Masked autoregressive flow for density estimation. In Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017.

George Papamakarios, Eric Nalisnick, Danilo Jimenez Rezende, Shakir Mohamed, and Balaji Lakshminarayanan. Normalizing flows for probabilistic modeling and inference. Journal of Machine Learning Research, 22(57):1-64, 2021.

---

#### Page 17

Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Köpf, Edward Yang, Zach DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. PyTorch: An imperative style, high-performance deep learning library, December 2019.

Alexandre Pouget, Jeffrey M Beck, Wei Ji Ma, and Peter E Latham. Probabilistic brains: Knowns and unknowns. Nature Neuroscience, 16(9):1170-1178, 2013.

Stefan T. Radev, Ulf K. Mertens, Andreas Voss, Lynton Ardizzone, and Ullrich Köthe. BayesFlow: Learning complex stochastic models with invertible neural networks. IEEE Transactions on Neural Networks and Learning Systems, 33(4):1452-1466, April 2022. ISSN 2162-2388. doi: 10.1109/TNNLS.2020.3042395.

Rajesh Ranganath, Sean Gerrish, and David Blei. Black box variational inference. In Artificial Intelligence and Statistics, pages 814-822. PMLR, 2014.

Carl Edward Rasmussen. Gaussian processes to speed up hybrid Monte Carlo for expensive Bayesian integrals. Bayesian Statistics, 7:651-659, 2003.

Danilo Rezende and Shakir Mohamed. Variational inference with normalizing flows. In Proceedings of the 32nd International Conference on Machine Learning, pages 1530-1538. PMLR, June 2015.

Matteo Rizzato and Elena Sellentin. Extremely expensive likelihoods: A variational-Bayes solution for precision cosmology. Monthly Notices of the Royal Astronomical Society, 521 (1):1152-1161, March 2023. ISSN 0035-8711, 1365-2966. doi: 10.1093/mnras/stad638.

Jerome Sacks, William J. Welch, Toby J. Mitchell, and Henry P. Wynn. Design and analysis of computer experiments. Statistical Science, 4(4):409 - 423, 1989. doi: 10.1214/ss/ 1177012413. URL https://doi.org/10.1214/ss/1177012413.

Gurjeet Sangra Singh and Luigi Acerbi. PyBADS: Fast and robust black-box optimization in Python. Journal of Open Source Software, 9(94):5694, 2024. doi: 10.21105/joss.05694.

Vincent Stimper, Bernhard Schölkopf, and Jose Miguel Hernandez-Lobato. Resampling base distributions of normalizing flows. In Proceedings of The 25th International Conference on Artificial Intelligence and Statistics, pages 4915-4936. PMLR, May 2022.

Bas van Opheusden, Luigi Acerbi, and Wei Ji Ma. Unbiased and efficient log-likelihood estimation with inverse binomial sampling. PLOS Computational Biology, 16(12):e1008483, 2020. doi: 10.1371/journal.pcbi. 1008483 .

Aki Vehtari, Daniel Simpson, Andrew Gelman, Yuling Yao, and Jonah Gabry. Pareto smoothed importance sampling. Journal of Machine Learning Research, 25(72):1-58, 2024. ISSN 1533-7928.

Yu Wang, Fang Liu, and Daniele E. Schiavazzi. Variational inference with NoFAS: Normalizing flow with adaptive surrogate for computationally expensive models. Journal of Computational Physics, 467:111454, October 2022. ISSN 0021-9991. doi: $10.1016 /$ j.jcp.2022.111454.

---

#### Page 18

Robert C Wilson and Anne GE Collins. Ten simple rules for the computational modeling of behavioral data. Elife, 8:e49547, 2019.

Yuling Yao, Aki Vehtari, Daniel Simpson, and Andrew Gelman. Yes, but did it work?: Evaluating variational inference. In Proceedings of the 35th International Conference on Machine Learning, pages 5581-5590. PMLR, July 2018.