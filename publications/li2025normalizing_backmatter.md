# Normalizing Flow Regression for Bayesian Inference with Offline Likelihood Evaluations - Backmatter

---

#### Page 13

# References 

Luigi Acerbi. Variational Bayesian Monte Carlo. Advances in Neural Information Processing Systems, 31:8222-8232, 2018.

Luigi Acerbi. Variational Bayesian Monte Carlo with noisy likelihoods. Advances in Neural Information Processing Systems, 33:8211-8222, 2020.

Luigi Acerbi and Wei Ji Ma. Practical Bayesian optimization for model fitting with Bayesian adaptive direct search. Advances in Neural Information Processing Systems, 30:18341844, 2017.

Luigi Acerbi, Daniel M Wolpert, and Sethu Vijayakumar. Internal representations of temporal statistics and feedback calibrate motor-sensory interval timing. PLoS Computational Biology, 8(11):e1002771, 2012.

Luigi Acerbi, Kalpana Dokka, Dora E Angelaki, and Wei Ji Ma. Bayesian comparison of explicit and implicit causal inference strategies in multisensory heading perception. PLoS Computational Biology, 14(7):e1006110, 2018.

Masaki Adachi, Satoshi Hayakawa, Martin Jørgensen, Harald Oberhauser, and Michael A Osborne. Fast bayesian inference with batch bayesian quadrature via kernel recombination. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems, volume 35, pages 16533-16547. Curran Associates, Inc., 2022.

Abhinav Agrawal, Daniel R Sheldon, and Justin Domke. Advances in Black-Box VI: Normalizing Flows, Importance Weighting, and Optimization. In Advances in Neural Information Processing Systems, volume 33, pages 17358-17369. Curran Associates, Inc., 2020 .

Takeshi Amemiya. Tobit models: A survey. Journal of Econometrics, 24(1):3-61, January 1984. ISSN 0304-4076. doi: $10.1016 / 0304-4076(84) 90074-5$.

David M Blei, Alp Kucukelbir, and Jon D McAuliffe. Variational inference: A review for statisticians. Journal of the American Statistical Association, 112(518):859-877, 2017.

Richard P. Brent. Algorithms for Minimization without Derivatives. Prentice-Hall, Englewood Cliffs, New Jersey, 1st edition, 1973.

Per A Brodtkorb and John D'Errico. numdifftools 0.9.41. https://github.com/pbrod/ numdifftools, 2022.

Steve Brooks, editor. Handbook for Markov Chain Monte Carlo. Taylor \& Francis, Boca Raton, 2011. ISBN 978-1-4200-7941-8.

Kenneth P Burnham and David R Anderson. Model selection and multimodel inference: a practical information-theoretic approach. Springer Science \& Business Media, 2003.

---

#### Page 14

Bob Carpenter. Predator-Prey Population Dynamics: the Lotka-Volterra model in Stan. https://mc-stan.org/users/documentation/case-studies/ lotka-volterra-predator-prey.html, 2018.

Bob Carpenter, Andrew Gelman, Matthew D Hoffman, Daniel Lee, Ben Goodrich, Michael Betancourt, Marcus Brubaker, Jiquang Guo, Peter Li, and Allen Riddell. Stan: A probabilistic programming language. Journal of Statistical Software, 76(1), 2017.

Daniel A De Souza, Diego Mesquita, Samuel Kaski, and Luigi Acerbi. Parallel MCMC without embarrassing failures. International Conference on Artificial Intelligence and Statistics, pages 1786-1804, 2022.

Akash Kumar Dhaka, Alejandro Catalina, Manushi Welandawe, Michael R Andersen, Jonathan Huggins, and Aki Vehtari. Challenges and opportunities in high dimensional variational inference. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan, editors, Advances in Neural Information Processing Systems, volume 34, pages 7787-7798. Curran Associates, Inc., 2021.

Laurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio. Density estimation using Real NVP. In International Conference on Learning Representations, February 2017.

Conor Durkan, Artur Bekasov, Iain Murray, and George Papamakarios. nflows: normalizing flows in PyTorch, November 2020. URL https://doi.org/10.5281/zenodo.4296287.

Jonas El Gammal, Nils Schöneberg, Jesús Torrado, and Christian Fidler. Fast and robust Bayesian inference using Gaussian processes with GPry. Journal of Cosmology and Astroparticle Physics, 2023(10):021, October 2023. ISSN 1475-7516. doi: $10.1088 / 1475-7516 / 2023 / 10 / 021$.

Daniel Foreman-Mackey. Corner.py: Scatterplot matrices in Python. Journal of Open Source Software, 1(2):24, June 2016. ISSN 2475-9066. doi: 10.21105/joss. 00024.

Vincent Fortuin. Priors in Bayesian Deep Learning: A Review. International Statistical Review, 90(3):563-591, 2022. ISSN 1751-5823. doi: 10.1111/insr. 12502.

Andrew Gelman, John B Carlin, Hal S Stern, David B Dunson, Aki Vehtari, and Donald B Rubin. Bayesian Data Analysis (3rd edition). CRC Press, 2013.

Mathieu Germain, Karol Gregor, Iain Murray, and Hugo Larochelle. MADE: Masked autoencoder for distribution estimation. In Francis Bach and David Blei, editors, Proceedings of the 32nd International Conference on Machine Learning, volume 37 of Proceedings of Machine Learning Research, pages 881-889, Lille, France, July 2015. PMLR.

Charles J Geyer. Estimating normalizing constants and reweighting mixtures. (Technical report). 1994.

David Greenberg, Marcel Nonnenmacher, and Jakob Macke. Automatic posterior transformation for likelihood-free inference. In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors, Proceedings of the 36th International Conference on Machine Learning, volume 97 of Proceedings of Machine Learning Research, pages 2404-2414. PMLR, 2019.

---

#### Page 15

Quentin F. Gronau, Alexandra Sarafoglou, Dora Matzke, Alexander Ly, Udo Boehm, Maarten Marsman, David S. Leslie, Jonathan J. Forster, Eric-Jan Wagenmakers, and Helen Steingroever. A tutorial on bridge sampling. Journal of Mathematical Psychology, 81:80-97, December 2017. ISSN 0022-2496. doi: 10.1016/j.jmp.2017.09.005.

Tom Gunter, Michael A Osborne, Roman Garnett, Philipp Hennig, and Stephen J Roberts. Sampling for inference in probabilistic models with fast Bayesian quadrature. Advances in Neural Information Processing Systems, 27:2789-2797, 2014.

Michael Gutmann and Aapo Hyvärinen. Noise-contrastive estimation: A new estimation principle for unnormalized statistical models. In Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics, pages 297-304. JMLR Workshop and Conference Proceedings, March 2010.

Nikolaus Hansen. The CMA Evolution Strategy: A Tutorial. arXiv preprint arXiv:1604.00772, April 2016.

Peter Howard. Modeling basics. Lecture Notes for Math, 442, 2009.
Bobby Huggins, Chengkun Li, Marlon Tobaben, Mikko J. Aarnos, and Luigi Acerbi. Pyvbmc: Efficient bayesian inference in python. Journal of Open Source Software, 8(86): 5428, 2023. doi: 10.21105/joss.05428. URL https://doi.org/10.21105/joss.05428.

Marko Järvenpää, Michael U Gutmann, Aki Vehtari, and Pekka Marttinen. Parallel Gaussian process surrogate Bayesian inference with noisy likelihood evaluations. Bayesian Analysis, 16(1):147-178, 2021.

Mehrdad Jazayeri and Michael N Shadlen. Temporal context calibrates interval timing. Nature Neuroscience, 13(8):1020-1026, 2010.

Marc C. Kennedy and Anthony O'Hagan. Bayesian calibration of computer models. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 63(3):425-464, 2001. ISSN 1467-9868. doi: $10.1111 / 1467-9868.00294$.

Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. Proceedings of the 3rd International Conference on Learning Representations, 2014.

Konrad P Körding, Ulrik Beierholm, Wei Ji Ma, Steven Quartz, Joshua B Tenenbaum, and Ladan Shams. Causal inference in multisensory perception. PLoS One, 2(9):e943, 2007.

Alp Kucukelbir, Dustin Tran, Rajesh Ranganath, Andrew Gelman, and David M. Blei. Automatic differentiation variational inference. Journal of Machine Learning Research, 18(1):430-474, January 2017. ISSN 1532-4435.

Chengkun Li, Grégoire Clarté, Martin Jørgensen, and Luigi Acerbi. Fast post-process Bayesian inference with variational sparse Bayesian quadrature, 2024. URL https:// arxiv.org/abs/2303.05263.

Dong C. Liu and Jorge Nocedal. On the limited memory BFGS method for large scale optimization. Mathematical Programming, 45(1):503-528, August 1989. ISSN 1436-4646. doi: $10.1007 / \mathrm{BF} 01589116$.

---

#### Page 16

Jan-Matthis Lueckmann, Jan Boelts, David Greenberg, Pedro Goncalves, and Jakob Macke. Benchmarking Simulation-Based Inference. In Proceedings of The 24th International Conference on Artificial Intelligence and Statistics, pages 343-351. PMLR, March 2021.

Wei Ji Ma, Konrad Paul Kording, and Daniel Goldreich. Bayesian models of perception and action: An introduction. MIT press, 2023.

David J.C. MacKay. Choice of Basis for Laplace Approximation. Machine Learning, 33(1): 77-86, October 1998. ISSN 1573-0565. doi: 10.1023/A:1007558615313.

David JC MacKay. Information theory, inference and learning algorithms. Cambridge University Press, 2003.

Petrus Mikkola, Osvaldo A. Martin, Suyog Chandramouli, Marcelo Hartmann, Oriol Abril Pla, Owen Thomas, Henri Pesonen, Jukka Corander, Aki Vehtari, Samuel Kaski, PaulChristian Bürkner, and Arto Klami. Prior Knowledge Elicitation: The Past, Present, and Future. Bayesian Analysis, 19(4):1129-1161, December 2024. ISSN 1936-0975, 1931-6690. doi: $10.1214 / 23-\mathrm{BA} 1381$.

Radford M. Neal. MCMC using Hamiltonian dynamics. arXiv:1206.1901 [physics, stat], May 2011. doi: $10.1201 / \mathrm{b} 10905$.

George Papamakarios, Theo Pavlakou, and Iain Murray. Masked autoregressive flow for density estimation. In I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017.

George Papamakarios, Eric Nalisnick, Danilo Jimenez Rezende, Shakir Mohamed, and Balaji Lakshminarayanan. Normalizing flows for probabilistic modeling and inference. Journal of Machine Learning Research, 22(57):1-64, 2021.

Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Köpf, Edward Yang, Zach DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. PyTorch: An Imperative Style, High-Performance Deep Learning Library, December 2019.

Alexandre Pouget, Jeffrey M Beck, Wei Ji Ma, and Peter E Latham. Probabilistic brains: Knowns and unknowns. Nature Neuroscience, 16(9):1170-1178, 2013.

Stefan T. Radev, Ulf K. Mertens, Andreas Voss, Lynton Ardizzone, and Ullrich Köthe. BayesFlow: Learning Complex Stochastic Models With Invertible Neural Networks. IEEE Transactions on Neural Networks and Learning Systems, 33(4):1452-1466, April 2022. ISSN 2162-2388. doi: 10.1109/TNNLS.2020.3042395.

Rajesh Ranganath, Sean Gerrish, and David Blei. Black box variational inference. In Artificial Intelligence and Statistics, pages 814-822. PMLR, 2014.

Carl Edward Rasmussen. Gaussian processes to speed up hybrid Monte Carlo for expensive Bayesian integrals. Bayesian Statistics, 7:651-659, 2003.

---

#### Page 17

Danilo Jimenez Rezende and Shakir Mohamed. Variational inference with normalizing flows. In Proceedings of the 32nd International Conference on International Conference on Machine Learning - Volume 37, ICML'15, pages 1530-1538, Lille, France, July 2015. JMLR.org.

Matteo Rizzato and Elena Sellentin. Extremely expensive likelihoods: A variational-Bayes solution for precision cosmology. Monthly Notices of the Royal Astronomical Society, 521 (1):1152-1161, March 2023. ISSN 0035-8711, 1365-2966. doi: 10.1093/mnras/stad638.

Jerome Sacks, William J. Welch, Toby J. Mitchell, and Henry P. Wynn. Design and Analysis of Computer Experiments. Statistical Science, 4(4):409 - 423, 1989. doi: 10.1214/ss/ 1177012413. URL https://doi.org/10.1214/ss/1177012413.

Gurjeet Sangra Singh and Luigi Acerbi. PyBADS: Fast and robust black-box optimization in Python. Journal of Open Source Software, 9(94):5694, 2024. doi: 10.21105/joss.05694.

Bas van Opheusden, Luigi Acerbi, and Wei Ji Ma. Unbiased and efficient log-likelihood estimation with inverse binomial sampling. PLOS Computational Biology, 16(12):e1008483, 2020. doi: $10.1371 /$ journal.pcbi. 1008483 .

Aki Vehtari, Daniel Simpson, Andrew Gelman, Yuling Yao, and Jonah Gabry. Pareto Smoothed Importance Sampling, August 2022.

Yu Wang, Fang Liu, and Daniele E. Schiavazzi. Variational inference with NoFAS: Normalizing flow with adaptive surrogate for computationally expensive models. Journal of Computational Physics, 467:111454, October 2022. ISSN 0021-9991. doi: $10.1016 /$ j.jcp.2022.111454.

Robert C Wilson and Anne GE Collins. Ten simple rules for the computational modeling of behavioral data. Elife, 8:e49547, 2019.

Yuling Yao, Aki Vehtari, Daniel Simpson, and Andrew Gelman. Yes, but Did It Work?: Evaluating Variational Inference. In Proceedings of the 35th International Conference on Machine Learning, pages 5581-5590. PMLR, July 2018.