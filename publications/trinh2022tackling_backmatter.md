# Tackling covariate shift with node-based Bayesian neural networks - Backmatter

---

## Acknowledgement

This work was supported by the Academy of Finland (Flagship programme: Finnish Center for Artificial Intelligence FCAI and grants no. 292334, 294238, 319264, 328400) and UKRI Turing AI World-Leading Researcher Fellowship, EP/W002973/1. We acknowledge the computational resources provided by Aalto Science-IT project and CSC-IT Center for Science, Finland.

## References

Alquier, P. and Ridgway, J. Concentration of tempered posteriors and of their variational approximations. The Annals of Statistics, 48(3):1475-1497, 2020.

Arpit, D., Jastrzębski, S., Ballas, N., Krueger, D., Bengio, E., Kanwal, M. S., Maharaj, T., Fischer, A., Courville, A., Bengio, Y., and Lacoste-Julien, S. A closer look at memorization in deep networks. In ICML, pp. 233-242. PMLR, 2017.

Blei, D. M., Kucukelbir, A., and McAuliffe, J. D. Varia-
tional inference: A review for statisticians. Journal of the American statistical Association, 112(518):859-877, 2017.

Bouthillier, X., Konda, K., Vincent, P., and Memisevic, R. Dropout as data augmentation. arXiv preprint arXiv:1506.08700, 2015.

Dusenberry, M., Jerfel, G., Wen, Y., Ma, Y., Snoek, J., Heller, K., Lakshminarayanan, B., and Tran, D. Efficient and scalable Bayesian neural nets with rank-1 factors. In ICML, pp. 2782-2792, 2020.

Gal, Y. and Ghahramani, Z. Dropout as a Bayesian approximation: Representing model uncertainty in deep learning. In ICML, 2016.

Goodfellow, I. J., Shlens, J., and Szegedy, C. Explaining and harnessing adversarial examples. arXiv preprint arXiv:1412.6572, 2014.

Grünwald, P. The safe bayesian. In International Conference on Algorithmic Learning Theory, pp. 169-183. Springer, 2012.

He, K., Zhang, X., Ren, S., and Sun, J. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770-778, 2016a.

He, K., Zhang, X., Ren, S., and Sun, J. Identity mappings in deep residual networks. In European conference on computer vision, pp. 630-645. Springer, 2016b.

Hendrycks, D. and Dietterich, T. Benchmarking neural network robustness to common corruptions and perturbations. Proceedings of the International Conference on Learning Representations, 2019.

Hinton, G. E. and van Camp, D. Keeping the neural networks simple by minimizing the description length of the weights. In COLT, pp. 5-13, 1993.

Izmailov, P., Maddox, W. J., Kirichenko, P., Garipov, T., Vetrov, D., and Wilson, A. G. Subspace inference for Bayesian deep learning. In UAI, pp. 1169-1179, 2020.

Izmailov, P., Nicholson, P., Lotfi, S., and Wilson, A. G. Dangers of bayesian model averaging under covariate shift. arXiv preprint arXiv:2106.11905, 2021a.

Izmailov, P., Vikram, S., Hoffman, M. D., and Wilson, A. G. What are bayesian neural network posteriors really like? arXiv preprint arXiv:2104.14421, 2021b.

Jebara, T. and Kondor, R. Bhattacharyya and expected likelihood kernels. In Learning theory and kernel machines, pp. 57-71. Springer, 2003.

---

#### Page 10

Jebara, T., Kondor, R., and Howard, A. Probability product kernels. The Journal of Machine Learning Research, 5: 819-844, 2004.

Jeddi, A., Shafiee, M. J., Karg, M., Scharfenberger, C., and Wong, A. Learn2perturb: an end-to-end feature perturbation learning to improve adversarial robustness. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1241-1250, 2020.

Jiang, L., Zhou, Z., Leung, T., Li, L.-J., and Fei-Fei, L. Mentornet: Learning data-driven curriculum for very deep neural networks on corrupted labels. In ICML, 2018.

Karaletsos, T. and Bui, T. D. Hierarchical gaussian process priors for bayesian neural network weights. arXiv preprint arXiv:2002.04033, 2020.

Karaletsos, T., Dayan, P., and Ghahramani, Z. Probabilistic meta-representations of neural networks. arXiv preprint arXiv:1810.00555, 2018.

Kingma, D. P., Salimans, T., and Welling, M. Variational dropout and the local reparameterization trick. In NIPS, pp. 2575-2583, 2015.

Kolchinsky, A. and Tracey, B. D. Estimating mixture entropy with pairwise distances. Entropy, 19(7), 2017. ISSN 1099-4300. doi: 10.3390/e19070361. URL https: //www.mdpi. com/1099-4300/19/7/361.

Krizhevsky, A., Nair, V., and Hinton, G. Cifar-10 and cifar100 datasets. URI: https://www. cs. toronto. edu/kriz/cifar. html, 6(1):1, 2009.

Kwon, J., Kim, J., Park, H., and Choi, I. K. Asam: Adaptive sharpness-aware minimization for scale-invariant learning of deep neural networks. arXiv preprint arXiv:2102.11600, 2021.

Lakshminarayanan, B., Pritzel, A., and Blundell, C. Simple and scalable predictive uncertainty estimation using deep ensembles. In NIPS, pp. 6405-6416, 2017.

Le, Y. and Yang, X. S. Tiny imagenet visual recognition challenge. 2015.

Li, B., Chen, C., Wang, W., and Carin, L. Certified adversarial robustness with additive noise. arXiv preprint arXiv:1809.03113, 2018.

Louizos, C. and Welling, M. Multiplicative normalizing flows for variational bayesian neural networks. In International Conference on Machine Learning, pp. 2218-2227. PMLR, 2017.

MacKay, D. J. C. A practical Bayesian framework for backpropagation networks. Neural Computation, 4(3): 448-472, May 1992. ISSN 0899-7667.

MacKay, D. J. C. Probable networks and plausible predictions - a review of practical Bayesian methods for supervised neural networks. Network: Computation in Neural Systems, 6(3):469-505, 1995.

Maddox, W. J., Izmailov, P., Garipov, T., Vetrov, D. P., and Wilson, A. G. A simple baseline for bayesian uncertainty in deep learning. In Advances in Neural Information Processing Systems, pp. 13153-13164, 2019.

Mandt, S., McInerney, J., Abrol, F., Ranganath, R., and Blei, D. Variational tempering. In Artificial Intelligence and Statistics, pp. 704-712. PMLR, 2016.

Medina, M. A., Olea, J. L. M., Rush, C., and Velez, A. On the robustness to misspecification of $\alpha$-posteriors and their variational approximations. arXiv preprint arXiv:2104.08324, 2021.

Miller, J. W. and Dunson, D. B. Robust Bayesian Inference via Coarsening. Journal of the American Statistical Association, 114(527):1113-1125, July 2019. ISSN 0162-1459. doi: 10.1080/01621459.2018. 1469995. URL https://doi.org/10.1080/ 01621459.2018.1469995. Publisher: Taylor \& Francis.

Naeini, M. P., Cooper, G. F., and Hauskrecht, M. Obtaining well calibrated probabilities using Bayesian binning. In AAAI, 2015.

Neal, R. M. Bayesian Learning for Neural Networks. Lecture Notes in Statistics. Springer-Verlag, New York, 1996. ISBN 978-0-387-94724-2.

Nguyen, S., Nguyen, D., Nguyen, K., Than, K., Bui, H., and Ho, N. Structured dropout variational inference for bayesian neural networks. In NeurIPS, 2021.

Ovadia, Y., Fertig, E., Ren, J., Nado, Z., Sculley, D., Nowozin, S., Dillon, J. V., Lakshminarayanan, B., and Snoek, J. Can you trust your model's uncertainty? evaluating predictive uncertainty under dataset shift. arXiv preprint arXiv:1906.02530, 2019.

Pradier, M. F., Pan, W., Yao, J., Ghosh, S., and Doshi-Velez, F. Projected bnns: Avoiding weight-space pathologies by learning latent representations of neural network weights. arXiv preprint arXiv:1811.07006, 2018.

Simonyan, K. and Zisserman, A. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014.

Springenberg, J. T., Dosovitskiy, A., Brox, T., and Riedmiller, M. Striving for simplicity: The all convolutional net. arXiv preprint arXiv:1412.6806, 2014.

---

#### Page 11

Sun, S., Zhang, G., Shi, J., and Grosse, R. Functional variational bayesian neural networks. arXiv preprint arXiv:1903.05779, 2019.

Trinh, T., Kaski, S., and Heinonen, M. Scalable bayesian neural networks by layer-wise input augmentation. arXiv preprint arXiv:2010.13498, 2020.

Vadera, M. P., Shukla, S. N., Jalaian, B., and Marlin, B. M. Assessing the adversarial robustness of monte carlo and distillation methods for deep bayesian neural network classification. arXiv preprint arXiv:2002.02842, 2020.

Wenzel, F., Roth, K., Veeling, B. S., Świątkowski, J., Tran, L., Mandt, S., Snoek, J., Salimans, T., Jenatton, R., and Nowozin, S. How good is the bayes posterior in deep neural networks really? arXiv preprint arXiv:2002.02405, 2020.

Zhang, R., Li, C., Zhang, J., Chen, C., and Wilson, A. G. Cyclical stochastic gradient mcmc for bayesian deep learning. International Conference on Learning Representations, 2020.