```
@article{opheusden2020unbiased,
  title={Unbiased and Efficient Log-Likelihood Estimation with Inverse Binomial Sampling},
  author={Bas {van Opheusden} and Luigi Acerbi and Wei Ji Ma},
  year={2020},
  journal={PLoS Computational Biology},
  doi={10.1371/journal.pcbi.1008483}
}
```

---

#### Page 1

# Unbiased and Efficient Log-Likelihood Estimation with Inverse Binomial Sampling

Bas van Opheusden*, 1, 2, Luigi Acerbi*, 1, 3, 4, Wei Ji Ma ${ }^{1,5}$<br>${ }^{1}$ Center for Neural Science, New York University, New York, NY 10003, U.S.A.<br>${ }^{2}$ Department of Psychology, Princeton University, Princeton, NJ 08540, U.S.A.<br>${ }^{3}$ Department of Computer Science, University of Helsinki, 00560 Helsinki, Finland.<br>${ }^{4}$ Department of Basic Neuroscience, University of Geneva, 1206 Geneva, Switzerland.<br>${ }^{5}$ Department of Psychology, New York University, New York, NY 10003, U.S.A.<br>\* These authors contributed equally to this work.

Keywords: Maximum-likelihood estimation, likelihood-free inference, simulation model.

Corresponding authors: svo@princeton.edu; luigi.acerbi@helsinki.fi.

---

#### Page 2

#### Abstract

The fate of scientific hypotheses often relies on the ability of a computational model to explain the data, quantified in modern statistical approaches by the likelihood function. The log-likelihood is the key element for parameter estimation and model evaluation. However, the log-likelihood of complex models in fields such as computational biology and neuroscience is often intractable to compute analytically or numerically. In those cases, researchers can often only estimate the log-likelihood by comparing observed data with synthetic observations generated by model simulations. Standard techniques to approximate the likelihood via simulation either use summary statistics of the data or are at risk of producing substantial biases in the estimate. Here, we explore another method, inverse binomial sampling (IBS), which can estimate the log-likelihood of an entire data set efficiently and without bias. For each observation, IBS draws samples from the simulator model until one matches the observation. The log-likelihood estimate is then a function of the number of samples drawn. The variance of this estimator is uniformly bounded, achieves the minimum variance for an unbiased estimator, and we can compute calibrated estimates of the variance. We provide theoretical arguments in favor of IBS and an empirical assessment of the method for maximum-likelihood estimation with simulation-based models. As case studies, we take three model-fitting problems of increasing complexity from computational and cognitive neuroscience. In all problems, IBS generally produces lower error in the estimated parameters and maximum log-likelihood values than alternative sampling methods with the same average number of samples. Our results demonstrate the potential of IBS as a practical, robust, and easy to implement method for log-likelihood evaluation when exact techniques are

---

#### Page 3

not available.

# 1 Introduction

The likelihood function is one of the most important mathematical objects for modern statistical inference. Briefly, the likelihood function measures how well a model with a given set of parameters can explain an observed data set. For a data set of discrete observations, the likelihood has the intuitive interpretation of the probability that a random sample generated from the model matches the data, for a given setting of the model parameters.

In many scientific disciplines, such as computational neuroscience and cognitive science, computational models are used to give a precise quantitative form to scientific hypotheses and theories. Statistical inference then plays at least two fundamental roles for scientific discovery. First, our goal may be parameter estimation for a model of interest. Parameter values may have a significance in themselves, for example we may be looking for differences in parameters between distinct experimental conditions in a clinical or behavioral study. Second, we may be considering a number of competing scientific hypotheses, instantiated by different models, and we want to evaluate which model 'best' captures the data according to some criteria, such as explanation (what evidence the data provide in favor of each model?) and prediction (which model best predicts new observations?).

Crucially, the likelihood function is a key element for both parameter estimation and model evaluation. A principled method to find best-fitting model parameters for a

---

#### Page 4

given data set is maximum-likelihood estimation (MLE), which entails optimizing the likelihood function over the parameter space (Myung, 2003). Other common parameter estimation methods, such as maximum-a-posteriori (MAP) estimation or full or approximate Bayesian inference of posterior distributions, still involve the likelihood function (Gelman et al., 2013). Moreover, almost all model comparison metrics commonly used for scientific model evaluation are based on likelihood computations, from predictive metrics such as Akaike's information criterion (AIC; Akaike, 1974), the deviance information criterion (DIC; Spiegelhalter et al., 2002), the widely applicable information criterion (WAIC; Watanabe, 2010), leave-one-out cross-validation (Vehtari et al., 2017); to evidence-based metrics such at the marginal likelihood (MacKay, 2003) and (loose) approximations thereof, such as the Bayesian information criterion (BIC; Schwarz et al., 1978) or the Laplace approximation (MacKay, 2003).

However, many complex computational models, such as those developed in computational biology (Pritchard et al., 1999; Ratmann et al., 2007; Wilkinson, 2011), neuroscience (Pospischil et al., 2008; Sterratt et al., 2011) and cognitive science (van Opheusden et al., 2016), take the form of a generative model or simulator, that is an algorithm which given some context information and parameter settings returns one or more simulated observations (a synthetic data set). In those cases, the likelihood is often impossible to calculate analytically, and even when the likelihood might be available in theory, the numerical calculations needed to obtain it might be overwhelmingly expensive and intractable in practice. In such situations, the only thing one can do is to run the model to simulate observations ('samples'). In the absence of a likelihood function, common approaches to 'likelihood-free inference' generally try and match summary statistics

---

#### Page 5

of the data with summary statistics of simulated observations (Beaumont et al., 2002; Wood, 2010).

In this paper, we ask instead the question of whether we can use samples from a simulator model to directly estimate the likelihood of the full data set, without recurring to summary statistics, in a 'correct' and 'efficient' manner, for some specific definition of these terms. The answer is yes, as long as we use the right sampling method.

In brief, a sampling method consists of a 'sampling policy' (a rule that determines how long to keep drawing samples for) and an 'estimator' which converts the samples to a real-valued number. To estimate the likelihood of a single observation (e.g., the response of a participant on a single trial of a behavioral experiment), the most obvious sampling policy is to draw a fixed amount of samples from the simulator model, and the simplest estimator is the fraction of samples that match the observation (or is 'close enough' to it, for continuous observations). However, most basic applications, such as computing the likelihood of multiple observations, require one to estimate the logarithm of the likelihood, or log-likelihood (see Section 2.3 for the underlying technical reasons). The 'fixed sampling' method described above cannot provide unbiased estimates for the log-likelihood (see Section 3). Such bias vanishes in the asymptotic limit of infinite samples, but drawing samples from the model can be computationally expensive, especially if the simulator model is complex. In practice, the bias introduced by any fixed sampling method can translate to considerable biases in estimates of model parameters, or even reverse the outcome of model comparison analyses. In other words, using poor sampling methods can cause researchers to draw conclusions about scientific hypotheses which are not supported by their data.

---

#### Page 6

In this work, we introduce inverse binomial sampling (IBS) as a powerful and simple technique for correctly and efficiently estimating log-likelihoods of simulator-based models. Crucially, IBS is a sampling method that provides uniformly unbiased estimates of the log-likelihood (Haldane, 1945b; de Groot, 1959) and calibrated estimates of their variance, which is also uniformly bounded.

We note that the problem of estimating functions $f(p)$ from observations of a Bernoulli distribution with parameter $p$ has been studied for mostly theoretical reasons in the mid20th century, with major contributions from Haldane (1945a,b), Girshick et al. (1946), Dawson (1953) and de Groot (1959). These works have largely focused on deriving the set of functions $f(p)$ for which an unbiased estimate exists, and demonstrating that for those functions, the inverse sampling policy (see Section 2.4) is in a precise sense 'efficient'. Our main contribution here is to demonstrate that inverse binomial sampling provides a practically and theoretically efficient solution for a common problem in computational modeling; namely likelihood-free inference of complex models. To back up our claims, we provide theoretical arguments for the efficiency of IBS and a practical demonstration of its value for log-likelihood estimation and fitting of simulationbased models, in particular those used in computational cognitive science. We note that Duncan (2004) had previously proposed inverse binomial sampling as a method for likelihood-free inference for certain econometric models, but did not present an empirical assessment of the quality of the estimation and to our knowledge has not led to further adoption of IBS.

The paper is structured as follows. After setting the stage with useful definitions and notation (Section 2), we describe more in detail the issues with the fixed sampling

---

#### Page 7

method and why they cannot be fixed (Section 3). We then present a series of arguments for why IBS solves these issues, and in particular why being unbiased here is of particular relevance (Section 4). Then, we present an empirical comparison of IBS and fixed sampling in the setting of maximum-likelihood estimation (Section 5). As case studies, we take three model-fitting problems of increasing complexity from computational cognitive science: an 'orientation discrimination' task, a 'change localization' task, and a complex sequential decision making task. In all problems, IBS generally produces lower error in the estimated parameters than fixed sampling with the same average number of samples. IBS also returns solutions that are very close in value to the true maximum log-likelihood. We conclude by discussing further applications and extensions of IBS (Section 6). Our theoretical analyses and empirical results demonstrate the potential of IBS as a practical, robust, and easy-to-implement method for log-likelihood evaluation when exact or numerical solutions are unavailable.

Implementations of IBS with tutorials and examples are available at the following link: https://github.com/lacerbi/ibs.

# 2 Definitions and notation

The two fundamental ingredients to run IBS are:

1. A data set $\mathcal{D}=\left\{\left(\boldsymbol{s}_{i}, \boldsymbol{r}_{i}\right)\right\}_{i=1}^{N}$ consisting of $N$ 'trials' characterized by 'stimuli' $\boldsymbol{s}_{i}$ and discrete 'responses' $\boldsymbol{r}_{i}$.
2. A generative model $g$ for the data (also known as a 'simulator'): a stochastic function that takes as input a stimulus $\boldsymbol{s}$ and a parameter vector $\boldsymbol{\theta}$ (and possibly

---

#### Page 8

other information) and outputs a response $\boldsymbol{r}$.

In this section we expand on and provide motivations for the above assumptions, and introduce related definitions and notation used in the rest of the paper.

Here and in the following, for ease of reference, we use the language of behavioral and cognitive modeling (e.g., 'trial' for data points, 'stimulus' for independent or contextual variables, 'response' for observations or outcomes), but the statistical techniques that we discuss in the paper apply to any model and data set from any domain as long as they satisfy the fundamental assumption of IBS delineated above.

# 2.1 The likelihood function

We assume that we want to model a data set $\mathcal{D}=\left\{\left(\boldsymbol{s}_{i}, \boldsymbol{r}_{i}\right)\right\}_{i=1}^{N}$ consisting of $N$ 'trials' (data points), where

- $\boldsymbol{s}_{i}$ is the stimulus (i.e., the experimental context, or independent variable) presented on the $i$-th trial; typically, $\boldsymbol{s}_{i}$ is a scalar or vector of discrete or continuous variables (more generally, there are no restrictions on what $\boldsymbol{s}_{i}$ can be as long as the simulator can accept it as input);
- $\boldsymbol{r}_{i}$ is the response (i.e., the experimental observations, outcomes, or dependent variables) measured on the $i$-th trial; $\boldsymbol{r}_{i}$ can be a scalar or vector, but crucially we assume it takes discrete values.

The requirement that $\boldsymbol{r}_{i}$ be discrete will be discussed below, in Section 2.2.
Given a data set $\mathcal{D}$, and a model parametrized by parameter vector $\boldsymbol{\theta}$, we can write

---

#### Page 9

the likelihood function for the responses given the stimuli and model parameters as

$$
\begin{aligned}
\operatorname{Pr}\left(\left\{\boldsymbol{r}_{i}\right\}_{i=1}^{N} \mid\left\{\boldsymbol{s}_{i}\right\}_{i=1}^{N}, \boldsymbol{\theta}\right) & =\prod_{i=1}^{N} \operatorname{Pr}\left(\boldsymbol{r}_{i} \mid \boldsymbol{r}_{1}, \ldots, \boldsymbol{r}_{i-1}, \boldsymbol{s}_{1}, \ldots, \boldsymbol{s}_{N}, \boldsymbol{\theta}\right) \\
& =\prod_{i=1}^{N} \operatorname{Pr}\left(\boldsymbol{r}_{i} \mid \boldsymbol{r}_{1}, \ldots, \boldsymbol{r}_{i-1}, \boldsymbol{s}_{1}, \ldots, \boldsymbol{s}_{i}, \boldsymbol{\theta}\right)
\end{aligned}
$$

where the first line follows from the chain rule of probability, and holds in general, whereas in the second step we applied the reasonable 'causal' (or 'no-time-travel') assumption that the response at the $i$-th trial is not influenced by future stimuli. ${ }^{1}$

Note that Equation 1 assumes that the researcher is not interested in statistically modeling the stimuli, which are taken to be given (i.e., on the right-hand side of the conditional probability). This choice is without loss of generality, as any variable of statistical interest can always be relabeled and become an element of the 'response' vector. For compactness, from now on we will denote $\operatorname{Pr}\left(\left\{\boldsymbol{r}_{i}\right\}_{i=1}^{N} \mid\left\{\boldsymbol{s}_{i}\right\}_{i=1}^{N}, \boldsymbol{\theta}\right) \equiv \operatorname{Pr}(\mathcal{D} \mid \boldsymbol{\theta})$, in a slight abuse of notation.

Equation 1 describes the most general class of models, in which the response in the current trial might be influenced by the history of both previous stimuli and previous responses. Many models commonly make a stronger conditional independence assumption between trials, such that the response on the current trial only depends on the current stimulus. Under this stronger assumption, the likelihood takes a simpler form,

$$
\operatorname{Pr}(\mathcal{D} \mid \boldsymbol{\theta})=\prod_{i=1}^{N} \operatorname{Pr}\left(\boldsymbol{r}_{i} \mid \boldsymbol{s}_{i}, \boldsymbol{\theta}\right)
$$

While Equation 2 is simpler, it still includes a wide variety of models. For example,

[^0]
[^0]: ${ }^{1}$ We also used the causality assumption that current responses are not influenced by future responses to choose a specific order to apply the chain rule in the first line.

---

#### Page 10

note that time-dependence can be easily included in the model by incorporating time into the 'stimulus' $\boldsymbol{s}$, and including time-dependent parameters explicitly in the model specification. In the rest of this work, for simplicity we consider models that make conditional independence assumptions as in Equation 2, but our techniques apply in general also for likelihoods as per Equation 1.

Given that the likelihood of the $i$-th trial can be directly interpreted as the probability of observing response $\boldsymbol{r}_{i}$ in the $i$-th trial (conditioned on everything else), we denote such quantity with $p_{i} \in[0,1]$. The value $p_{i}$ is a function of $\boldsymbol{\theta}$, depends on the current stimulus and response, and may or may not depend on previous stimuli or responses.

With this notation, we can simply write the likelihood as

$$
\operatorname{Pr}(\mathcal{D} \mid \boldsymbol{\theta})=\prod_{i=1}^{N} p_{i}
$$

Finally, we note that it is common practice to work with the logarithm of the likelihood, or log-likelihood, that is

$$
\mathcal{L}(\boldsymbol{\theta}) \equiv \log \operatorname{Pr}(\mathcal{D} \mid \boldsymbol{\theta})=\log \prod_{i=1}^{N} p_{i}=\sum_{i=1}^{N} \log p_{i}
$$

The typical rationale for switching to the log-likelihood is that for large $N$ the likelihood tends to be a vanishingly small quantity, so the logarithm makes it easier to handle numerically ('numerical convenience'). However, we will see later (see Section 3.2) that there are statistically meaningful reasons to prefer the logarithmic representation (i.e., a sum of independent terms).

Crucially, we assume that the likelihood function is unavailable in a tractable form for example, because the model is too complex to derive an analytical expression for the likelihood. Instead, IBS provides a technique for estimating Equation 4 via simulation.

---

#### Page 11

# 2.2 The generative model or simulator

While we assume no availability of an explicit representation of the likelihood function, we assume that the model of interest is represented implicitly by a stochastic generative model (or 'simulator'). In the most general case, the simulator is a stochastic function $g$ that takes as input the current stimulus $\boldsymbol{s}_{i}$, arrays of past stimuli and responses, and a parameter vector $\boldsymbol{\theta}$, and outputs a discrete response $\boldsymbol{r}_{i}$, conditional on all past events,

$$
\boldsymbol{r}_{i} \sim g\left(\boldsymbol{s}_{1}, \ldots, \boldsymbol{s}_{i}, \boldsymbol{r}_{1}, \ldots, \boldsymbol{r}_{i-1} ; \boldsymbol{\theta}\right)
$$

As mentioned in the previous section, a common assumption for a model is that the response in the current trial only depends on the current stimulus and parameter vector, in which case

$$
\boldsymbol{r} \sim g(\boldsymbol{s} ; \boldsymbol{\theta})
$$

For example, the model $g(\cdot)$ could be simulating the responses of a human participant in a complex cognitive task; the (discrete) choices taken by a rodent in a perceptual decision-making experiment; or the spike count of a neuron in sensory cortex for a specific time bin after a stimulus presentation.

We list now the requirements that the simulator model needs to satisfy to be used in conjuction with IBS.

## Discrete response space

Lacking an expression for the likelihood function, the only way to estimate the likelihood or any function thereof is by drawing samples $\boldsymbol{r} \sim g\left(\boldsymbol{s}_{i}, \ldots ; \boldsymbol{\theta}\right)$ on each trial, and matching them to the response $\boldsymbol{r}_{i}$. This approach requires that there is a nonzero

---

#### Page 12

probability for a random sample $\boldsymbol{r}$ to match $\boldsymbol{r}_{i}$, hence the assumption that the space of responses is discrete. We will discuss in Section 6.3 a possible method to extend IBS to larger or continuous response spaces.

# Conditional simulation

An important requirement of the generative model, stated implicitly by Equations 5 and 6 , is that the simulator should afford conditional simulation, in that we can simulate the response $\boldsymbol{r}_{i}$ for any trial $i$, given the current stimulus $\boldsymbol{s}_{i}$, and possibly previous stimuli and responses. Note that this class of models, while large, does not include all possible simulators, in that some simulators might not afford conditional generation of responses. For example, models with latent dynamics might be able to generate a full sequence of responses given the stimuli, but it might not be easy or computationally tractable to generate the response in a given trial, conditional on a specific sequence of previous responses.

## Computational cost

Finally, for the purpose of some of our analyses we assume that drawing a sample from the generative model is at least moderately computationally expensive, which limits the approximate budget of samples one is willing to use for each likelihood evaluation (in our analyses, up to about a hundred, on average, per likelihood evaluation). Number of samples is a reasonable proxy for any realistic resource expenditure since most costs (e.g., time, energy, number of processors) would be approximately proportional to it. Therefore, we also require that every response value in the data has a non-negligible

---

#### Page 13

probability of being sampled from the model - given the available budget of samples one can reasonably draw. In this paper, we will focus on the low-sample regime, since that is where IBS considerably outperforms other approaches. For our analyses of performance of the algorithm, we also assume that the computational cost is independent of the stimulus, response or model parameters, but this is not a requirement of the method.

# 2.3 Reduction to Bernoulli sampling

Given the conditional independence structure codified by Equation 3, to estimate the log-likelihood of the entire data set, we cannot do better than estimating $p_{i}$ on each trial independently, and combining the results. However, combining estimates $\hat{p}_{i}$ into a well-behaved estimate of $\prod_{i=1}^{N} p_{i}$ is non-trivial (see Section 3.2). Instead, it is easier to estimate $\mathcal{L}_{i} \equiv \log p_{i}$ for each trial and calculate the log-likelihood

$$
\mathcal{L}(\boldsymbol{\theta})=\log \operatorname{Pr}(\mathcal{D} \mid \boldsymbol{\theta})=\sum_{i=1}^{N} \log p_{i}=\sum_{i=1}^{N} \mathcal{L}_{i}
$$

We can estimate this log-likelihood by simply summing estimates $\hat{\mathcal{L}}_{i}$ across trials, in which case the central limit theorem guarantees that the distribution of $\hat{\mathcal{L}}(\boldsymbol{\theta})$ is normally distributed for large values of $N$, which is true for typical values of $N$ of the order of a hundred or more (see also Section 4.6).

We can make one additional simplification, without loss of generality. The generative model specifies an implicit probability distribution $\boldsymbol{r}_{i} \sim g\left(\boldsymbol{s}_{i}, \ldots ; \boldsymbol{\theta}\right)$ for each trial. However, to estimate the log-likelihood, we do not need to know the full distribution, only the probability for a random sample $\boldsymbol{r}$ from the model to match the observed

---

#### Page 14

response $\boldsymbol{r}_{i}$. Therefore, we can convert each sample $\boldsymbol{r}$ to

$$
x=\left\{\begin{array}{lll}
1 & \text { if } \boldsymbol{r}=\boldsymbol{r}_{i} & \text { ('hit') } \\
0 & \text { otherwise } & \text { ('miss'), }
\end{array}\right.
$$

and lose no information relevant for estimating the log-likelihood. By construction, $x$ follows a Bernoulli distribution with probability $p_{i}$. Note that this holds regardless of the type of data, the structure of the generative model or the model parameters. The only difference between different models and data sets is the distribution of the likelihood $p_{i}$ across trials. Moreover, since $p_{i}$ is interpreted as the parameter of a Bernoulli distribution, we can apply standard frequentist or Bayesian statistical reasoning to it.

In conclusion, we can reduce the problem of estimating the log-likelihood of a given model by sampling to a smaller problem: given a method to draw samples $\left(x_{1}, x_{2}, \ldots\right)$ from a Bernoulli distribution with unknown parameter $p$, estimate $\log p$ as precisely and accurately as possible using on average as few samples as possible.

# 2.4 Sampling policies and estimators

A sampling policy is a function that, given a sequence of samples $\boldsymbol{x} \equiv\left(x_{1}, x_{2}, \ldots, x_{k}\right)$, decides whether to draw an additional sample or not (Girshick et al., 1946). In this work, we compare two sampling policies:

1. The commonly used fixed policy: Draw a fixed number of samples $M$, then stop.
2. The inverse binomial sampling policy: Keep drawing samples until $x_{k}=1$, then stop.

In our case, an estimator (of $\log p$ ) is a function $\hat{\mathcal{L}}(\boldsymbol{x})$ that takes as input a sequence of samples $\boldsymbol{x}=\left(x_{1}, x_{2}, \ldots, x_{k}\right)$ and returns an estimate of $\log p$. We recall that the bias

---

#### Page 15

of an estimator $\hat{\mathcal{L}}$ of $\log p$, for a given true value of the Bernoulli parameter $p$, is defined as

$$
\text { Bias }[\hat{\mathcal{L}} \mid p]=\mathbb{E}[\hat{\mathcal{L}}]-\log p
$$

where the expectation is taken over all possible sequences $\boldsymbol{x}$ generated by the chosen sampling policy under the Bernoulli probability $p$. Such estimator is (uniformly) unbiased if $\operatorname{Bias}[\hat{\mathcal{L}} \mid p]=0$ for all $0<p \leq 1$ (that is, the estimator is centered around the true value).

# Fixed sampling

For the fixed sampling policy, since all samples are independent and identically distributed, a sufficient statistic for estimating $p$ from the samples $\left(x_{1}, x_{2}, \ldots, x_{M}\right)$ is the number of 'hits', $m(\boldsymbol{x}) \equiv \sum_{k=1}^{M} x_{k}$. The most obvious estimator for an obtained sequence of samples $\boldsymbol{x}$ is then

$$
\hat{\mathcal{L}}_{\text {naive }}(\boldsymbol{x})=\log \left(\frac{m(\boldsymbol{x})}{M}\right)
$$

but this estimator has infinite bias; since as long as $p \neq 1$, there is always a nonzero chance that $m(\boldsymbol{x})=0$, in which case $\hat{\mathcal{L}}_{\text {naive }}(\boldsymbol{x})=-\infty$ (and thus $\mathbb{E}\left[\hat{\mathcal{L}}_{\text {naive }}\right]=-\infty$ ). This divergence can be fixed in multiple ways; in the main text we use

$$
\hat{\mathcal{L}}_{\text {fixed }}(\boldsymbol{x})=\log \left(\frac{m(\boldsymbol{x})+1}{M+1}\right)
$$

Note that any estimator based on the fixed sampling policy will always produce biased estimates of $\log p$, as guaranteed by the reasoning in Section 3 below. As an empirical validation, we show in Appendix B. 1 that our results do not depend on the specific choice of estimator for fixed sampling (Equation 11).

---

#### Page 16

# Inverse binomial sampling

For inverse binomial sampling we note that, since $x$ is a binary variable, the policy will always result in a sequence of samples of the form

$$
\boldsymbol{x}=\overbrace{0,0,0,0,0, \ldots, 0,1}^{K}
$$

where the length of the sequence is a stochastic variable, which we label $K$ (a positive integer). Moreover, since each sample is independent and a 'hit' with probability $p$, the length $K$ follows a geometric distribution with parameter $1-p$,

$$
\operatorname{Pr}(K=k)=p(1-p)^{k-1}
$$

We convert a value of $K$ into an estimate for $\log p$ using the IBS estimator,

$$
\hat{\mathcal{L}}_{\mathrm{IBS}}(\boldsymbol{x})= \begin{cases}0 & \text { for } K=1 \\ -\sum_{k=1}^{K-1} \frac{1}{k} & \text { for } K>1\end{cases}
$$

Crucially, Equation 14 combined with the IBS policy provides a uniformly unbiased estimator of $\log p$ (de Groot, 1959). Moreover, we can show that $\hat{\mathcal{L}}_{\text {IBS }}$ is the uniformly minimum-variance unbiased estimator of $\log p$ under the IBS policy. For a full derivation of the properties of the IBS estimator, we refer to Appendix A.1. Equation 14 can be written compactly as $\hat{\mathcal{L}}_{\text {IBS }}(K)=\psi(1)-\psi(K)$, where $\psi(z)$ is the digamma function (Abramowitz and Stegun, 1948).

We now provide an understanding of why fixed sampling is not a good policy, despite its intuitive appeal, and then show why IBS solves many of the problems with fixed sampling.

---

#### Page 17

# 3 Why fixed sampling fails

We summarize in Figure 1 the properties of the IBS estimator and of fixed sampling, for different number of samples $M$, as a function of the trial likelihood $p$. In particular, we plot the expected number of samples, the bias, and the standard deviation of the estimators.

> **Image description.** The image consists of three line graphs, labeled A, B, and C, arranged horizontally. Each graph plots data against the variable 'p' on the x-axis, ranging from 0 to 1.
>
> **Panel A:**
>
> - The y-axis is labeled "Number of samples" and ranges from 0 to 100.
> - A blue curve starts at a high value near p=0 and decreases rapidly, approaching 0 as 'p' approaches 1. This represents the "IBS" method.
> - Several horizontal lines in shades of red, becoming darker as the number of samples increases, represent fixed sampling methods. These lines are at y-values of approximately 1, 2, 5, 10, 20, 50, and 100.
>
> **Panel B:**
>
> - The y-axis is labeled "Bias" and ranges from 0 to 3.
> - A blue horizontal line is at y=0, representing the "IBS" method.
> - Multiple curves in shades of red, from light to dark, start at high values near p=0 and decrease rapidly, approaching 0 as 'p' approaches 1. These curves represent the bias for fixed sampling with different numbers of samples (1, 2, 5, 10, 20, 50, and 100). A legend to the right of the graph identifies each curve.
>
> **Panel C:**
>
> - The y-axis is labeled "Standard deviation" and ranges from 0 to 3.
> - A blue curve starts at a high value near p=0 and decreases rapidly, approaching 0 as 'p' approaches 1. This represents the "IBS" method.
> - Several curves in shades of red, from light to dark, start at lower values near p=0, increase to a peak, and then decrease, approaching 0 as 'p' approaches 1. These curves represent the standard deviation for fixed sampling with different numbers of samples.

Figure 1: A. Number of samples used by fixed (red curves) or inverse binomial sampling (blue; expected value) to estimate the log-likelihood $\log p$ on a single trial with probability $p$. IBS uses on average $\frac{1}{p}$ trials. B. Bias of the log-likelihood estimate. The bias of IBS is identically zero. C. Standard deviation of the log-likelihood estimate.

The critical disadvantage of the fixed sampling policy with $M$ samples is that its estimates of the log-likelihood are inevitably biased (see Figure 1B). Fixed sampling is 'inevitably' biased because the bias decreases as one takes more samples, but for $p \rightarrow 0$, the estimator remains biased. More precisely, in a joint limit where $M \rightarrow \infty, p \rightarrow 0$ and $p M \rightarrow \lambda$ for some constant $\lambda$, the bias collapses onto a single 'master curve' (see Figure 2; and Appendix A. 2 for the derivation). In particular, we observe that the bias is close to zero for $\lambda \gg 1$ and that it diverges when $\lambda \ll 1$, or equivalently, for $M \gg \frac{1}{p}$ and $M \ll \frac{1}{p}$, respectively.

---

#### Page 18

> **Image description.** This image contains two line graphs, labeled A and B, comparing fixed sampling estimators.
>
> **Panel A:**
>
> - The graph is labeled "A" in the top left corner.
> - The y-axis is labeled "Bias" and ranges from 0 to 3.
> - The x-axis is labeled "pM" and ranges from 0 to 5.
> - Multiple lines are plotted on the graph, each representing a different fixed sampling value. The lines are colored in shades of red, ranging from a very light pink to a dark red.
> - A black line, labeled "Master curve," is also plotted.
> - A legend to the right of the graph identifies each line: "Fixed sampling: 1," "Fixed sampling: 2," "Fixed sampling: 5," "Fixed sampling: 10," "Fixed sampling: 20," "Fixed sampling: 50," "Fixed sampling: 100," and "Master curve."
> - All lines start at a high bias value when pM is close to 0, and decrease as pM increases. The master curve appears to have the highest initial bias.
>
> **Panel B:**
>
> - The graph is labeled "B" in the top left corner.
> - The y-axis is labeled "Standard deviation" and ranges from 0 to 0.6.
> - The x-axis is labeled "pM" and ranges from 0 to 5.
> - Similar to Panel A, multiple lines are plotted, representing different fixed sampling values, with the same color scheme (shades of red) and a black "Master curve" line.
> - The legend from Panel A applies here as well.
> - The lines generally start at 0, increase to a peak, and then decrease as pM increases. The master curve appears to have its peak at the lowest pM value.

Figure 2: A. Bias of fixed sampling estimators of the log-likelihood, plotted as a function of $p M$, where $p$ is the likelihood on a given trial, and $M$ the number of samples. As $M \rightarrow \infty$, the bias converges to a master curve (Equation S3). B. Same, but for standard deviation of the estimate.

To convey the intuition for why the bias diverges for small probabilities, we provide a gambling analogy. Imagine playing a slot machine and losing the first 100 bets you make. You can now deduce that this slot machine likely has a win rate less than $1 \%$, but there is no way of knowing whether it is $1 \%, 0.1 \%, 0.01 \%$ or even $0 \%$ apart from any prior beliefs you may have (for example, you expect that the house has stacked the odds in their favor but not overwhelmingly so). In practice, this uncertainty is unlikely to affect your decision whether to continue playing the slot machine, since the expected value of the slot machine depends linearly on its win rate. However, if your goal is to estimate the logarithm of the win rate, the difference between these percentages becomes infinitely large as the true win rate tends to 0 . We provide a more formal treatment of the bias of fixed sampling in Appendix A.2.

---

#### Page 19

# 3.1 Why fixed sampling cannot be fixed

The asymptotic analyses above suggest an obvious solution to prevent fixed sampling estimators from becoming strongly biased: make sure to draw enough samples so that $M \gg \max _{i=1 \ldots N} \frac{1}{p_{i}}$. Although this solution will succeed in theory, it has practical issues. First of all, choosing $M$ requires knowledge of $p_{i}$ on each trial, which is equivalent to the problem we set out the solve in the first place. Moreover, even if one can derive or estimate an upper bound on $\frac{1}{p_{i}}$ (for example, in behavioral models that include a lapse rate, that is a nonzero probability of giving a uniformly random response), fixed sampling will be inefficient. As shown in Figure 2, the bias in $\hat{\mathcal{L}}_{\text {fixed }}$ is small when $\lambda \approx 1$ or $M \approx \frac{1}{p}$ and increasing $M$ even further has diminishing returns, at least for the purpose of reducing bias. If we choose $M$ inversely proportional to the probability $p_{i}$ on the trial where the model is least likely to match the observed response, we will draw many more samples than necessary for all other trials.

One might hope that in practice the likelihood $p_{i}$ is approximately the same across trials, but the opposite is true. As an example, take a typical 'orientation discrimination' psychophysical task in which a participant has to detect whether a presented oriented grating is tilted clockwise or anti-clockwise from vertical, and consider a generative model for the observer's responses that includes sensory measurement noise and lapses (see Section 5.2 for details). Moreover, imagine that the experiment contains $\approx 500$ trials, and the participant's true lapse rate is $1 \%$. The model will always assign more probability to correct responses than errors, so, for all correct trials, $p_{i}$ will be at least 0.5 . However, there will likely be a handful of trials where the participant lapses and makes a grave error (responding incorrectly to a stimulus very far from the decision

---

#### Page 20

boundary), in which case $p_{i}$ will be 0.5 times the lapse rate. This hypothetical scenario is not exceptional, in fact it is almost inevitable in any experiment where participants occasionally make unpredictable responses, and perform hundreds or more trials.

A more sophisticated solution would relax the assumption that $M$ needs to be constant for all trials, and instead choose $M$ as a function of $p_{i}$ on each trial. However, since $p_{i}$ is unknown, one would need to first estimate $p_{i}$ by sampling, choose $M_{i}$ for each trial, then re-estimate $\mathcal{L}_{i}$. Such an iterative procedure would create a non-fixed sampling scheme, in which $M_{i}$ adapts to $p_{i}$ on each trial. This approach is promising, and it is, in fact, how we originally arrived at the idea of using inverse binomial sampling for log-likelihood estimation, while working on the complex cognitive model described in Section 5.4.

Finally, a heuristic solution would be to disregard any statistical concerns, pick $M$ based on some intuition or from similar studies in the literature, and hope that the bias turns out to be negligible. We do not intend to dissuade researchers from using such pragmatic approaches if they work in practice. Unfortunately, this one does not. As Figure 2 shows, estimating log-likelihoods with fixed sampling can cause biases of 1 or more points of model evidence if the data set contains even a single trial on which $p_{i} \leq$ $\frac{1}{2 M}$. Since differences in log-likelihoods larger than 5 to 10 points are often regarded as strong evidence for one model over another (Kass and Raftery, 1995; Jeffreys, 1998; Anderson and Burnham, 2002), it is well possible for such biases to reverse the outcome of a model comparison analysis. This point bears repeating; if one uses fixed sampling to estimate log-likelihoods and the number of samples is too low, one risks of drawing conclusions about scientific hypotheses that are not supported by the experimental data

---

#### Page 21

# 3.2 Why not an unbiased estimator of the likelihood?

In this paper, we focus on finding an unbiased estimator of the log-likelihood, but one might wonder why we do not look instead for an unbiased estimator of the likelihood. In fact, we already have such an estimator. Fixed sampling provides an unbiased estimate of $p_{i}$ for each trial, and since all estimates are unbiased and statistically independent, $\prod_{i} p_{i}$ is also unbiased.

The critical issue is the shape of the distribution of these estimates. While a central limit theorem for products of random variables exists, it only holds if all the estimates are almost surely not zero. For estimates of $p_{i}$ obtained via fixed sampling, this is not the case, and we would not obtain a well-behaved (i.e., log-normal) distribution of $\prod_{i} p_{i}$. In fact, the distribution would be highly multimodal with the main peak being at $\prod_{i} p_{i}=0$. This property makes this estimator unusable for all practical purposes (e.g., from maximum-likelihood estimation to Bayesian inference).

Instead, by switching to log-likelihood estimation, we can find an estimator (IBS) which is both unbiased and whose estimates are guaranteed to be well-behaved (in particular, normally distributed). We stress that normality is not just a desirable addition, but a fundamental feature with substantial practical consequences for how estimators are used, as we will see more in detail in the following section.

---

#### Page 22

# 4 Is inverse binomial sampling really better?

While one could expect that the unbiasedness of the IBS estimator would come at a cost, such as more samples, a much higher variance, or perhaps a particularly complex implementation, we show here that IBS is not only unbiased, but it is sample-efficient, its estimates are low-variance, and can be implemented in a few lines of code.

### 4.1 Implementation

We present in Algorithm 1 a description in pseudo-code of the basic IBS procedure to estimate the log-likelihood of a given parameter vector $\boldsymbol{\theta}$ for a given data set and generative model. The procedure is based on the inverse binomial sampling scheme introduced in Section 2.4, generalized sequentially to multiple trials.

For each trial, we draw sampled responses from the generative model, given the stimulus $\boldsymbol{s}_{i}$ in that trial, using the subroutine sample*from_model, until one matches the observed response $\boldsymbol{r}*{i}$. This yields a value of $K_{i}$ on each trial $i$, which IBS converts to an estimate $\hat{\mathcal{L}}_{i}$ (where we use the convention that a sum with zero terms equals 0 ). We make our way sequentially across all trials, returning then the summed log-likelihood estimate $\hat{\mathcal{L}}_{\text {IBS }}$ for the entire data set.

In practice, depending on the programming language of choice, it might be useful to take advantage of numerical features such as vectorization to speed up computations. An alternative 'parallel' implementation of IBS is described in Appendix C.1.

One might wonder how to choose the $\boldsymbol{\theta}$ to evaluate in Algorithm 1 in the first place. IBS is agnostic of how candidate $\boldsymbol{\theta}$ are proposed. Most often, the function that implements Algorithm 1 will be passed to a chosen optimization or inference algorithm,

---

#### Page 23

Algorithm 1 Inverse Binomial Sampling (sequential implementation)
Input: Stimuli $\left\{\boldsymbol{s}_{i}\right\}_{i=1}^{N}$, responses $\left\{\boldsymbol{r}_{i}\right\}_{i=1}^{N}$, generative model $\mathcal{M}$, parameters $\boldsymbol{\theta}$
for $i \leftarrow 1 \ldots N$ do
$\triangleright$ Sequential loop over all trials
$K_{i} \leftarrow 1$
while sample*from_model $\left(\mathcal{M}, \boldsymbol{\theta}, \boldsymbol{s}*{i}\right) \neq \boldsymbol{r}_{i}$ do
$K_{i} \leftarrow K*{i}+1$
$\hat{\mathcal{L}}*{i} \leftarrow-\sum*{k=1}^{K*{i}-1} \frac{1}{k}$
$\triangleright$ IBS estimator from Equation 14
6: return $\sum_{i=1}^{N} \hat{\mathcal{L}}_{i}$
$\triangleright$ Return total log-likelihood estimate
and the job of proposing $\boldsymbol{\theta}$ will be taken care of by the chosen method. For maximumlikelihood estimation, we recommend derivative-free optimization methods that deal effectively and robustly with noisy evaluations, such as Bayesian Adaptive Direct Search (BADS; Acerbi and Ma, 2017) or noise-robust CMA-ES (Hansen et al., 2003, 2008). At the end of optimization, most methods will then return a candidate solution $\widehat{\boldsymbol{\theta}}_{\text {MLE }}$ and possibly an estimate of the value of the target function at $\widehat{\boldsymbol{\theta}}_{\text {MLE }}$. However, since the target function is noisy and the final estimate is biased (because, by definition, it is better than the other evaluated locations), we recommend to re-estimate $\hat{\mathcal{L}}_{\text {IBS }}\left(\widehat{\boldsymbol{\theta}}_{\text {MLE }}\right)$ multiple times to obtain a higher-precision, unbiased estimate of the log-likelihood at $\widehat{\boldsymbol{\theta}}_{\text {MLE }}$ (see also Section 4.4).

Implementations of IBS in different programming languages can be found at the following web page: https://github.com/lacerbi/ibs.

---

#### Page 24

# 4.2 Computational time

The number of samples that IBS takes on a trial with probability $p_{i}$ is geometrically distributed with mean $\frac{1}{p_{i}}$. We saw earlier that for fixed-sampling estimators to be approximately unbiased, one needs at least $\frac{1}{p_{i}}$ samples, and IBS does exactly that in expectation. Moreover, since IBS adapts the number of samples it takes on different trials, it will be considerably more sample-efficient than fixed sampling with constant $M$ across trials. For example, in the aforementioned example of the orientation discrimination task, when most trials have a likelihood $p_{i} \geq 0.5$, IBS will often take just 1 or 2 samples on those trials. Therefore, it will allocate most of its samples and computational time on trials where $p_{i}$ is low and those samples are needed.

### 4.3 Variance

The variance of the IBS estimator can be derived as

$$
\operatorname{Var}\left[\hat{\mathcal{L}}_{\text {IBS }}\right]=\sum_{k=1}^{\infty} \frac{1}{k^{2}}(1-p)^{k}=\operatorname{Li}_{2}(1-p)
$$

where we introduced the dilogarithm or Spence's function $\operatorname{Li}_{2}(z)$ (Maximon, 2003). The variance (plotted in Figure 1C as standard deviation) increases when $p \rightarrow 0$, but it does not diverge; instead, it converges to $\frac{\pi^{2}}{6}$. Therefore, IBS is not only uniformly unbiased, but its variance is uniformly bounded. The full derivation of Equation 15 is reported in Appendix A.3.

We already mentioned that $\hat{\mathcal{L}}_{\text {IBS }}$ is the minimum-variance unbiased estimator of $\log p$ given the inverse binomial sampling policy, but it also comes close (less than $\sim 30 \%$ distance) to saturating the information inequality, which specifies the minimum

---

#### Page 25

variance that can be theoretically achieved by any estimator under a non-fixed sampling policy (an analogue of the Cramer-RÃ¡o bound; de Groot, 1959). We note that fixed sampling eventually saturates the information inequality in the limit $M \rightarrow \infty$, but as mentioned in the previous section, the fixed-sampling approach can be highly wasteful or substantially biased (or both), not knowing a priori how large $M$ has to be across trials. See Appendix A. 4 for a full discussion of the information inequality and comparison between estimators.

Equation 15 has theoretical relevance, but requires us to know the true value of the likelihood $p$, which is unknown in practice. Instead, we define the estimator of the variance of a specific IBS estimate, having sampled for $K$ times until a 'hit', as

$$
\operatorname{Var}\left[\hat{\mathcal{L}}_{\text {IBS }} \mid K\right]=\psi_{1}(1)-\psi_{1}(K)
$$

where $\psi_{1}(z)$ is the trigamma function, the derivative of the digamma function (Abramowitz and Stegun, 1948). We derived Equation 16 from a Bayesian interpretation of the IBS estimator, which can be found in Appendix A.5. Note that Equations 15 and 16 correspond to slightly different concepts, in that the former represents the variance of the estimator for a known $p$ (from a frequentist point of view), while the latter is the posterior variance of $\mathcal{L}$ for a known $K$ (for which there is no frequentist analogue). See also Section 4.6 for further discussion.

# 4.4 Iterative multi-fidelity

We define a multi-fidelity estimator as an estimator with a tunable parameter that affords different degrees of precision at different computational costs (i.e., from a cheaper, inaccurate estimate to a very accurate but expensive one), borrowing the term from

---

#### Page 26

the literature on computer simulations and surrogate models (Kennedy and O'Hagan, 2000; Forrester et al., 2007). IBS provides a particularly convenient way to construct an iterative multi-fidelity estimator in that we can perform $R$ independent 'repeats' of the IBS estimate at $\boldsymbol{\theta}$, and combine them by averaging,

$$
\begin{aligned}
\hat{\mathcal{L}}_{\text {IBS- } R}(\boldsymbol{\theta}) & =\frac{1}{R} \sum_{r=1}^{R} \hat{\mathcal{L}}_{\text {IBS }}^{(r)}(\boldsymbol{\theta}) \\
\operatorname{Var}\left[\hat{\mathcal{L}}_{\text {IBS- } R}(\boldsymbol{\theta})\right] & =\frac{1}{R^{2}} \sum_{r=1}^{R} \operatorname{Var}\left[\hat{\mathcal{L}}_{\text {IBS }}^{(r)}(\boldsymbol{\theta})\right]
\end{aligned}
$$

where $\hat{\mathcal{L}}_{\text {IBS }}^{(r)}$ denotes the $r$-th independent estimate obtained via IBS. For $R=1$, we recover the standard ('1-rep') IBS estimator. The variances in Equation 17 are computed empirically using the estimator in Equation 16.

Importantly, we do not need to perform all $R$ repeats at the same time, but we can iteratively refine our estimates whenever needed, and only need to store the current estimate, its variance and the number of repeats performed so far:

$$
\begin{aligned}
\hat{\mathcal{L}}_{\text {IBS- } R+1}(\boldsymbol{\theta}) & =\frac{1}{R+1}\left[R \cdot \hat{\mathcal{L}}_{\text {IBS- } R}(\boldsymbol{\theta})+\hat{\mathcal{L}}_{\text {IBS }}^{(r+1)}(\boldsymbol{\theta})\right] \\
\operatorname{Var}\left[\hat{\mathcal{L}}_{\text {IBS- } R+1}(\boldsymbol{\theta})\right] & =\frac{1}{(R+1)^{2}}\left\{R^{2} \cdot \operatorname{Var}\left[\hat{\mathcal{L}}_{\text {IBS- } R}(\boldsymbol{\theta})\right]+\operatorname{Var}\left[\hat{\mathcal{L}}_{\text {IBS }}^{(r+1)}(\boldsymbol{\theta})\right]\right\}
\end{aligned}
$$

Crucially, while a similar procedure could be performed with any estimator (including fixed sampling), the fact that IBS is unbiased and its variance is bounded ensures that the combined iterative estimator is also unbiased and eventually converges to the true value for $R \rightarrow \infty$, with variance bounded above by $\frac{\pi^{2}}{6 R}$.

Finally, we note that the iterative multi-fidelity approach described in this section can be extended such that, instead of having the same number of repeats $R$ for all trials, one could adaptively allocate a different number of repeats $R_{i}$ to each trial so as to minimize the overall variance of the estimated log-likelihood (see Appendix C.2).

---

#### Page 27

# 4.5 Bias or variance?

In the previous sections, we have seen that IBS is always unbiased, whereas fixed sampling can be highly biased when using too few samples. However, with the right choice of $M$, fixed sampling can have lower variance. We now list several practical and theoretical arguments for why bias can have a larger negative impact than variance, and being unbiased is a desirable property for an estimator of the log-likelihood.

1. To use IBS or fixed sampling to estimate the log-likelihood of a given data set, we sum estimates of $\mathcal{L}_{i}$ across trials. Being the sum of independent random variables, as $N \rightarrow \infty$, the standard deviation of $\hat{\mathcal{L}}(\boldsymbol{\theta})$ will grow proportional to $\sqrt{N}$, whereas the bias grows linearly with $N$. For a concrete example, see Appendix A.6.
2. When using the log-likelihood (or a derived metric) for model selection, it is common to collect evidence for a model, possibly hierarchically, across multiple datasets (e.g., different participants in a behavioral experiment), which provides a second level of averaging that can reduce variance but not bias.
3. Besides model selection, the other key reason to estimate log-likelihoods is to infer parameters of a model, for example via maximum-likelihood estimation. For this purpose, one would use an optimization algorithm that calls the routine that estimates $\hat{\mathcal{L}}(\boldsymbol{\theta})$ many times with different candidate values of $\boldsymbol{\theta}$, and uses this information to estimate the value that maximizes $\mathcal{L}(\boldsymbol{\theta})$. Powerful, sample-efficient optimization algorithms, such as those based on Bayesian optimization, work by building a statistical approximation (a surrogate) of the objective function (Jones

---

#### Page 28

et al., 1998; Snoek et al., 2012; Shahriari et al., 2015; Acerbi and Ma, 2017), most commonly via Gaussian processes (Rasmussen and Williams, 2006). These methods can operate successfully with noisy objectives by effectively averaging function values from nearby parameter vectors. By contrast, no optimization algorithm can handle bias. This argument is not limited to maximum-likelihood estimation, as recent methods have been proposed to use Gaussian process surrogates to perform (approximate) Bayesian inference and infer posterior distributions (Kandasamy et al., 2015; Acerbi, 2018, 2020; JÃ¤rvenpÃ¤Ã¤ et al., 2019); also these techniques can handle variance in the estimates but not bias. 4. The ability to combine unbiased estimates of known variance iteratively (as described in Section 4.4) is particularly useful with adaptive fitting methods based on Gaussian processes, whose algorithmic cost grows super-linearly in the number of distinct training points (Rasmussen and Williams, 2006). Thanks to iterative multi-fidelity estimation, these methods would have the opportunity to refine their estimates of the log-likelihood at a previously evaluated point, whenever deemed useful, without incurring an increased algorithmic cost. 5. On a conceptual level, bias is potentially more dangerous than variance. Bias can cause researchers to confidently draw false conclusions, variance, when properly accounted for, causes decreased statistical power and lack of confidence. Appropriate statistical tools can account for variance and explain seemingly conflicting findings resulting from underpowered studies (Maxwell et al., 2015), whereas bias is much harder to recognize or correct no matter what statistical techniques

---

#### Page 29

one uses.

Finally, while for the sake of argument we structured this section as an opposition between bias and variance, it is not an exact dichotomy and both properties matter, together with other considerations (see Appendix A.6). For example, there are situations in which the researcher may knowingly decide to increase bias to reduce both variance and computational costs. Notably, while this trade-off is easy to achieve with the IBS estimator (see Appendix C.1), there is no similarly easy technique to de-bias a biased estimator.

# 4.6 Higher-order moments

So far, we have considered the mean (or bias) and variance of $\hat{\mathcal{L}}_{\text {fixed }}$ and $\hat{\mathcal{L}}_{\text {IBS }}$ in detail, but ignored any higher-order moments. This is justified since to estimate the loglikelihood of a model with a given parameter vector we will sum these estimates across many trials. Therefore, the central limit theorem guarantees that the distribution of $\mathcal{L}(\boldsymbol{\theta})$ is Gaussian with mean and variance determined by the mean and variance of $\hat{\mathcal{L}}_{\text {fixed }}$ or $\hat{\mathcal{L}}_{\text {IBS }}$ on each trial, at least as long as the distribution of $p_{i}$ across trials satisfies a regularity condition. ${ }^{2}$ A sufficient but far from necessary condition is that there exists a lower bound on $p_{i}$, which is the case for example for a behavioral model with a lapse rate. Using the same argument, the total number of samples $K_{\text {tot }}$ that IBS uses to estimate $\mathcal{L}(\theta)$ is also approximately Gaussian.

[^0]
[^0]: ${ }^{2}$ Specifically, the Lindeberg (1922) or Lyapunov conditions (Ash et al., 2000, Chapter 7.3), both of which place restrictions on the degree to which the variance of any single trial can dominate the distribution of $\sum_{i} \hat{\mathcal{L}}_{i}$.

---

#### Page 30

> **Image description.** The image contains three similar plots, labeled A, B, and C, each displaying a histogram overlaid with a curve. All three plots share the same axis labels and similar data distributions.
>
> - **Overall Structure:** The image is divided into three panels, labeled A, B, and C from left to right. Each panel contains a histogram and a superimposed curve.
>
> - **Axes:**
>
>   - The horizontal axis in each plot is labeled "Z-score" and ranges from approximately -3 to 3, with tick marks at -2.5, 0, and 2.5.
>   - The vertical axis in each plot is labeled "Density" and ranges from 0 to 0.4, with tick marks at 0, 0.2, and 0.4.
>
> - **Histograms:** Each plot features a histogram composed of many narrow, vertical blue bars. The histograms are centered around 0 on the Z-score axis, forming a bell-shaped distribution.
>
> - **Curves:** A smooth, dark gray curve is overlaid on each histogram. The curve also has a bell shape and closely follows the distribution of the histogram.
>
> - **Legend:** In plot A, there is a legend in the upper-left corner. It indicates that the gray curve represents the "Standard normal pdf" and the blue bars represent the "Empirical distribution."
>
> - **Panel Labels:** The panels are labeled with the letters A, B, and C in the top-left corner of each panel.

Figure 3: A. $z$-score plot for the total number of samples used by IBS. B. $z$-score plot for the estimates returned by IBS, using the exact variance formula for known probability. C. Calibration plot for the estimates returned by IBS, using the variance estimate from Equation 16. These figures show that the number of samples taken by IBS and the estimated log-likelihood are (approximately) Gaussian, and that the variance estimate from Equation 16 is calibrated.

In the following, we demonstrate empirically that the distributions of the number of samples taken by IBS and of the estimates $\hat{\mathcal{L}}_{\text {IBS }}$ are Gaussian. Importantly, we also show that the estimate of the variance from Equation 16, $\hat{V}_{\text {IBS }}$, is calibrated. That is, we expect the fraction of estimates within the credible interval $\hat{\mathcal{L}}_{\text {IBS }} \pm \beta \sqrt{\hat{V}_{\text {IBS }}}$ to be (approximately) $\Phi(\beta)-\Phi(-\beta)$, where $\Phi(x)$ is the cumulative normal distribution function and $\beta>0$.

As a realistic scenario, we consider the psychometric function model described in Section 5.2. For each simulated data set, we estimated the log-likelihood under the true data-generating parameters $\boldsymbol{\theta}_{\text {true }}$ (see Appendix B. 1 for details). Specifically, for each data set we ran IBS and recorded the estimated log-likelihood $\hat{\mathcal{L}}_{\text {IBS }}$, the total number of samples $K_{\text {tot }}$ taken, and a Bayesian estimate for the variance of $\hat{\mathcal{L}}_{\text {IBS }}$ from Equation 16.

---

#### Page 31

For the total number of samples $K_{\text {tot }}$ and the $\hat{\mathcal{L}}_{\text {IBS }}$ estimate, we can compute the theoretical mean and variance by knowing the trial likelihoods $p_{i}$, which we can evaluate exactly in this example.

For each obtained $K_{\text {tot }}$, we computed a $z$-score by subtracting the exact mean and dividing by the exact standard deviation, obtained by knowing the mean and variance of geometric random variables underlying the samples taken in each trial. If $K_{\text {tot }}$ is normally distributed, we expect that the variable $z$ across data sets should appear to be distributed as a standard normal, $z \sim \mathcal{N}(0,1)$. If $K_{\text {tot }}$ is not normally distributed, we should see deviations from normality in the distribution of $z$, especially in the tails. By comparing the histogram of $z$-scores with a standard normal in Figure 3A, we see that the total number of samples is approximately normal, with some residual skew.

We did the same analysis for the estimate $\hat{\mathcal{L}}_{\text {IBS }}$, using the $z$-scored variable

$$
z \equiv \frac{\hat{\mathcal{L}}_{\mathrm{IBS}}-\mathcal{L}_{\text {true }}}{\sqrt{\operatorname{Var}\left[\hat{\mathcal{L}}_{\mathrm{IBS}}\right]}}
$$

where here $\operatorname{Var}\left[\hat{\mathcal{L}}_{\text {IBS }}\right]$ is the exact variance of the estimator computed via Equation 15. The histogram of $z$-scores in Figure 3B is again very close to a standard normal.

Finally, in practical scenarios we do not know the true likelihoods, so the key question is whether we can obtain valid estimates of the variance of $\hat{\mathcal{L}}_{\text {IBS }}$ via Equation 16. If such an estimate is correctly calibrated, the distribution of $z$-scores should remain approximately Gaussian if we use Equation 16 for the denominator of Equation 19. Indeed, the calibration plot in Figure 3C shows an excellent match with a standard normal, confirming that our proposed estimator of the variance is well calibrated.

---

#### Page 32

# 5 Numerical experiments

In this section, we examine the performance of IBS and fixed sampling on several realistic model-fitting problems of increasing complexity. The example problems we consider here model tasks drawn from psychophysics and cognitive science: an orientation discrimination experiment (Section 5.2); a change localization task (Section 5.3); and playing a four-in-a-row game that involves complex sequential decision making (Section 5.4). For the first problem, we can derive the exact analytical expression for the log-likelihood; for the second problem, we have an integral expression for the loglikelihood that we can approximate numerically; and finally, for the third problem, we are in the true scenario in which the log-likelihood is intractable.

The rationale for our numerical experiments is that so far we have analyzed fixed sampling and IBS in terms of the bias in their log-likelihood estimates for individual parameter vectors. However, these log-likelihood estimators are often used as elements of a more complex statistical procedure, such as maximum-likelihood estimation. It is plausible that biases in log-likelihood estimates will lead to biases in parameter estimates obtained by maximizing the log-likelihood, but the exact relationship between those biases, and the role of variance in optimization is not immediate. Similarly, it is unclear how bias and variance of individual log-likelihood estimates will affect the estimate of the maximum log-likelihood, often used for model selection (e.g., unbiased estimates of the log-likelihood do not guarantee an error-free estimate of the maximum log-likelihood, which is affected by other factors; see Section 5.5). Therefore, we conduct an empirical study showing that, in practice, IBS leads to more accurate parameter and maximum log-likelihood estimates than fixed sampling, given the same budget of

---

#### Page 33

computational resources.

First, we describe the procedure used to perform our numerical experiments. Code to run all our numerical experiments and analyses is available at the following link: https://github.com/basvanopheusden/ibs-development.

# 5.1 Procedure

For each problem, we simulate data from the generative model given different known settings $\boldsymbol{\theta}_{\text {true }}$ of model parameters, and we compare the accuracy (and other statistics) of both IBS and fixed sampling in recovering the true data-generating parameters through maximum-likelihood estimation. Since these methods provide noisy and possibly biased estimates of $\mathcal{L}(\boldsymbol{\theta})$, and due to variability in the simulated datasets, the estimates $\widehat{\boldsymbol{\theta}}_{\text {MLE }}$ that result from optimizing the log-likelihood will also be noisy and possibly biased. To explore performance in a variety of settings, and to account for variability in the data-generation process, for each problem we consider $40 \cdot D$ different parameter settings, where $D$ is the number of model parameters (that is, the dimension of $\boldsymbol{\theta}$ ), and for each parameter setting we generate 100 distinct synthetic datasets.

For each dataset, we compare fixed sampling with different numbers of samples $M$ (from $M=1$ to $M=50$ or $M=100$, depending on the problem), and IBS with different number of 'repeats' $R$, as defined in Section 4.4 (from $R=1$ to up to $R=50$, depending on the problem). In each scenario, we directly compare the two methods in terms of number of samples by computing the average number of samples used by IBS for a given number of repeats $R$. To prevent IBS from 'hanging' on particularly bad parameter vectors, we adopt the 'early stopping threshold' technique described in Ap-

---

#### Page 34

pendix C.1. Finally, if available, we also test the performance of maximum-likelihood estimation using the 'exact' log-likelihood function (calculated either analytically or via numerical integration).

For all methods, we maximize the log-likelihood with Bayesian Adaptive Direct Search (BADS, Acerbi and Ma, 2017; github.com/lacerbi/bads), a hybrid Bayesian optimization algorithm based on the mesh-adaptive direct search framework (Audet and Dennis Jr, 2006), which affords a fast, robust exploration of the function landscape via Gaussian process surrogates. Briefly, BADS works by alternating between two stages: in the Poll stage, the algorithm evaluates points in a random mesh surrounding the current point, in a fairly model-free way; in the Search stage, following the principles of Bayesian optimization (Jones et al., 1998), the algorithm builds a local Gaussian process model of the target function, and chooses the next point by taking into account both mean and variance of the surrogate model, balancing exploration of unknown but promising regions and exploitation of regions known to be high-valued (for maximization). By combining model-free and powerful model-based search, BADS has been shown to be much more effective than alternative optimization methods particularly when dealing with stochastic objective functions, and with a relatively limited budget of a few hundreds to a few thousands function evaluations (Acerbi and Ma, 2017). We refer the interested reader to Acerbi and Ma (2017) and to the extensive online documentation for further information about the algorithm.

---

#### Page 35

# 5.2 Orientation discrimination

The first task we simulate is an orientation discrimination task, in which a participant observes an oriented patch on a screen, and indicates whether they believe it was rotated leftwards or rightwards with respect to a reference line (see Figure 4A). Here, on each trial the stimulus $s$ is the orientation of the patch with respect to the reference (in degrees), and the response $r$ is 'rightwards' or 'leftwards'.

For each dataset, we simulated $N=600$ trials, drawing on each trial the stimulus $s$ from a Gaussian distribution with mean $0^{\circ}$ (the reference) and standard deviation $3^{\circ}$. The generative model assumes that the observer makes a noisy measurement $x$ of the stimulus, which is normally distributed with mean $s$ and standard deviation $\sigma$, as per standard signal detection theory (Green and Swets, 1966). They then respond 'rightwards' if $x$ is larger than $\mu$ (a parameter which captures response bias, or an incorrect memory of the reference line) and 'leftward' otherwise. However, a fraction of the time, given by the lapse rate $\gamma \in(0,1]$, the observer guesses randomly. We visually illustrate the model in Figure 4B. For both theoretical reasons and numerical convenience, we parametrize the slope $\sigma$ as $\eta \equiv \log \sigma$. Thus, the model has parameter vector $\boldsymbol{\theta}=(\eta, \mu, \gamma)$.

We can derive the likelihood of each trial analytically:

$$
\operatorname{Pr}\left(\text { 'rightwards' response} \mid s, \boldsymbol{\theta}\right)=\frac{\gamma}{2}+(1-\gamma) \Phi\left(\frac{s-\mu}{\sigma}\right)
$$

where $\Phi(x)$ is the cumulative normal distribution function. Equation 20 takes the form of a typical psychometric function (Wichmann and Hill, 2001). Note that in this section we use Gaussian distributions for circularly distributed variables, which is justified

---

#### Page 36

under the assumption that both the stimulus distribution and the measurement noise are small. For more details about the numerical experiments, see Appendix B.1.

> **Image description.** The image is composed of two panels, labeled A and B, each depicting different aspects of an orientation discrimination task.
>
> Panel A illustrates the trial structure of the task. It shows a sequence of three gray squares, each representing a stage in the trial. The first square, labeled "1500 ms," contains a small black cross in the center. The second square, labeled "250 ms," displays a Gabor patch (a sinusoidal grating with a Gaussian envelope) tilted slightly. The third square, labeled "Until response," contains the text "left or right?". This panel visually represents the sequence of events in a single trial: a fixation period, stimulus presentation, and response prompt.
>
> Panel B presents a graph of the behavioral model. The x-axis is labeled "Orientation (deg)" and ranges from -3 to 3. The y-axis is labeled "Pr(respond rightwards)" and ranges from 0 to 1. A sigmoid curve is plotted on the graph, representing the probability of responding "rightwards" as a function of the stimulus orientation. Three parameters of the curve are visually annotated: "Ï" indicates the (inverse) slope of the curve near the center, "Î¼" indicates the horizontal offset, and "Î³" indicates the (double) asymptote of the curve. Dashed lines are used to indicate the slope at the inflection point.

Figure 4: A. Trial structure of the simulated orientation discrimination task. A oriented patch appears on a screen for 250 ms , after which participants decide whether it is rotated rightwards or leftwards with respect to a vertical reference. B. Graphical illustration of the behavioral model, which specifies the probability of choosing rightwards as a function of the true stimulus orientation. The three model parameters $\sigma$, $\mu$, and $\gamma$ correspond to the (inverse) slope, horizontal offset and (double) asymptote of the psychometric curve, as per Equation 20. Note that we parametrize the model with $\eta \equiv \log \sigma$.

In Figure 5, we show the parameter recovery using fixed sampling, IBS and the exact log-likelihood function from Equation 20. First, we show that IBS can estimate the sensory noise parameter $\eta$ and lapse rate $\gamma$ more accurately than fixed sampling while using on average the same or fewer samples (Figure 5A,D). For visualization purposes, we show here a representative example with $R=1$ or $R=3$ repeats of IBS and $M=10$ or $M=20$ fixed samples (see Figure S6 in the Appendix for the plots with all tested values of $R$ and $M$ ). As baseline, we also plot the mean and standard deviation

---

#### Page 37

of exact maximum-likelihood estimation, which is imperfect due to the finite data size (600 trials), and stochasticity and heuristics used in the optimization algorithm. We omit results for estimates of the response bias $\mu$, since even fixed sampling can match the performance of exact MLE with only 1 sample per trial.

Next, we fix $\eta_{\text {true }} \equiv \log \sigma_{\text {true }}=\log 2^{\circ}, \mu_{\text {true }}=0.1^{\circ}, \gamma_{\text {true }}=0.1$ and plot the mean and standard deviation of the estimated $\hat{\eta}$ and $\hat{\gamma}$ across 100 simulated data sets as a function of the (average) number of samples per trial used by IBS or fixed sampling (Figure 5B,E). We find that fixed sampling is highly sensitive to the number of samples, and with less than 20 samples per trial, its estimate of $\eta$ is strongly biased. Estimating $\gamma$ accurately remains unattainable even with 100 samples per trial. By contrast, IBS estimates $\eta$ and $\gamma$ reasonably accurately regardless of the number of samples per trial. IBS has a slight tendency to underestimate $\gamma$, which is a result of an interaction of the uncertainty handling in BADS with our choice of model parametrization and parameter bounds. In general, estimating lapse rates is notoriously prone to biases (Prins, 2012).

Finally, we measure the root mean squared error (RMSE) of IBS, fixed sampling and the exact solution, averaged across all simulated data sets, as a function of number of samples per trial (Figure 5C,F). This analysis confirms the same pattern: fixed sampling makes large errors in estimating $\eta$ with fewer than 20 samples, and for $\gamma$ it requires as many as 100 samples per trial to become approximately unbiased. IBS outperforms fixed sampling for both parameters and any number of samples, and even with as few as 2 or 3 repeats comes close to matching the RMSE of exact maximum-likelihood inference.

---

#### Page 38

> **Image description.** The image contains six plots arranged in a 2x3 grid, labeled A through F. All plots are graphs comparing different estimation methods.
>
> **Panel A:**
>
> - This is a scatter plot.
> - The x-axis is labeled "Î·".
> - The y-axis is labeled "Î®".
> - Data is plotted for three methods: "exact" (green with error bars), "ibs 2.22" (blue with error bars), and "fixed 10" (red with error bars).
> - A black line representing equality is also plotted.
>
> **Panel B:**
>
> - This is a line graph.
> - The x-axis is labeled "Number of samples".
> - The y-axis is labeled "Î®".
> - Data is plotted for "exact" (green), "IBS" (blue), and "fixed" (red).
> - A dashed black line labeled "true" is also plotted.
>
> **Panel C:**
>
> - This is a line graph.
> - The x-axis is labeled "Number of samples".
> - The y-axis is labeled "RMSE (Î·)".
> - Data is plotted for "exact" (green), "fixed" (red), and "IBS" (blue).
>
> **Panel D:**
>
> - This is a scatter plot.
> - The x-axis is labeled "Î³".
> - The y-axis is labeled "Å·".
> - Data is plotted for "exact" (green with error bars), "ibs 6.49" (blue with error bars), and "fixed 20" (red with error bars).
> - A black line representing equality is also plotted.
>
> **Panel E:**
>
> - This is a line graph.
> - The x-axis is labeled "Number of samples".
> - The y-axis is labeled "Å·".
> - Data is plotted for "IBS" (blue), and "fixed" (red) with shaded regions representing standard error.
> - A dashed black line is also plotted.
>
> **Panel F:**
>
> - This is a line graph.
> - The x-axis is labeled "Number of samples".
> - The y-axis is labeled "RMSE (Î³)".
> - Data is plotted for "exact" (green), "fixed" (red), and "IBS" (blue) with shaded regions representing standard error.

Figure 5: A. Estimated values of $\eta \equiv \log \sigma$ as a function of the true $\eta$ in simulated data using IBS with $R=1$ repeat (blue), fixed sampling with $M=10$ (red) or the exact likelihood function (green). The black line denotes equality. Error bars indicate standard deviation across 100 simulated data sets. IBS uses on average 2.22 samples per trial. B. Mean and standard error (shaded regions) of estimates of $\eta$ for 100 simulated data sets with $\eta_{\text {true }}=\log 2^{\circ}$, using fixed sampling, IBS or the exact likelihood function. For fixed sampling and IBS, we plot mean and standard error as a function of the (average) number of samples used. C. Root mean squared error (RMSE) of estimates of $\eta$, averaged across the range of $\eta_{\text {true }}$ in $\mathbf{A}$, as a function of the number of samples used by IBS or fixed sampling. Shaded regions denote $\pm 1$ standard deviation across the 100 simulated data sets. We also plot the RMSE of exact maximum-likelihood estimation, which is nonzero since we simulated data sets with only 600 trials. D-F Same, for $\gamma$ (with $R=3, M=20$ in panel $\mathbf{D}$ ). These results demonstrate that IBS estimates parameters of the model for orientation discrimination more accurately than fixed sampling using equally many or even fewer samples.

---

#### Page 39

# 5.3 Change localization

The second problem we consider is a typical 'change localization' task (see Figure 6A), in which participants observe a display of 6 oriented patches, and after a short interstimulus interval, a second display of 6 patches (Van den Berg et al., 2012). Of these patches, 5 are identical between displays and one denoted by $c \in\{1, \ldots, 6\}$ will have changed orientation. The participant responds by indicating which patch they believe changed orientation. Here, on each trial the stimulus $s$ is a vector of 12 elements corresponding to a vector of orientations (in degrees) of the six patches in the first display, concatenated with the vector of orientation of the six patches in the second display. The response $r \in\{1, \ldots, 6\}$ is the patch reported by the participant.

For each dataset, we simulated $N=400$ trials. On each trial, the patches on the first display are all independently drawn from a uniform distribution Uniform[0,360]. For the second display, we randomly select one of the patches and change its orientation by an amount drawn from a von Mises distribution centered at $0^{\circ}$ with concentration parameter $\kappa_{\mathrm{s}}=1$. A von Mises distribution is the equivalent of a Gaussian distribution in circular space, and the concentration parameter is monotonically related to the precision (inverse variance) of the distribution. Note that, for mathematical convenience (but without loss of generality) we assume that patch orientations are defined on the whole circle, whereas in fact they are defined on the half-circle $\left[0^{\circ}, 180^{\circ}\right)$.

The generative model assumes that participants independently measure the orientation of each patch in both displays. For each patch, the measurement distribution is a von Mises centered on the true orientation with concentration parameter $\kappa$, representing sensory precision. The participant then selects the patch for which the absolute circular

---

#### Page 40

difference of the measurements between the first and second display is largest. This model too includes a lapse rate $\gamma \in(0,1]$, the probability with which the participant guesses uniformly randomly across responses.

Since thinking in terms of concentration parameter is not particularly intuitive, we reparametrize participants' sensory noise as $\eta \equiv \log \sigma \equiv-\frac{1}{2} \log \kappa$, since in the limit $\kappa \gg 1$, the von Mises distribution with concentration parameter $\kappa$ tends to a Gaussian distribution with standard deviation $\sigma=\frac{1}{\sqrt{\kappa}}$. The model has then two parameters, $\boldsymbol{\theta}=(\eta, \gamma)$.

We can express the trial likelihood for the change localization model in an integral form that does not have a known analytical solution (see Appendix B. 2 for a derivation). We can, however, evaluate the integral numerically, which can take a few seconds for a high-precision likelihood evaluation across all trials in a dataset. The key quantity in the computation of the trial likelihood is $\Delta_{s}^{(c)}$, the difference in orientation between the changed stimulus at position $c$ between the first and second display. We plot the probability of a correct response, $P_{\text {correct }}\left(\Delta_{s}^{(c)} ; \boldsymbol{\theta}\right)$, as a function of $\Delta_{s}^{(c)}$ in Figure 6B. As expected, the probability of a correct response increases monotonically with the amount of change, with the slope being modulated by sensory noise and the asymptote by the lapse rate (but also by the sensory noise, for large noise, as we will discuss later). For more details about the numerical experiments, see Appendix B.2.

In Figure 7, we compare the performance of IBS, fixed sampling and the 'exact' log-likelihood evaluated through numerical integration. As before, IBS estimates both $\eta$ and $\gamma$ more accurately with fewer samples than fixed sampling (Figure 7A,D). As an example, we show IBS with $R=1$ repeats and fixed sampling with $M=20$ or

---

#### Page 41

> **Image description.** The image is composed of two panels, labeled A and B.
>
> Panel A depicts a sequence of four gray squares, each representing a stage in a visual experiment. Each square contains a fixation cross in the center. The first square is labeled "1500 ms". The second square, labeled "250 ms", contains six oriented patches arranged around the fixation cross. The third square, labeled "1000 ms", again shows only the fixation cross. The fourth square, labeled "Until response", shows the same arrangement of six oriented patches as the second square, with one patch highlighted by a mouse cursor.
>
> Panel B is a line graph. The x-axis is labeled "Orientation difference (deg)" and ranges from 0 to 90. The y-axis is labeled "Pr(correct)" and ranges from 0 to 1. Two curves are plotted on the graph. The upper curve is labeled "Ï = 11.5Â°", and the lower curve is labeled "Ï = 18.1Â°". A vertical double-headed arrow labeled "Î³" is positioned at the right side of the graph, indicating the difference between the asymptotes of the two curves.

Figure 6: A. Trial structure of the simulated change localization task. While the participant fixates on a cross, 6 oriented patches appear for 250 ms , disappear and the re-appear after a delay. In the second display, one patch will have changed orientation, in this example the top left. The participant indicates with a mouse click which patch they believe changed. B. The generative model is fully characterized by the proportion correct as function of model parameters and circular distance between the orientations of the changed patch in its first and second presentation (see text). Here we plot this curve for two values of $\eta \equiv \log \sigma$. In both curves, $\gamma=0.2$. We can read off $\eta$ from the slope and $\gamma$ from the asymptote.

---

#### Page 42

$M=50$; the full results with all tested values of $R$ and $M$ are reported in Figure S7 in the Appendix.

Interestingly, maximum-likelihood estimation via the 'exact' method provides biased estimates of $\eta$ when the noise is high. This is because sensory noise and lapse become empirically non-identifiable for large $\eta$, as large noise produces a nearly-flat response distribution, which is indistinguishable from lapse. For these particular settings of $\boldsymbol{\theta}_{\text {true }}$, due to the interaction between noisy log-likelihood evaluations and the optimization method, IBS and fixed sampling perform better at recovering $\eta$ than the 'exact' method, but it does not necessarily hold true in general. Issues of identifiability can be amelioriated by using Bayesian inference instead of maximum-likelihood estimation (Acerbi et al., 2014).

In Figure 7B,E, we show the estimates of fixed sampling and IBS for simulated data with $\eta_{\text {true }} \equiv \log \sigma_{\text {true }}=\log 17.2^{\circ}$ and $\gamma_{\text {true }}=0.1$, and find that fixed sampling substantially underestimates $\eta$ when using less then 50 samples, and underestimates $\gamma$ even with 100 samples per trial. By contrast, IBS produces parameter estimates with relatively little bias and standard deviation close to that of exact maximum-likelihood estimation. Finally, in Figure 7C,F we show that IBS has lower RMSE than fixed sampling for both parameters when compared on equal terms of number of samples.

# 5.4 Four-in-a-row game

The third problem we examine is a complex sequential decision-making task, a variant of tic-tac-toe in which two players compete to place 4 pieces in a row, column or diagonal on a 4-by-9 board (see Figure 8A). In previous work, van Opheusden et al. (2016)

---

#### Page 43

> **Image description.** This image contains six plots arranged in a 2x3 grid, labeled A through F. Each plot displays data related to parameter estimation, comparing different methods: "exact," "IBS," and "fixed."
>
> **Panel A:**
>
> - This is a scatter plot.
> - The x-axis is labeled "Î·" (eta).
> - The y-axis is labeled "Î·Ì" (eta hat).
> - Three data series are plotted:
>   - "exact" (green) with error bars
>   - "ibs 6.19" (blue) with error bars
>   - "fixed 20" (red) with error bars
>
> **Panel B:**
>
> - This is a line graph.
> - The x-axis is labeled "Number of samples."
> - The y-axis is labeled "Î·Ì" (eta hat).
> - Four data series are plotted:
>   - "exact" (green)
>   - "IBS" (blue)
>   - "fixed" (red)
>   - "true" (black dashed line)
>
> **Panel C:**
>
> - This is a line graph.
> - The x-axis is labeled "Number of samples."
> - The y-axis is labeled "RMSE (Î·)" (Root Mean Squared Error of eta).
> - Three data series are plotted:
>   - "exact" (green)
>   - "fixed" (red)
>   - "IBS" (blue)
> - A green horizontal bar is present, likely indicating a target RMSE value for the "exact" method.
>
> **Panel D:**
>
> - This is a scatter plot.
> - The x-axis is labeled "Î³" (gamma).
> - The y-axis is labeled "Î³Ì" (gamma hat).
> - Three data series are plotted:
>   - "exact" (green) with error bars
>   - "ibs 13.42" (blue) with error bars
>   - "fixed 50" (red) with error bars
>
> **Panel E:**
>
> - This is a line graph.
> - The x-axis is labeled "Number of samples."
> - The y-axis is labeled "Î³Ì" (gamma hat).
> - Four data series are plotted:
>   - "exact" (green)
>   - "IBS" (blue) with a shaded area around the line
>   - "fixed" (red) with a shaded area around the line
>   - "true" (black dashed line)
>
> **Panel F:**
>
> - This is a line graph.
> - The x-axis is labeled "Number of samples."
> - The y-axis is labeled "RMSE (Î³)" (Root Mean Squared Error of gamma).
> - Three data series are plotted:
>   - "exact" (green)
>   - "fixed" (red) with a shaded area around the line
>   - "IBS" (blue) with a shaded area around the line
> - A green horizontal bar is present, likely indicating a target RMSE value for the "exact" method.

Figure 7: Same as Figure 5, for the change localization experiment and estimates of $\eta \equiv \log \sigma$ and $\gamma$. In panel $\mathbf{A}$ we show results for $R=1$ and $M=20$; in panel $\mathbf{D}, R=2$ and $M=50$.

---

#### Page 44

showed that people's decision-making process in this game can be modeled accurately as heuristic search. A heuristic search algorithm makes a move in a given board state by searching through a decision tree of move sequences starting at that board state for a number of moves into the future. To decide which candidate future moves to include in the tree, the algorithm uses a value function defined as

$$
V(\text { board }, \text { move })=\sum_{i=1}^{n_{\mathrm{f}}} w_{i} f_{i}(\text { board }, \text { move })+\mathcal{N}\left(0, \sigma^{2}\right)
$$

in which $f_{i}$ denotes a set of $n_{\mathrm{f}}$ features (i.e., configurations of pieces on the board, such as 'three pieces on a row of the same color'; see Figure 8B), $w_{i} \in \mathbb{R}$ the corresponding feature weights, and $\sigma>0$ is a model parameter which controls value noise. As before, we parameterize the model with $\eta \equiv \log \sigma$.

> **Image description.** The image shows two panels, labeled A and B, each depicting a 4-by-9 grid representing a board game. The grids are composed of gray squares outlined in black.
>
> Panel A:
>
> - The grid contains a configuration of black and white circular pieces.
> - The pieces are arranged in the left portion of the grid, with empty spaces on the right.
> - Specifically, the pieces are arranged as follows (from top to bottom, left to right):
>   - Row 1: White, Black, Black
>   - Row 2: Black, White
>   - Row 3: White
>   - Row 4: Black, Black, White
>
> Panel B:
>
> - The grid contains the same configuration of black and white pieces as in Panel A.
> - Additionally, there are colored lines overlaid on the grid, highlighting specific patterns or features.
> - The colored lines are:
>   - Blue: A vertical line connecting two white pieces in the first column.
>   - Orange: A vertical line connecting two black pieces in the second column.
>   - Purple: A horizontal line connecting two black pieces in the first row. Also a diagonal line connecting two white pieces. And a shorter horizontal line connecting two black pieces.

Figure 8: A. Example board configuration in the 4-in-a-row task, in which two players alternate placing pieces (white or black circles) on a 4-by-9 board (gray grid), and the first player to get 4 pieces in a row wins. In this example, the black player can win by placing a piece on the square on the bottom row, third column. B. Illustration of features used in the value function of the heuristic search model (Equation 21). For details on the model, see Appendix B. 3 and van Opheusden et al. (2016).

The interpretation of this value function is that moves which lead to a high value $V$ (board, move) are given priority in the search algorithm, and the model is more likely

---

#### Page 45

to make those moves. As a heuristic to reduce the search space, any moves for which the value $V$ (board, move) is less than that of the highest-value move minus a threshold parameter $\xi>0$ are pruned from the tree and never considered as viable options. Finally, when evaluating $V$ (board, move), the model stochastically omits features from the sum $\sum_{i=1}^{n_{t}} w_{i} f_{i}$; the probability for any feature to be omitted (or dropped) is independent with probability $\delta \in[0,1]$ (the drop rate). van Opheusden et al. (2016) considered various heuristic search models with different feature sets, and estimated the value of feature weights $w_{i}$ as well as the size (number of nodes) of the decision tree based on human data. Here, we consider a reduced model in which the feature identity $f_{i}$, feature weights $w_{i}$ and tree size are fixed (see Appendix B. 3 for their values). Thus, the current model has three parameters, $\boldsymbol{\theta}=(\eta, \xi, \delta)$.

Note that even though the 4 -in-a-row task is a sequential game, the heuristic search model makes an independent choice on each move, with the 'stimulus' $s$ on each trial being the current board state. Hence, the model satisfies the conditional independence assumptions of Equations 2 and 6 . Note also that, even though the heuristic search algorithm can be specified as a generative 'simulator' which we can query to make moves in any board position, we have no way of calculating the distribution over its moves, since this would require integrating over all possible trees it could build, features which may be dropped, and realizations of the value noise. Therefore, we are in the scenario in which log-likelihood estimation is only possible by simulation, and we cannot compare the performance of fixed sampling or IBS to any 'exact' method.

To generate synthetic data sets for the 4 -in-a-row task, we first compiled a set of 5482 board positions which occurred in human-versus-human play (van Opheusden

---

#### Page 46

et al., 2016). For each data set, we then randomly sampled $N=100$ board positions without replacement which we used as 'stimuli' for each trial, and sampled a move from the heuristic search algorithm for each position to use as 'responses'. For more details about the numerical experiments, see Appendix B.3.

In Figure 9, we perform the same tests as before, comparing fixed sampling and IBS, but lacking any 'exact' estimation method. Due to the high computational complexity of the model, we only consider IBS with up to $R=3$ repeats, corresponding to $\sim 80$ samples. The full results with all tested values of $R$ and $M$ are reported in Figure S8 in the Appendix. As a specific example for Figure 9B,E,H we show the estimates of fixed sampling and IBS for simulated data with $\eta_{\text {true }} \equiv \log \sigma_{\text {true }}=\log 1$, pruning threshold $\xi_{\text {true }}=5$ and $\delta_{\text {true }}=0.2$.

Fixed sampling underestimates the value noise $\eta$, even when using $M=100$ samples, whereas IBS estimates it accurately with 4 times fewer samples (Figure 9A). This bias of fixed sampling gets worse with fewer samples (Figure 9B), and overall, IBS outperforms fixed sampling when compared on equal terms (Figure 9C). The same holds true for the pruning threshold $\xi$. IBS estimates $\xi$ about equally well as fixed sampling, but with about half as many samples (Figure 9D), fixed sampling is severely biased when using too few samples (Figure 9E) and overall, IBS outperforms fixed sampling.

The results are slightly more complicated for the feature drop rate $\delta$. As before, fixed sampling produces strongly biased estimates of $\delta$ with up to 35 samples (Figure 9G), and the bias increases when using fewer samples (Figure 9H). However, for this parameter IBS is also biased, but towards 0.25 (Figure 9G \& H), which is the midpoint of the 'plausible' upper and lower bounds used as reference by the optimization algorithm

---

#### Page 47

> **Image description.** This image contains nine plots arranged in a 3x3 grid, labeled A through I. Each plot compares two methods, "ibs" (blue) and "fixed" (red), in estimating parameters related to value noise, pruning threshold, and feature drop rate in a 4-in-a-row experiment.
>
> **General Observations:**
>
> - All plots have a clean, academic style with clearly labeled axes and legends.
> - Error bars or shaded regions are used to represent uncertainty or variance in the data.
> - The "fixed" method is consistently represented by red lines or data points, while the "ibs" method is represented by blue.
>
> **Panel-Specific Details:**
>
> - **A:** Scatter plot showing estimated value noise (Å·) vs. true value noise (Î·). The x-axis and y-axis are labeled "Î·" and "Å·" respectively, ranging from approximately -2 to 1. Data points for "ibs 25.70" (blue) and "fixed 100" (red) are plotted with error bars. A black diagonal line represents the ideal estimation.
> - **B:** Line plot showing estimated value noise (Å·) vs. number of samples. The x-axis is labeled "Number of samples" ranging from 0 to 100. The y-axis is labeled "Å·" ranging from approximately -1.5 to 0. The "fixed" method (red) shows a curve approaching 0 as the number of samples increases, while the "ibs" method (blue) remains near 0.
> - **C:** Line plot showing RMSE (Root Mean Squared Error) of value noise (Î·) vs. number of samples. The x-axis is labeled "Number of samples" ranging from 0 to 100. The y-axis is labeled "RMSE (Î·)" ranging from 0 to 1.5. Both "fixed" (red) and "ibs" (blue) methods show decreasing RMSE as the number of samples increases.
> - **D:** Scatter plot showing estimated pruning threshold (Î¾Ì) vs. true pruning threshold (Î¾). The x-axis and y-axis are labeled "Î¾" and "Î¾Ì" respectively, ranging from approximately 0 to 10. Data points for "ibs 27.50" (blue) and "fixed 50" (red) are plotted with error bars. A black diagonal line represents the ideal estimation.
> - **E:** Line plot showing estimated pruning threshold (Î¾Ì) vs. number of samples. The x-axis is labeled "Number of samples" ranging from 0 to 100. The y-axis is labeled "Î¾Ì" ranging from approximately 4 to 8. The "fixed" method (red) shows a curve approaching a value close to 5 as the number of samples increases, while the "ibs" method (blue) remains near 6. A dashed black horizontal line labeled "true" is present at approximately y=5.
> - **F:** Line plot showing RMSE of pruning threshold (Î¾) vs. number of samples. The x-axis is labeled "Number of samples" ranging from 0 to 100. The y-axis is labeled "RMSE (Î¾)" ranging from 0 to 4. Both "fixed" (red) and "ibs" (blue) methods show decreasing RMSE as the number of samples increases.
> - **G:** Scatter plot showing estimated feature drop rate (Î´Ì) vs. true feature drop rate (Î´). The x-axis and y-axis are labeled "Î´" and "Î´Ì" respectively, ranging from approximately 0 to 0.4. Data points for "ibs 27.92" (blue) and "fixed 35" (red) are plotted with error bars. A black diagonal line represents the ideal estimation.
> - **H:** Line plot showing estimated feature drop rate (Î´Ì) vs. number of samples. The x-axis is labeled "Number of samples" ranging from 0 to 100. The y-axis is labeled "Î´Ì" ranging from approximately 0 to 0.2. The "fixed" method (red) shows a curve approaching a value close to 0.2 as the number of samples increases, while the "ibs" method (blue) remains near 0.2. A dashed black horizontal line is present at approximately y=0.2.
> - **I:** Line plot showing RMSE of feature drop rate (Î´) vs. number of samples. The x-axis is labeled "Number of samples" ranging from 0 to 100. The y-axis is labeled "RMSE (Î´)" ranging from 0 to 0.25. Both "fixed" (red) and "ibs" (blue) methods show decreasing RMSE as the number of samples increases.

Figure 9: Same as Figures 5 and 7, for the 4-in-a-row experiment and estimates of the value noise $\eta \equiv \log \sigma$, pruning threshold $\xi$ and feature drop rate $\delta$. In panel $\mathbf{A}$ we show results for $R=1$ and $M=100$; in panel $\mathbf{D}, R=1$ and $M=50$; in panel $\mathbf{G}, R=1$ and $M=35$.

---

#### Page 48

(see Appendix B. 3 for details). This bias can be interpreted as a form of regression towards the mean; likely a by-product of the optimization algorithm struggling with a low signal-to-noise ratio for this parameter and these settings (i.e., a nearly flat likelihood landscape for the amount of estimation noise on the log-likelihood). The negative bias of fixed sampling helps to reduce its variance in the low- $\delta$ regime, and therefore in terms of RMSE, fixed sampling performs similarly to IBS for this parameter (Figure 9I).

# 5.5 Log-likelihood loss

In the previous sections, we have analyzed the bias and error of different estimation methods when recovering the generating model parameters in various scenarios. Another important question, crucial for model selection, is how well different methods are able to recover the true maximum log-likelihood. The ability to recover the true parameters and the true maximum log-likelihood are related but distinct properties because, for example, a relatively flat likelihood landscape could yield parameter estimates very far from ground truth, but still afford recovery of a value of the log-likelihood close to the true maximum. We recall that differences in log-likelihood much greater than one point are worrisome as they might significantly affect the outcomes of a model comparison (Kass and Raftery, 1995; Jeffreys, 1998; Anderson and Burnham, 2002).

To compute the log-likelihood loss of a method for a given data set, we estimate the difference between the 'exact' log-likelihood evaluated at the 'true' maximumlikelihood solution (as found after multiple optimization runs) and the 'exact' loglikelihood of the solution returned by the multi-start optimization procedure for a given method, as described in Section 5.1. In terms of methods, we consider IBS and fixed-

---

#### Page 49

sampling with different amounts of samples. We perform the analysis for the two scenarios, orientation discrimination (Section 5.2) and change localization (Section 5.3), for which we have access to the exact likelihood, either analytically or numerically.

> **Image description.** This image contains two line graphs, labeled A and B, showing the log-likelihood loss as a function of the number of samples.
>
> **Panel A:**
>
> - The y-axis is labeled "Log-likelihood loss" and uses a logarithmic scale ranging from 0.01 to 100.
> - The x-axis is labeled "Number of samples" and ranges from 0 to 100.
> - Two lines are plotted on the graph: a blue line representing "IBS" and a red line representing "fixed".
> - Both lines show a decreasing trend as the number of samples increases.
> - The blue line ("IBS") starts at a lower log-likelihood loss than the red line ("fixed") and decreases more rapidly.
> - There are shaded areas around each line, representing the standard deviation. The shaded area around the blue line is light blue, and the shaded area around the red line is light red.
>
> **Panel B:**
>
> - The y-axis is labeled "Log-likelihood loss" and uses a logarithmic scale ranging from 0.01 to 100.
> - The x-axis is labeled "Number of samples" and ranges from 0 to 100.
> - Two lines are plotted on the graph: a blue line representing "IBS" and a red line representing "fixed".
> - Both lines show a decreasing trend as the number of samples increases.
> - The blue line ("IBS") starts at a lower log-likelihood loss than the red line ("fixed") and decreases more rapidly.
> - There are shaded areas around each line, representing the standard deviation. The shaded area around the blue line is light blue, and the shaded area around the red line is light red.
>
> The two graphs are visually similar, showing the same types of data with similar trends.

Figure 10: A. Log-likelihood loss with respect to ground truth, as a function of number of samples, for the orientation discrimination task. Lines are mean $\pm 1$ standard deviation (in log space) across 120 generating parameter values, with 100 simulated datasets each. B. Log-likelihood loss for the change localization task ( 80 generating parameter values).

The results in Figure 10 show that IBS, even with only a few repeats, is able to return solutions which are very close to the true maximum-likelihood solution in terms of loglikelihood (within 1-2 points); whereas fixed sampling remains strongly biased (loglikelihood loss $\gg 1$ ) even with large number of samples, being thus at risk of inducing wrong inferences in model selection. Note that our analyses of the loss are based on the 'exact' log-likelihood values evaluated at the solution returned by the optimization procedure. In practice, we would not have access to the 'exact' log-likelihood at the solution; but its value can be estimated up to the desired precision with IBS, by taking multiple repeats at the returned solution (see Section 4.4).

---

#### Page 50

Finally, the results in Figure 10 also display clearly that, while IBS is unbiased in estimating the log-likelihood for a given parameter setting $\boldsymbol{\theta}$, the maximum-likelihood procedure per se will have some error. Due to estimation noise and specific features of the data, model, and stochastic optimization method at hand, the returned solution will rarely be the true maximum-likelihood solution, and thus, by definition, the value of the log-likelihood at the solution will underestimate the true value of the maximum log-likelihood. Still, Figure 10 shows that the underestimation error, at least in the IBS case, tends to be acceptable, as opposed to the large errors obtained with fixed sampling.

# 5.6 Summary

The results in this section demonstrate that in realistic scenarios, fixed sampling with too few samples causes substantial biases in parameter and maximum log-likelihood estimates, whereas inverse binomial sampling is much more accurate and robust to the number of samples used. Across all 3 models and all parameters, IBS yields parameter estimates with little bias and RMSE, close to that of 'exact' maximum-likelihood estimation, even when using only a handful of repeats ( $R$ between 1 and 5). Conversely, fixed sampling yields substantially biased parameter estimates when using too few samples per trial, especially for parameters which control decision noise, such as measurement noise and lapse rates in the two perceptual decision-making tasks, and value noise in the 4-in-a-row task. Moreover, for the two models for which we have access to 'exact' log-likelihood estimates, we found that IBS is able to recover maximum-likelihood solutions close to the true maximum log-likelihood, whereas fixed sampling remains severely biased even for many samples.

---

#### Page 51

It is true that, given a large enough number of samples, fixed sampling is eventually able to recover most parameters and maximum log-likelihood values with reasonable accuracy. However, we have seen empirically that the number of samples required for reliable estimation varies between tasks, models and parameters of interests. For tasks and models where an exact likelihood or a numerical approximation thereof is unavailable, such as the problem we examined in Section 5.4, this limitation renders fixed sampling hardly usable in practice. By contrast, IBS automatically chooses the number of samples to allocate to the problem.

Finally, for complex models with a large response space, accurate parameter estimation with fixed sampling will require many more samples per trial than are feasible given the computational time needed to generate them. Therefore, in such scenarios accurate and efficient parameter estimation is only possible with IBS.

# 6 Discussion

In this work, we presented inverse binomial sampling (IBS), a method for estimating the log-likelihood of simulation-based models given an experimental data set. We demonstrated that estimates from IBS are uniformly unbiased, their variance is uniformly bounded, and we introduced a calibrated estimator of the variance. IBS is sampleefficient and, for the purpose of maximum-likelihood estimation, combines naturally with gradient-free optimization algorithms that handle stochastic objective functions, such as Bayesian Adaptive Direct Search (BADS; Acerbi and Ma, 2017). We compared IBS to fixed sampling and showed that the bias inherent in fixed sampling can cause

---

#### Page 52

researchers to draw false conclusions when performing model selection. Moreover, we showed in three realistic scenarios of increasing complexity that maximum-likelihood estimation of model parameters is more accurate with IBS than with fixed sampling with the same average number of samples.

In the rest of this section, we discuss additional applications of IBS, possible extensions, and give some practical usage recommendations.

# 6.1 Additional applications

We developed inverse binomial sampling for log-likelihood estimation of models with intractable likelihoods, for the purpose of model comparison or fitting model parameters with maximum-likelihood estimation, but IBS has other practical uses.

## Checking analytical or numerical log-likelihood calculations

We presented IBS as a solution for when the log-likelihood is intractable to compute analytically or numerically. However, even for models where the log-likelihood could be specified, deriving it can be quite involved and time-consuming, and mistakes in the calculation or implementation of the resulting equations are not uncommon. In this scenario, IBS can be useful for:

- quickly prototyping (testing) of new models, as writing the generative model and fitting it to the data is usually much quicker than deriving and implementing the exact log-likelihood;
- checking for derivation or implementation mistakes, as one can compare the supposedly 'exact' log-likelihood against estimates from IBS (on real or simulated

---

#### Page 53

data);

- assessing the quality of numerical approximations used to calculate the log-likelihood, for example when using methods such as adaptive quadrature for numerical integration (Press et al., 1992).

# Estimating entropy and other information-theoretic quantities

We can also use inverse binomial sampling to estimate the entropy of an arbitrary discrete probability distribution $\operatorname{Pr}(x)$, with $x \in \Omega$, a discrete set (see, e.g., Cover and Thomas, 2012, for an introduction to information theory). To do this, we first draw a sample $x$ from the distribution, then use IBS to estimate $\log \operatorname{Pr}(x)$. The first sample and the samples in IBS are independent, and therefore we can calculate the expected value of the outcome of IBS,

$$
\mathbb{E}\left[\hat{\mathcal{L}}_{\mathrm{IBS}}\right]=\mathbb{E}_{x \sim \operatorname{Pr}(\cdot)}[\log \operatorname{Pr}(x)]=\sum_{x \in \Omega} \operatorname{Pr}(x) \log \operatorname{Pr}(x)
$$

which is the definition of the negative entropy of $\operatorname{Pr}(x)$.
We can use this technique to estimate the entropy of the predicted response distribution of a generative model with a given parameter vector on any trial. For example, such quantity could be used in a behavioral model to test for the generalized HickHyman law, that states that reaction time is proportional to the entropy of the available choices (Hyman, 1953). Moreover, we can generalize the method to estimate the crossentropy between two distributions (sample from one, estimate log-likelihood with the other), or the Kullback-Leibler divergence between distributions. We note that all the

---

#### Page 54

estimates of these quantities are also uniformly unbiased. ${ }^{3}$

# 6.2 Bayesian inference

In this paper we focused on maximum-likelihood estimation, but another common approach to parameter estimation is Bayesian inference (Gelman et al., 2013). Bayesian inference has the goal of computing the posterior distribution of the parameters given the observations, computed as

$$
p(\boldsymbol{\theta} \mid \mathcal{D})=\frac{\operatorname{Pr}(\mathcal{D} \mid \boldsymbol{\theta}) p(\boldsymbol{\theta})}{\mathcal{Z}} \quad \text { with } \mathcal{Z} \equiv \int d \operatorname{Pr}(\mathcal{D} \mid \boldsymbol{\theta}) p(\boldsymbol{\theta}) d \boldsymbol{\theta}
$$

where $\operatorname{Pr}(\mathcal{D} \mid \boldsymbol{\theta})$ is the likelihood, $p(\boldsymbol{\theta})$ the prior density of the parameters (typically assumed continuous), and $\mathcal{Z}$ the normalization constant, known as the evidence or marginal likelihood, a quantity used for Bayesian model selection due to a number of desirable properties (MacKay, 2003). Since $\mathcal{Z}$ is often hard to compute, many (approximate) Bayesian inference techniques are able to calculate the posterior distribution by having access only to the unnormalized posterior, or joint distribution $\operatorname{Pr}(\mathcal{D} \mid \boldsymbol{\theta}) p(\boldsymbol{\theta})$; or equivalently to the log joint $\mathcal{L}(\boldsymbol{\theta})+\log p(\boldsymbol{\theta})$. We see then that IBS could be used to perform Bayesian inference of likelihood-free models by providing a means to compute the log-likelihood in the log joint distribution (the prior is assumed to be a simple distribution which we can express in closed form).

In Appendix C.3, we describe how several approaches to approximate Bayesian

[^0]
[^0]: ${ }^{3}$ The lack of bias in entropy estimates by IBS may be surprising in light of a theorem stating that uniformly unbiased estimators of the entropy given a finite set of samples cannot exist (Paninski, 2003). This theorem does not apply to IBS since its sample size is a stochastic variable. It does, however, prove that one cannot estimate entropy (or similar information-theoretic quantities) with fixed sampling.

---

#### Page 55

inference could be used in conjunction with the unbiased log-likelihood estimates provided by IBS: Markov Chain Monte Carlo (Hastings, 1970; Brooks et al., 2011); variational inference (Jordan et al., 1999; Ranganath et al., 2014); and Gaussian process surrogate methods (Kandasamy et al., 2015; JÃ¤rvenpÃ¤Ã¤ et al., 2019), including Variational Bayesian Monte Carlo (VBMC; Acerbi 2018, 2019). In particular, Acerbi (2020) demonstrates the effectiveness of IBS, combined with VBMC, for robust and sampleefficient Bayesian inference, using a variety of models from computational and cognitive neuroscience.

Finally, note that the techniques in this paper can be easily applied to maximum-a-posteriori (MAP) estimation - which is not quite Bayesian inference, but more like a regularized form of maximum-likelihood, that still yields a point estimate instead of a full posterior distribution. MAP estimation is attained by simply adding the logprior to the log-likelihood in the optimization objective, where the log-prior acts as a regularization term.

# 6.3 Approximate IBS for continuous responses

So far, we have assumed that the space of possible responses is discrete. This assumption is necessary since, for continuous responses, the probability that a sample from the generative model exactly matches an observed response is zero (technically, near-zero since any computer implementation of a real number is finite). For this reason, IBS will never terminate, or at least not within a physically sensible time scale.

A simple approach to make continuous responses discrete is via binning the response space. Alternatively, we recommend an approach inspired by Approximate

---

#### Page 56

Bayesian Computation (ABC; Beaumont et al., 2002), which we call Approximate IBS (AIBS). Given a metric $D(\cdot, \cdot)$ to measure distance in response space, and a tolerance threshold $\varepsilon>0$, we can use IBS to estimate

$$
\mathcal{L}_{\varepsilon}(\boldsymbol{\theta})=\sum_{i=1}^{N} \log \frac{\operatorname{Pr}\left(D\left(\tilde{\boldsymbol{r}}_{i}, \boldsymbol{r}_{i}\right) \leq \varepsilon \mid \boldsymbol{s}_{i}, \boldsymbol{\theta}\right)}{\left|B_{\varepsilon}\left(\boldsymbol{r}_{i}\right)\right|}
$$

where the $\tilde{\boldsymbol{r}}_{i}$ are responses drawn from the generative model, and $\left|B_{\varepsilon}\left(\boldsymbol{r}_{i}\right)\right|$ denotes the volume of the set of responses whose distance from $\boldsymbol{r}_{i}$ is no more than $\varepsilon$.

The $\varepsilon$-approximate log-likelihood in Equation 24 can then be used as normal for maximum-likelihood estimation or Bayesian inference. As $\varepsilon \rightarrow 0$, the approximate likelihood tends to the true likelihood, under some regularity conditions which we leave to explore for future work (see Prangle 2017 for a similar proof for ABC). However, the expected number of samples used by IBS diverges in that limit, so in practice there is a lower bound for $\varepsilon$ that is feasible and one needs to extrapolate to the $\varepsilon=0$ limit, or be satisfied to perform inference with an $\varepsilon$-approximate likelihood.

The common idea between AIBS and ABC is that they both use a distance metric to judge similarity between simulated samples and data. However, ABC commonly bases the comparison on summary statistics of the data (which may not be sufficient statistics, and thus not capture all aspects of the data); whereas AIBS uses the full responses. Secondly, ABC in practice requires dedicated algorithms to perform parameter estimation and inference (basic techniques, such as rejection sampling, can be extremely inefficient); whereas AIBS simply provides a (noisy) log-likelihood, which can then be used in combination with a wider class of likelihood-based inference methods, as long as they support noisy estimates (see Appendix C. 3 for some examples). We leave a further analysis of AIBS, and a comparison with other likelihood-free inference approaches, as

---

#### Page 57

a promising direction for future work.

# 6.4 Usage recommendations

We conclude with a number of recommendations for researchers who want to fit a model to a data set, having access only to a simulator or generative model.

- First, try to derive a closed-form analytic expression for the log-likelihood of the model. If this is tractable, validate that the log-likelihood is free of implementation mistakes by comparing its output against log-likelihood estimates obtained by IBS with well-chosen test trials and model parameters.
- If exact analytics are intractable, find an analytical or numerical approximation, for example using variational inference or Riemannian integration, and once again validate the quality of the approximation using IBS.
- If the model is too complex for analytical or numerical approximations, estimate the log-likelihood using inverse binomial sampling.
- Finally, perform inference using the analytical, numerical, or IBS-based loglikelihood function with a sample-efficient inference algorithm, such as those based on Gaussian process surrogate modeling. For maximum-likelihood (or maximum-a-posteriori) estimation, hybrid Bayesian optimization methods have proved to be quite effective (Acerbi and Ma, 2017).

---

#### Page 58

# Avoiding infinite loops

One issue of IBS is that it can 'hang', in the sense that the implementation of the estimator can run indefinitely, without returning an answer, if the simulator is unable to match a particularly unlikely observation. This is a natural behavior of IBS that stems from its efficiency in allocating samples, as we examined in Section 4.2. We recommend two easy solutions to avoid infinite loops:

- Implement a 'lapse rate' $\gamma \in(0,1)$ in the simulator model, which represents the probability of a completely random response (typically uniform across all possible responses). The lapse rate could be fixed to a small, non-zero value (e.g., $\gamma=0.01$ ), or let as a free model parameter; in which case, ensure that the minimum lapse rate is a small, non-zero value (e.g., $\gamma_{\min }=0.005$ ).
- Introduce an early-stopping threshold, such that IBS stops sampling when the estimated log-likelihood of the entire data set goes below a threshold $\mathcal{L}_{\text {lower }}$ (see Appendix C.1).

We implemented both of these solutions in our analyses in Section 5.

---

# Unbiased and Efficient Log-Likelihood Estimation with Inverse Binomial Sampling - Backmatter

---

## Acknowledgments

This work has utilized the NYU IT High Performance Computing resources and services. We thank Aspen Yoo for help with Figures 4 and 6 and useful comments on the manuscript, and Michael Landy for helpful discussion about the derivation of the variance of the IBS estimator. Luigi Acerbi was partially supported by the Academy of Finland Flagship programme: Finnish Center for Artificial Intelligence (FCAI).

---

#### Page 59

# References 

Abramowitz, M. and Stegun, I. A. (1948). Handbook of mathematical functions with formulas, graphs, and mathematical tables, volume 55. US Government Printing Office.

Acerbi, L. (2018). Variational Bayesian Monte Carlo. In Advances in Neural Information Processing Systems, pages 8213-8223.

Acerbi, L. (2019). An exploration of acquisition and mean functions in Variational Bayesian Monte Carlo. In Symposium on Advances in Approximate Bayesian Inference, pages $1-10$.

Acerbi, L. (2020). Variational Bayesian Monte Carlo with noisy likelihoods. arXiv preprint arXiv:2006.08655.

Acerbi, L. and Ma, W. J. (2017). Practical Bayesian optimization for model fitting with Bayesian Adaptive Direct Search. In Advances in Neural Information Processing Systems, pages 1836-1846.

Acerbi, L., Ma, W. J., and Vijayakumar, S. (2014). A framework for testing identifiability of Bayesian models of perception. In Advances in Neural Information Processing Systems, pages 1026-1034.

Akaike, H. (1974). A new look at the statistical model identification. In Selected Papers of Hirotugu Akaike, pages 215-222. Springer.

Anderson, D. and Burnham, K. (2002). Model selection and multi-model inference: A practical information-theoretic approach. Springer-Verlag.

---

#### Page 60

Andrieu, C., Roberts, G. O., et al. (2009). The pseudo-marginal approach for efficient Monte Carlo computations. The Annals of Statistics, 37(2):697-725.

Ash, R. B., Robert, B., Doleans-Dade, C. A., and Catherine, A. (2000). Probability and measure theory. Academic Press.

Audet, C. and Dennis Jr, J. E. (2006). Mesh adaptive direct search algorithms for constrained optimization. SIAM Journal on Optimization, 17(1):188-217.

Beaumont, M. A., Zhang, W., and Balding, D. J. (2002). Approximate Bayesian computation in population genetics. Genetics, 162(4):2025-2035.

Brooks, S., Gelman, A., Jones, G., and Meng, X.-L. (2011). Handbook of Markov Chain Monte Carlo. CRC press.

Cover, T. M. and Thomas, J. A. (2012). Elements of information theory. John Wiley \& Sons.

Dawson, R. (1953). Unbiased tests, unbiased estimators, and randomized similar regions. PhD thesis, Harvard University.
de Groot, M. H. (1959). Unbiased sequential estimation for binomial populations. The Annals of Mathematical Statistics, 30(1):80-101.

Duncan, G. M. (2004). Unbiased simulators for anaytic functions and maximum unbiased simulated likelihood estimation. Available at SSRN 692921.

Forrester, A. I., SÃ³bester, A., and Keane, A. J. (2007). Multi-fidelity optimization via surrogate modelling. Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences, 463(2088):3251-3269.

---

#### Page 61

Gelman, A., Carlin, J. B., Stern, H. S., Dunson, D. B., Vehtari, A., and Rubin, D. B. (2013). Bayesian data analysis. Chapman and Hall/CRC.

Girshick, M., Mosteller, F., and Savage, L. (1946). Unbiased estimates for certain binomial sampling problems with applications. The Annals of Mathematical Statistics, 17(1):13-23.

Green, D. M. and Swets, J. A. (1966). Signal detection theory and psychophysics, volume 1. Wiley New York.

Haldane, J. (1945a). A labour-saving method of sampling. Nature, 155(3924):49.

Haldane, J. (1945b). On a method of estimating frequencies. Biometrika, 33(3):222225.

Haldane, J. B. S. (1932). A note on inverse probability. In Mathematical Proceedings of the Cambridge Philosophical Society, volume 28, pages 55-61. Cambridge University Press.

Hansen, N., MÃ¼ller, S. D., and Koumoutsakos, P. (2003). Reducing the time complexity of the derandomized evolution strategy with covariance matrix adaptation (CMAES). Evolutionary Computation, 11(1):1-18.

Hansen, N., Niederberger, A. S., Guzzella, L., and Koumoutsakos, P. (2008). A method for handling uncertainty in evolutionary optimization with an application to feedback control of combustion. IEEE Transactions on Evolutionary Computation, 13(1):180197.

---

#### Page 62

Hastings, W. K. (1970). Monte Carlo sampling methods using Markov chains and their applications. Biometrika, 57(1):97-109.

Hyman, R. (1953). Stimulus information as a determinant of reaction time. Journal of Experimental Psychology, 45(3):188-196.

JÃ¤rvenpÃ¤Ã¤, M., Gutmann, M., Vehtari, A., and Marttinen, P. (2019). Parallel Gaussian process surrogate method to accelerate likelihood-free inference. arXiv preprint arXiv:1905.01252.

Jeffreys, H. (1998). The theory of probability. OUP Oxford.

Jones, D. R., Schonlau, M., and Welch, W. J. (1998). Efficient global optimization of expensive black-box functions. Journal of Global Optimization, 13(4):455-492.

Jordan, M. I., Ghahramani, Z., Jaakkola, T. S., and Saul, L. K. (1999). An introduction to variational methods for graphical models. Machine Learning, 37(2):183-233.

Kandasamy, K., Schneider, J., and PÃ³czos, B. (2015). Bayesian active learning for posterior estimation. In Twenty-Fourth International Joint Conference on Artificial Intelligence.

Kass, R. E. and Raftery, A. E. (1995). Bayes factors. Journal of the American Statistical Association, 90(430):773-795.

Kennedy, M. C. and OâHagan, A. (2000). Predicting the output from a complex computer code when fast approximations are available. Biometrika, 87(1):1-13.

Kolmogorov, A. N. (1950). Unbiased esimates. Izvestiya RAN. Seriya Matematicheskaya, 14(4):303-326.

---

#### Page 63

Lehmann, E. L. and Casella, G. (2006). Theory of point estimation. Springer Science \& Business Media.

Lindeberg, J. W. (1922). Eine neue herleitung des exponentialgesetzes in der wahrscheinlichkeitstechnung. Mathematische Zeitschrift, 15(1):211-225.

MacKay, D. J. (2003). Information theory, inference and learning algorithms. Cambridge University Press.

Maximon, L. C. (2003). The dilogarithm function for complex argument. In Proceedings of the Royal Society of London A: Mathematical, Physical and Engineering Sciences, volume 459, pages 2807-2819. The Royal Society.

Maxwell, S. E., Lau, M. Y., and Howard, G. S. (2015). Is psychology suffering from a replication crisis? What does "failure to replicate" really mean? American Psychologist, 70(6):487.

Myung, I. J. (2003). Tutorial on maximum likelihood estimation. Journal of Mathematical Psychology, 47(1):90-100.

OâHagan, A. (1991). Bayes-Hermite quadrature. Journal of Statistical Planning and Inference, 29(3):245-260.

Paninski, L. (2003). Estimation of entropy and mutual information. Neural Computation, 15(6):1191-1253.

Pospischil, M., Toledo-Rodriguez, M., Monier, C., Piwkowska, Z., Bal, T., FrÃ©gnac, Y., Markram, H., and Destexhe, A. (2008). Minimal Hodgkin-Huxley type models

---

#### Page 64

for different classes of cortical and thalamic neurons. Biological Cybernetics, 99(45):427-441.

Prangle, D. (2017). Adapting the ABC distance function. Bayesian Analysis, 12(1):289-309.

Press, W. H., Teukolsky, S. A., Vetterling, W. T., and Flannery, B. P. (1992). Numerical recipes in $C++$, volume 2. Cambridge University Press.

Prins, N. (2012). The psychometric function: The lapse rate revisited. Journal of Vision, 12(6):25-25.

Pritchard, J. K., Seielstad, M. T., Perez-Lezaun, A., and Feldman, M. W. (1999). Population growth of human Y chromosomes: a study of Y chromosome microsatellites. Molecular Biology and Evolution, 16(12):1791-1798.

Ranganath, R., Gerrish, S., and Blei, D. (2014). Black box variational inference. In Artificial Intelligence and Statistics, pages 814-822.

Rasmussen, C. E. and Williams, C. K. (2006). Gaussian processes for machine learning, volume 2. The MIT press, Cambridge, MA.

Ratmann, O., JÃ¸rgensen, O., Hinkley, T., Stumpf, M., Richardson, S., and Wiuf, C. (2007). Using likelihood-free inference to compare evolutionary dynamics of the protein networks of H. pylori and P. falciparum. PLoS Computational Biology, 3(11):e230.

Schwarz, G. et al. (1978). Estimating the dimension of a model. The Annals of Statistics, 6(2):461-464.

---

#### Page 65

Shahriari, B., Swersky, K., Wang, Z., Adams, R. P., and De Freitas, N. (2015). Taking the human out of the loop: A review of Bayesian optimization. Proceedings of the IEEE, 104(1):148-175.

Snoek, J., Larochelle, H., and Adams, R. P. (2012). Practical Bayesian optimization of machine learning algorithms. In Advances in Neural Information Processing Systems, pages 2951-2959.

Spiegelhalter, D. J., Best, N. G., Carlin, B. P., and Van Der Linde, A. (2002). Bayesian measures of model complexity and fit. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 64(4):583-639.

Sterratt, D., Graham, B., Gillies, A., and Willshaw, D. (2011). Principles of computational modelling in neuroscience. Cambridge University Press.

Van den Berg, R., Shin, H., Chou, W.-C., George, R., and Ma, W. J. (2012). Variability in encoding precision accounts for visual short-term memory limitations. Proceedings of the National Academy of Sciences, 109(22):8780-8785.
van Opheusden, B., Bnaya, Z., Galbiati, G., and Ma, W. J. (2016). Do people think like computers? In International Conference on Computers and Games, pages 212-224. Springer.

Vehtari, A., Gelman, A., and Gabry, J. (2017). Practical Bayesian model evaluation using leave-one-out cross-validation and WAIC. Statistics and Computing, 27(5):14131432.

---

#### Page 66

Watanabe, S. (2010). Asymptotic equivalence of Bayes cross validation and widely applicable information criterion in singular learning theory. Journal of Machine Learning Research, 11(Dec):3571-3594.

Wichmann, F. A. and Hill, N. J. (2001). The psychometric function: I. Fitting, sampling, and goodness of fit. Perception \& Psychophysics, 63(8):1293-1313.

Wilkinson, D. J. (2011). Stochastic modelling for systems biology. CRC press.

Wood, S. N. (2010). Statistical inference for noisy nonlinear ecological dynamic systems. Nature, 466(7310):1102.

---

# Unbiased and Efficient Log-Likelihood Estimation with Inverse Binomial Sampling - Appendix

---

#### Page 67

# Supplementary Material 

## A Further theoretical analyses

## A. 1 Why inverse binomial sampling works

We start by showing that the inverse binomial sampling policy described in Section 2.4, combined with the estimator $\hat{\mathcal{L}}_{\text {ibs }}$ (Equation 14), yields a uniformly unbiased estimate of $\log p$. This derivation follows from de Groot (1959, Theorem 4.1), adapted to our special case of estimating $\log p$ instead of an arbitrary function $f(p)$ :

$$
\begin{aligned}
\mathbb{E}\left[\hat{\mathcal{L}}_{\text {ibs }}\right] & =-\mathbb{E}\left[\sum_{k=1}^{K-1} \frac{1}{k}\right]=-\mathbb{E}\left[\sum_{k=1}^{\infty} \frac{1}{k} \mathbb{1}_{k<K}\right] \\
& =-\sum_{k=1}^{\infty} \frac{1}{k} \mathbb{E}\left[\mathbb{1}_{k<K}\right]=-\sum_{k=1}^{\infty} \frac{1}{k} \operatorname{Pr}(k<K) \\
& =-\sum_{k=1}^{\infty} \frac{1}{k}(1-p)^{k}=\log p
\end{aligned}
$$

The first equality is the definition of $\hat{\mathcal{L}}_{\text {ibs }}$ (Equation 14), using the notational convention that $\sum_{k=1}^{0}=0$. In the second equality we introduce the indicator function $\mathbb{1}_{k<K}$ which is 1 when $k<K$ and 0 otherwise. The third equality follows by linearity of the expectation and the fourth directly from the definition of the indicator function. The fifth and second-to last equality uses the formula for the cumulative distribution function of a geometric variable, that is $\operatorname{Pr}(K \leq k)=1-(1-p)^{k}$, and thus $\operatorname{Pr}(k<K)=(1-p)^{k}$. The final equality is the definition of the Taylor series of $\log p$ expanded around $p=1$. Note that this series converges for all $p \in(0,1]$.

In the derivation above, we can replace $\frac{1}{k}$ by an arbitrary set of coefficients $a_{k}$ and

---

#### Page 68

show that

$$
\mathbb{E}\left[\sum_{k=1}^{K-1} a_{k}\right]=\sum_{k=1}^{\infty} a_{k}(1-p)^{k}
$$

for all $p$ for which the resulting Taylor series converges. Equation S2 immediately proves two corollaries. First, we can use the inverse binomial sampling policy to estimate any analytic function of $p$. Second, since we can rewrite any estimator $\hat{\mathcal{L}}(K)$ as $\sum_{k=1}^{K-1} a_{k}$, and since Taylor series are unique, $a_{k}=\frac{1}{k}$ is the only choice for which $\mathbb{E}[\hat{\mathcal{L}}(K)]$ equals $\log p$. In other words, $\hat{\mathcal{L}}_{\text {ibs }}$ is the only uniformly unbiased estimator of $\log p$ with the inverse sampling policy. Therefore, it trivially is the uniformly minimumvariance unbiased estimator under this policy, since no other unbiased estimator exist.

# A. 2 Analysis of bias of fixed sampling 

We provide here a more formal analysis of the bias of fixed sampling. We initially consider the estimator $\hat{\mathcal{L}}_{\text {fixed }}$ defined by Equation 11 in the main text, but we will see that our arguments hold generally for any estimator based on a fixed sampling policy.

We showed in Figure 2 that in the regime of $p \rightarrow 0, M \rightarrow \infty$, while keeping $p M \rightarrow \lambda$, the bias of $\hat{\mathcal{L}}_{\text {fixed }}$ tends to a master curve. This follows since, in this limit, the binomial distribution $\operatorname{Binom}\left(\frac{\lambda}{M}, M\right)$ converges to a Poisson distribution $\operatorname{Pois}(\lambda)$ and therefore the bias converges to

$$
\begin{aligned}
\text { Bias }\left[\hat{\mathcal{L}}_{\text {fixed }} \mid p\right] & =\mathbb{E}\left[\hat{\mathcal{L}}_{\text {fixed }}-\log \frac{\lambda}{M}\right] \\
& =\exp (-\lambda) \sum_{m=0}^{\infty} \frac{\lambda^{m}}{m!} \log (m+1)-\log (M+1)-\log \frac{\lambda}{M} \\
& \underset{\substack{M \rightarrow 0 \\
p M \rightarrow \lambda}}{\longrightarrow} \exp (-\lambda) \sum_{m=0}^{\infty} \frac{\lambda^{m}}{m!} \log (m+1)-\log \lambda
\end{aligned}
$$

---

#### Page 69

which is the master curve in Figure 2. In particular, the bias is close to zero for $\lambda \gg 1$ and it diverges when $\lambda \ll 1$, or equivalently, for $M \gg \frac{1}{p}$ and $M \ll \frac{1}{p}$, respectively.

This asymptotic behavior is not a coincidence. In fact, it is mathematically guaranteed since the Fisher information of $\operatorname{Pois}(\lambda)$ equals $\frac{1}{\lambda}$ and the reparametrization identity for the Fisher information yields $\mathcal{I}_{f}(\log \lambda)=\lambda$ (Lehmann and Casella, 2006). In the limit of $p \ll \frac{1}{\lambda^{2}}$, which corresponds to $\lambda \ll 1$, this Fisher information vanishes and the outcome of fixed sampling simply provides zero information about $\log \lambda$ or $\log p$. Therefore, any estimates of $\log p$ are not informed by the data and instead are a function of the regularization chosen in the estimator $\hat{\mathcal{L}}_{\text {fixed }}$ (Equation 11). Note that the argument above does not invoke the specific form of the estimator, and therefore holds for any choice of regularization.

We can express the problem with fixed sampling more clearly using Bayesian statistics, in a formal treatment of the 'gambling' analogy we presented in the main text. The 'correct' belief about $\log \lambda$ given the outcome of fixed sampling $(m)$ is quantified by the posterior distribution $p(\log \lambda \mid m)$, which is a product of the likelihood $\operatorname{Pr}(m \mid \log \lambda)$ and a prior $p(\log \lambda)$. In the limit $\lambda \ll 1$, the Poisson distribution converges to a Kronecker delta distribution concentrated on $m=0$. In other words, almost surely none of the samples taken by the behavioral model will match the participant's response. When $m=0$, the likelihood equals $\exp (-\lambda)$, which is mostly flat (when considered as a function of $\log \lambda$, see Figure S1) for $\log \lambda \in[-\infty,-2]$ and therefore our posterior belief ought to be dominated by the prior $p(\log \lambda)$ and become independent of the data. Therefore, we once again conclude that in the limit $p \ll \frac{1}{\lambda^{2}}$, the fixed sampling policy provides no information to base an estimate of $\log p$ on, and it is impossible to avoid

---

#### Page 70

bias.

> **Image description.** This is a line graph showing the relationship between log Î» and Pr(Î»|m = 0).
> 
> The graph has the following features:
> 
> *   **Axes:**
>     *   The x-axis is labeled "log Î»" and ranges from -8 to 2.
>     *   The y-axis is labeled "Pr(Î»|m = 0)" and ranges from 0 to 1. Tick marks are present at 0, 0.5, and 1.
> *   **Data:**
>     *   A blue line starts at y=1 when x is -8.
>     *   The line remains approximately flat at y=1 until x is around -2.
>     *   The line then curves downwards, decreasing rapidly as x increases.
>     *   The line approaches y=0 as x approaches 2.

Figure S1: Likelihood function of $\log \lambda$ given that fixed sampling returns $m=0$ (none of the samples from the model match the participant's response). The likelihood is approximately flat for all $\log \lambda \leq-2$. Since $\lambda$ is defined as $\frac{p}{M}$, this implies that the posterior distribution over $p$ will be dominated by a prior rather than evidence, as quantified by the likelihood.

# A. 3 Derivation of IBS variance 

In this section, we derive the expression for the variance of the IBS estimator (Equation 15 in the main text). We compute the variance of $\hat{L}_{\text {ibs }}$ starting from the identity

$$
\operatorname{Var}\left[\hat{L}_{\mathrm{ibs}}\right]=\mathbb{E}\left[\left(\hat{L}_{\mathrm{ibs}}\right)^{2}\right]-\left(\mathbb{E}\left[\hat{L}_{\mathrm{ibs}}\right]\right)^{2}
$$

We already know the second term is equal to $(\log p)^{2}$, but for the purpose of this derivation, and for reasons that will become clear later, we re-write it as

$$
\left(\mathbb{E}\left[\hat{L}_{\mathrm{ibs}}\right]\right)^{2}=\left(\sum_{m=1}^{\infty} \frac{1}{m}(1-p)^{m}\right)^{2}=\sum_{m, n=1}^{\infty} \frac{1}{m n}(1-p)^{m+n}
$$

In order to write this equation as a power series in $1-p$, we collect terms with the same exponent together. Specifically, we re-index this double summation as a summation

---

#### Page 71

over all values of $n$ and $m+n$ (which we label $k$ ), and substitute $k-n$ for $m$.

$$
\sum_{m, n=1}^{\infty} \frac{1}{m n}(1-p)^{m+n}=\sum_{k=1}^{\infty} \sum_{n=1}^{k-1} \frac{1}{(k-n) n}(1-p)^{k}
$$

Note that in the second summation over $n$ we only have to sum to $n=k-1$ since $m \equiv n-k$ has to be positive. We can carry out the internal summation over $n$ explicitly,

$$
\sum_{n=1}^{k-1} \frac{1}{(k-n) n}=\sum_{n=1}^{k-1}\left[\frac{1}{k(k-n)}+\frac{1}{k n}\right]=\frac{2}{k} \sum_{n=1}^{k-1} \frac{1}{n}=\frac{2}{k} H_{k-1}
$$

The first equality is an algebraic manipulation, the second follows by symmetry and the final equality defines $H_{k-1}$ as the $(k-1)$-th harmonic number. Therefore, we find that

$$
\left(\mathbb{E}\left[\hat{L}_{\mathrm{ibs}}\right]\right)^{2}=2 \sum_{k=1}^{\infty} \frac{H_{k-1}}{k}(1-p)^{k}
$$

To calculate $\mathbb{E}\left[\left(\hat{L}_{\mathrm{ibs}}\right)^{2}\right]$, we use a similar rationale as Equation S1,

$$
\begin{aligned}
\mathbb{E}\left[\left(\hat{L}_{\mathrm{ibs}}\right)^{2}\right] & =\mathbb{E}\left[\left(-\sum_{m=1}^{K-1} \frac{1}{m}\right)^{2}\right]=\mathbb{E}\left[\sum_{m, n=1}^{K-1} \frac{1}{m n}\right] \\
& =\mathbb{E}\left[\sum_{m, n=1}^{\infty} \frac{1}{m n} \mathbb{1}_{m<K} \mathbb{1}_{n<K}\right]=\sum_{m, n=1}^{\infty} \frac{1}{m n} \mathbb{E}\left[\mathbb{1}_{m<K, n<K}\right] \\
& =\sum_{m, n=1}^{\infty} \frac{1}{m n} \operatorname{Pr}\left(\mathbb{1}_{m<K, n<K}\right)=\sum_{m, n=1}^{\infty} \frac{1}{m n}(1-p)^{\max (m, n)}
\end{aligned}
$$

In these equations, $\mathbb{1}$ again denotes an indicator function, and we use the fact that the product of indicator functions for two different events is the indicator function for the joint event. Additionally, we use that the event $m<K, n<K$ is logically equivalent to $\max (m, n)<K$. To write this double summation as a power series, we split it into three parts: one where $m<n$, one where $m=n$ and one where $m>n$. By symmetry, the first and last part are equal, and we can write

$$
\mathbb{E}\left[\left(\hat{L}_{\mathrm{ibs}}\right)^{2}\right]=\sum_{m=1}^{\infty} \frac{1}{m m}\left[(1-p)^{\max (m, m)}\right]+2 \sum_{m=1}^{\infty} \sum_{n=1}^{m-1} \frac{1}{m n}\left[(1-p)^{\max (m, n)}\right]
$$

---

#### Page 72

By re-arranging some terms, and using the fact that $\max (m, m)=m$ and $\max (m, n)=$ $m$ for all $n<m$, we can reduce this to

$$
\mathbb{E}\left[\left(\hat{L}_{\mathrm{ibs}}\right)^{2}\right]=\sum_{m=1}^{\infty} \frac{1}{m^{2}}(1-p)^{m}+2 \sum_{m=1}^{\infty}\left[\sum_{n=1}^{m-1} \frac{1}{n}\right] \frac{1}{m}(1-p)^{m}
$$

We can now explicitly perform the summation over $n$ in the second term and write

$$
\mathbb{E}\left[\left(\hat{L}_{\mathrm{ibs}}\right)^{2}\right]=\sum_{m=1}^{\infty} \frac{1}{m^{2}}(1-p)^{m}+2 \sum_{m=1}^{\infty} \frac{H_{m-1}}{m}(1-p)^{m}
$$

Finally, putting everything together, we obtain

$$
\begin{aligned}
\operatorname{Var}\left[\hat{L}_{\mathrm{ibs}}\right] & =\mathbb{E}\left[\left(\hat{L}_{\mathrm{ibs}}\right)^{2}\right]-\left(\mathbb{E}\left[\hat{L}_{\mathrm{ibs}}\right]\right)^{2} \\
& =\sum_{m=1}^{\infty} \frac{1}{m^{2}}(1-p)^{m}+2 \sum_{m=1}^{\infty} \frac{H_{m-1}}{m}(1-p)^{m}-2 \sum_{k=1}^{\infty} \frac{H_{k-1}}{k}(1-p)^{k} \\
& =\sum_{m=1}^{\infty} \frac{1}{m^{2}}(1-p)^{m}
\end{aligned}
$$

# A. 4 Estimator variance and information inequality 

We proved in Section A. 1 that $\hat{\mathcal{L}}_{\text {IBS }}$ is the minimum-variance unbiased estimator of $\log p$ given the inverse binomial sampling policy. Here we show that the estimator also comes close to saturating the information inequality, the analogue of a Cramer-RÃ¡o bound for an arbitrary function $f(p)$ and a non-fixed sampling policy (de Groot, 1959),

$$
\operatorname{Std}(\hat{f} \mid p) \geq \sqrt{\frac{p(1-p)}{\mathbb{E}[K|p|)}}\left|\frac{\mathrm{d} f(p)}{\mathrm{d} p}\right|
$$

In our case, where $f(p)=\log p$, the information inequality reduces to $\operatorname{Std}\left(\hat{\mathcal{L}}_{\text {IBS }}\right) \geq$ $\sqrt{1-p}$. In Figure S2, we plot the standard deviation of IBS compared to this lower bound.

---

#### Page 73

> **Image description.** A line graph displays the standard deviation of IBS and its lower bound against the variable *p*.
> 
> *   The x-axis is labeled "*p*" and ranges from 0 to 1 in increments of 0.2.
> *   The y-axis is labeled "Standard deviation" and ranges from 0 to 1.5 in increments of 0.5.
> *   A blue curve represents "IBS" and starts at approximately 1.3 on the y-axis when *p* is 0, gradually decreasing as *p* increases.
> *   A black curve represents "lower bound" and starts at 1 on the y-axis when *p* is 0, decreasing more rapidly than the blue curve as *p* increases. The two curves converge around *p* = 0.7 and continue to decrease, reaching 0 at *p* = 1.
> *   A legend in the upper right corner identifies the blue curve as "IBS" and the black curve as "lower bound".

Figure S2: Standard deviation of IBS (Blue curve) and the lower bound given by the information inequality (black, see Equation S4). The standard deviation of IBS is within $30 \%$ of the lower bound across the entire range of $p$.

It may be disappointing that IBS does not match the information inequality. Kolmogorov (1950) showed that the only functions $f(p)$ for which the fixed sampling policy with $M$ samples allows an unbiased estimator are polynomials of degree up to $M$, and those estimators can saturate the information equality. Dawson (1953) and later de Groot (1959) showed that if an unbiased estimator of a non-polynomial function $f(p)$ exists and it matches the information inequality, it must use the inverse binomial sampling policy. Moreover, de Groot derived necessary and sufficient conditions for $f(p)$ to allow such estimators (de Groot, 1959). Applying this argument to $f(p)=\log (p)$, the standard deviation in IBS is close (within $30 \%$ ) to its theoretical minimum.

To compare the variance of IBS and fixed sampling on equal terms, we use the scaling behavior of $\hat{\mathcal{L}}_{\text {fixed }}$ as $M \rightarrow \infty$. Specifically, for fixed sampling, we plot $\sqrt{M} \times$ $\operatorname{Std}\left(\hat{\mathcal{L}}_{\text {fixed }}\right)$ and for IBS we plot $\frac{1}{\sqrt{p}} \times \operatorname{Std}\left(\hat{\mathcal{L}}_{\text {IBS }}\right)$ (see Figure S3). With this scaling, the curves for fixed sampling again collapse onto a master curve ${ }^{4}$. Note that repeated-

[^0]
[^0]:    ${ }^{4}$ These curves converge pointwise on $(0,1]$ and uniformly on any interval $(\varepsilon, 1]$, but not uniformly on $(0,1]$. The limits $M \rightarrow \infty$ and $p \rightarrow 0$ are not exchangeable.

---

#### Page 74

sampling IBS estimators $\hat{\mathcal{L}}_{\text {IBS- } R}$ (see Section 4.4), obtained by averaging multiple IBS estimates, overlap with the curve for regular IBS for any $R$.

> **Image description.** A line graph compares the standard deviation of different sampling methods.
> 
> The graph has the x-axis labeled "p" and the y-axis labeled "Standard deviation x âsamples". The x-axis ranges from 0 to 1. The y-axis ranges from 0 to 5.
> 
> Several lines are plotted on the graph, each representing a different sampling method. The lines are colored as follows:
> - Pale pink: "Fixed: 1 sample"
> - Light pink: "Fixed: 2 samples"
> - Light red: "Fixed: 5"
> - Red: "Fixed: 10"
> - Dark red: "Fixed: 20"
> - Darker red: "Fixed: 50"
> - Darkest red: "Fixed: 100"
> - Black: "master curve"
> - Blue: "IBS"
> 
> All the lines start at the right side of the graph, near x=1, and increase as x approaches 0. The lines representing "Fixed" sampling methods peak at different points before decreasing, with the peak increasing as the number of samples increases. The "master curve" and "IBS" lines do not peak but continue to increase as x approaches 0.

Figure S3: Standard deviation times square root of the expected number of samples drawn by IBS (blue) and fixed sampling (red), and the master curve (black) that fixed sampling converges to when $M \rightarrow \infty$.

All these curves increase and diverge as $p \rightarrow 0$, reflecting the fact that estimating log-likelihoods for small $p$ is hard. The standard deviation of fixed sampling is always lower than that of IBS, especially when $p \rightarrow 0$ (specifically when $p \ll \frac{1}{M}$ ). In other words, fixed sampling produces low-variance estimators exactly in the range in which its estimates are biased, as guaranteed by the Cramer-RÃ¡o bound. However, in the large- $M$ limit, fixed sampling does saturate the information inequality, so its master curve lies below IBS. In other words, if one is able to draw so many samples that bias is no issue, then fixed sampling provides a slightly better trade-off between variance and computational time. However, in Section C.2, we discuss an improvement to IBS which decreases its variance by a factor 2-20, in which case IBS is clearly superior. Finally, a quantity of interest for the researcher may not be the variance of the estimator per se, but a measure of the error such as the RMSE, for which IBS is also consistently superior

---

#### Page 75

(see Section A.6).

# A. 5 A Bayesian derivation of the IBS estimator 

In Sections 3 and A. 2 we hinted at a Bayesian interpretation of the problem of estimating $\log p$. We show here that indeed we can see the IBS estimator as a Bayesian point estimate of $\log p$ with a specific choice of prior for $p$. For the rest of this section, we use $q$ to denote the likelihood of a trial (instead of $p$ ); that is $q$ is the parameter of the Bernoulli distribution and $\log q$ the quantity we are seeking to estimate. We changed notation to avoid confusion with expressions such as the prior probability of $q$, which is $p(q)$.

Let $K$ be the number of samples until a 'hit', as per the IBS sampling policy. Following Bayes' rule, we can write the posterior over $q$ given $K$ as

$$
\begin{aligned}
p(q \mid K) & =\frac{\operatorname{Pr}(K \mid q) p(q)}{\operatorname{Pr}(K)} \\
& =\frac{(1-q)^{K-1} q \operatorname{Beta}(q ; \alpha, \beta)}{\int_{0}^{1}(1-q)^{K-1} q \operatorname{Beta}(q ; \alpha, \beta) d q} \\
& =\frac{\Gamma(K+\alpha+\beta)}{\Gamma(\alpha+1) \Gamma(K+\beta-1)}(1-q)^{K+\beta-2} q^{\alpha}
\end{aligned}
$$

where we used the fact that $\operatorname{Pr}(K \mid q)$ follows a geometric distribution, and we assumed a Beta $(\alpha, \beta)$ prior over $q$.

In particular, let us compute the posterior mean of $\log q$ under the Haldane prior, $\operatorname{Beta}(0,0)$ (Haldane, 1932). Thanks to the 'law of the unconscious statistician', we can

---

#### Page 76

compute the posterior mean of $p(\log q \mid K)$ directly from Equation S5,

$$
\begin{aligned}
\mathbb{E}_{p(\log q \mid K)}[\log q] & =(K-1) \int_{0}^{1}(\log q)(1-q)^{K-2} d q \\
& =\int_{0}^{1}(\log q) \text { Beta }(q ; 1, K-1) d q \\
& =\psi(1)-\psi(K) \\
& =-\sum_{k=1}^{K-1} \frac{1}{k}
\end{aligned}
$$

where the first row follows from setting $\alpha=0$ and $\beta=0$; it can be shown that the third row is the expectation of $\log q$ for a Beta distribution, with $\psi(z)$ the digamma function (Abramowitz and Stegun, 1948); and the last equality follows from the relationship between the digamma function and harmonic numbers, that is $\psi(n)=-\gamma+\sum_{k=1}^{n-1} \frac{1}{k}$, where $\gamma$ is Euler-Mascheroni constant. We also used the notational convention that $\sum_{k=1}^{0} a_{k}=0$ for any $a_{k}$. Note that the last row is equal to the IBS estimator, $\hat{\mathcal{L}}_{\text {IBS }}(K)$, as defined in Equation 14 in the main text.

Crucially, Equation S5 shows that we can recover the IBS estimator as the posterior mean of $\log q$ given $K$, under the Haldane prior for $q$. This interpretation allows us to also define naturally the variance of our estimate for a given $K$, as the variance of the posterior over $\log q$,

$$
\operatorname{Var}_{p(\log q \mid K)}[\log q]=\psi_{1}(1)-\psi_{1}(K)
$$

where $\psi_{1}(z)$ is the trigamma function, the derivative of the digamma function; the equality follows from a known expression for the variance of $\log q$ under a Beta distribution for $q$.

---

#### Page 77

# A. 6 Estimator RMSE 

In the main text and in previous comparisons we have discussed the bias and the variance of estimators of the log-likelihood, which are important statistical properties, but one might wonder how bias and variance combine to yield an error metric of practical relevance such as the root mean squared error (RMSE). Crucially, this analysis depends on the number of trials $N$ (because bias and standard deviation scale differently with $N$ ) and on the distribution of values of the likelihood for different trials, $p_{i}$.

For illustrative purposes, we took as an example the psychometric model described in Section 5.2, and calculated the distribution of $p_{i}$ for typical datasets and parameters settings. We then calculated the RMSE in estimating the total log-likelihood of a number of randomly generated datasets (sampled from the empirical distribution of $p_{i}$ ) with different number of trials; for different numbers of samples used by the IBS and fixed-sampling estimators.

> **Image description.** This image consists of three line graphs arranged horizontally. Each graph plots the Root Mean Squared Error (RMSE) on the y-axis against the "Number of samples" on the x-axis.
> 
> Here's a breakdown of the common elements and the variations between the graphs:
> 
> *   **Common Elements:**
>     *   **Axes:** Each graph has an x-axis labeled "Number of samples" ranging from 0 to 100. The y-axis is labeled "RMSE."
>     *   **Curves:** Each graph displays two curves: a red curve labeled "Fixed" and a blue curve labeled "IBS." Both curves generally decrease as the number of samples increases, indicating a reduction in RMSE with more samples.
>     *   **Legend:** A legend is present in the first graph, labeling the red curve as "Fixed" and the blue curve as "IBS."
>     *   **General Shape:** Both the red and blue curves show a steep decline initially, followed by a more gradual decrease as the number of samples increases.
> 
> *   **Variations (Panel-Specific Details):**
>     *   **Panel 1 (Left):**
>         *   Title: "10 Trials"
>         *   Y-axis range: 0 to 2.5
>     *   **Panel 2 (Center):**
>         *   Title: "100 Trials"
>         *   Y-axis range: 0 to 25
>     *   **Panel 3 (Right):**
>         *   Title: "500 Trials"
>         *   Y-axis range: 0 to 120
> 
> In summary, the image compares the RMSE of two estimators, "Fixed" and "IBS," as a function of the number of samples, with each panel representing a different number of trials (10, 100, and 500). The RMSE decreases with the number of samples for both estimators, but the specific values and the relative performance of the estimators vary depending on the number of trials.

Figure S4: RMSE of the log-likelihood estimate as a function of number of samples, for the IBS and fixed-sampling estimators. Different panels display the RMSE curves for different number of trials.

Figure S4 shows that starting from even a handful of trials $(N=10)$, IBS is con-

---

#### Page 78

sistently better than fixed sampling at estimating the true value of the log-likelihood of a given parameter vector, and overwhelmingly so for a moderate number of trials $(N \geq 100)$.

# B Experimental details 

In this section, we report details for the three numerical experiments described in the main text and supplementary results.

## B. 1 Orientation discrimination

The parameters of the orientation discrimination model are the (inverse) slope, or sensory noise, represented as $\eta \equiv \log \sigma$, the bias $\mu$, and the lapse rate $\gamma$. The logarithmic representation for $\sigma$ is a natural choice for scale parameters (and more in general, for positive parameters that can span several orders of magnitude).

We define the lower bound (LB), upper bound (UB), plausible lower bound (PLB), and plausible upper bound (PUB) of the parameters as per Table S1. The upper and lower bounds are hard constraints, whereas the plausible bounds provide information to the algorithm to where the global optimum is likely to be, and are used by BADS, for example, to draw a set of initial points to start building the surrogate Gaussian process, and to set priors over the Gaussian process hyperparameters (Acerbi and Ma, 2017). Here we also use the plausible bounds to select ranges for the parameters used to generate simulated datasets, and to initialize the optimization, as described below.

To generate synthetic data sets, we select 120 'true' parameter settings for the ori-

---

#### Page 79

Table S1: Parameters and bounds of the orientation discrimination model.

| Parameter | Description | LB | UB | PLB | PUB |
| :--: | :--: | :--: | :--: | :--: | :--: |
| $\eta \equiv \log \sigma$ | Slope | $\log 0.1$ | $\log 10$ | $\log 0.1$ | $\log 5$ |
| $\mu$ | Bias $\left({ }^{\circ}\right)$ | -2 | 2 | -1 | 1 |
| $\gamma$ | Lapse | 0.01 | 1 | 0.01 | 0.2 |

entation discrimination task as follows. We set the baseline parameter $\boldsymbol{\theta}_{0}$ as $\eta=\log 2^{\circ}$, $\mu=0.1^{\circ}$, and $\gamma=0.1$. Then, for each parameter $\theta_{j} \in\{\eta, \mu, \gamma\}$, we linearly vary the value of $\theta_{j}$ in 40 increments from $\mathrm{PLB}_{j}$ to $\mathrm{PUB}_{j}$ as defined in Table S 1 (e.g., from $-1^{\circ}$ to $1^{\circ}$ for $\mu$ ), while keeping the other two parameters fixed to their baseline value. For each one of the 120 parameter settings $\boldsymbol{\theta}_{\text {true }}$ defined in this way, we randomly generated stimuli and responses for 100 datasets from the generative model, resulting in 12000 distinct data sets for which we know the true generating parameters.

We evaluated the log-likelihood with the following methods: fixed sampling with $M$ samples, with $M \in\{1,2,3,5,10,15,20,35,50,100\}$; IBS with $R$ repeats, with $R \in\{1,2,3,5,10,15,20,35,50\}$; and exact. To avoid wasting computations on particularly 'bad' parameter settings, for IBS we used the 'early stopping threshold' technique described in Section C.1, setting a lower bound on the log-likelihood of IBS equal to the log-likelihood of a chance model, that is $\mathcal{L}_{\text {lower }}=-N \log 2$. While this might seemingly provide an advantage to IBS with respect to Fixed sampling, note that it is simply a way to ameliorate a weakness of IBS (spending too much time on 'bad' parameters vectors, which are largely inconsequential for optimization), which Fixed does not suffer from. Even so, the stopping threshold was rarely reached ( $2 \%$ of evaluations).

---

#### Page 80

For each data set and method, we optimized the log-likelihood by running BADS 8 times with different starting points. We selected starting points as the points that lie on one-third or two-third of the distance between the plausible upper and lower bound for each parameter, that is all combinations of $\eta \in\{-0.998,0.305\}, \mu \in$ $\left\{-0.333^{\circ}, 0.333^{\circ}\right\}, \gamma \in\{0.073,0.137\}$. Each of these optimization runs returns a candidate for $\widehat{\boldsymbol{\theta}}_{\text {MLE }}$. For methods that return a noisy estimate of the log-likelihood, we then re-evaluate $\hat{\mathcal{L}}(\boldsymbol{\theta})$ for each of these 8 candidates with higher precision (for fixed sampling, we use $10 M$ samples; for IBS, we use $10 R$ repeats). Finally, we select the candidate with highest (estimated) log-likelihood.

When estimating parameters using IBS or fixed sampling, we enabled the 'uncertainty handling' option in BADS, informing it to incorporate measurement noise into its model of the objective function. Note that during the optimization, the algorithm iteratively infers a single common value for the observation noise $\sigma_{\text {obs }}$ associated with the function values in a neighborhood of the current point (Acerbi and Ma, 2017). A future extension of BADS may allow the user to explicitly provide the noise associated with each data point, which is easily computed for the IBS estimates (Equation 16 in the main text), affording the construction of a better surrogate model of the log-likelihood.

# Alternative fixed sampling estimator 

In the main text, we considered the fixed-sampling estimator defined by Equation 11. We performed an additional analysis to empirically validate that our results do not depend on the specific choice of estimator for fixed sampling (as expected given the theoretical arguments in Section 3).

---

#### Page 81

An alternative way of avoiding the divergence of fixed sampling is to correct samples that happen to be all zeros, for example with

$$
\hat{\mathcal{L}}_{\text {fixed-bound }}(\boldsymbol{x})=\log \left(\frac{\max \left\{m(\boldsymbol{x}), m_{\min }\right\}}{M}\right)
$$

for some $0<m_{\min }<1$, which sets a lower bound for the log-likelihood equal to $\log \left(m_{\min } / M\right)$. We then performed our analyses of the orientation discrimination task using the $\hat{\mathcal{L}}_{\text {fixed-bound }}$ estimator with $m_{\min }=\frac{1}{2}$. As shown in Figure S5, the results are remarkably similar to what we found using the fixed-sampling estimator $\hat{\mathcal{L}}_{\text {fixed }}$ defined by Equation 11. Finally, we also tried $\hat{\mathcal{L}}_{\text {fixed-bound }}$ with a small value $m_{\min }=10^{-3}$, which yielded even worse results (data not shown).

> **Image description.** This image contains six plots, arranged in a 2x3 grid, labeled A through F. The plots compare different methods ("exact", "ibs", and "fixed") for estimating parameters, likely related to a statistical model or simulation.
> 
> **Panel A:**
> *   This is a scatter plot.
> *   The x-axis is labeled "Î·".
> *   The y-axis is labeled "Î®".
> *   Three data sets are plotted: "exact" (green plus signs), "ibs 2.22" (blue plus signs), and "fixed 10" (red plus signs).
> *   A black line, representing a linear relationship, is also present.
> 
> **Panel B:**
> *   This is a line graph.
> *   The x-axis is labeled "Number of samples". The scale ranges from 0 to 100.
> *   The y-axis is labeled "Î®".
> *   Four data sets are plotted as lines: "exact" (green), "IBS" (blue), "fixed" (red), and "true" (black dashed line).
> 
> **Panel C:**
> *   This is a line graph.
> *   The x-axis is labeled "Number of samples". The scale ranges from 0 to 100.
> *   The y-axis is labeled "RMSE (Î·)".
> *   Three data sets are plotted as lines: "exact" (green), "fixed" (red), and "IBS" (blue).
> 
> **Panel D:**
> *   This is a scatter plot.
> *   The x-axis is labeled "Î³".
> *   The y-axis is labeled "Å·".
> *   Three data sets are plotted: "exact" (green plus signs), "ibs 6.49" (blue plus signs), and "fixed 20" (red plus signs).
> *   A black line, representing a linear relationship, is also present.
> 
> **Panel E:**
> *   This is a line graph.
> *   The x-axis is labeled "Number of samples". The scale ranges from 0 to 100.
> *   The y-axis is labeled "Å·".
> *   Three data sets are plotted as lines with shaded regions around them: "exact" (green), "fixed" (red), and "IBS" (blue).
> *   A black dashed line is also present.
> 
> **Panel F:**
> *   This is a line graph.
> *   The x-axis is labeled "Number of samples". The scale ranges from 0 to 100.
> *   The y-axis is labeled "RMSE (Î³)".
> *   Three data sets are plotted as lines with shaded regions around them: "exact" (green), "fixed" (red), and "IBS" (blue).
> 
> In summary, the figure compares the performance of different estimation methods ("exact", "ibs", "fixed") in terms of their estimates of parameters (Î· and Î³) and their Root Mean Squared Error (RMSE) as a function of the number of samples.

Figure S5: Same as Figure 5 in the main text, but for the alternative fixed-sampling estimator defined by Equation S8. The results are qualitatively identical.

---

#### Page 82

# Complete parameter recovery results 

For completeness, we report in Figure S6 the parameter recovery results for fixed sampling, inverse binomial sampling and 'exact' analytical methods for the orientation discrimination task, for all tested number of samples $M$ and IBS repeats $R$. All estimates were obtained via maximum-likelihood estimation using the Bayesian Adaptive Direct Search (Acerbi and Ma, 2017), as described previously in this section.

## B. 2 Change localization

First, we derive the trial likelihood of the change localization model. Assuming that the change happens at location $c \in\{1, \ldots, 6\}$, by symmetry we can write

$$
\operatorname{Pr}(\text { respond } i \mid c \text { changed })= \begin{cases}P_{\text {correct }}\left(\Delta_{s}^{(c)} ; \boldsymbol{\theta}\right) & \text { if } i=c \\ \frac{1}{5}\left(1-P_{\text {correct }}\left(\Delta_{s}^{(c)} ; \boldsymbol{\theta}\right)\right) & \text { otherwise }\end{cases}
$$

where $\Delta_{s}^{(c)}=\left|d_{\text {circ }}\left(s_{c}^{(1)}, s_{c}^{(2)}\right)\right|$ is the absolute circular distance between the true orientations of patch $c$ in the first and second display. We can derive an expression for $P_{\text {correct }}\left(\Delta_{s}^{(c)} ; \boldsymbol{\theta}\right)$ by marginalizing over the circular distance between the respective measurements,
$P_{\text {correct }}\left(\Delta_{s}^{(c)} ; \boldsymbol{\theta}\right)=\frac{\gamma}{6}+(1-\gamma) \int_{0}^{2 \pi} \operatorname{Pr}\left(\Delta_{x}^{(c)} \mid \Delta_{s}^{(c)}\right) \operatorname{Pr}\left(\forall i \neq c: \Delta_{x}^{(i)} \leq \Delta_{x}^{(c)} \mid \Delta_{s}^{(i)}=0\right) \mathrm{d} \Delta_{x}^{(c)}$,
where we have defined $\Delta_{x}^{(i)}=\left|d_{\text {circ }}\left(x_{i}^{(1)}, x_{i}^{(2)}\right)\right|$ and we suppressed the dependence on $\kappa$ to simplify the notation. The first term in this equation is the probability density function (pdf) of the circular distance between two von Mises random variables whose centers are $\Delta_{s}^{(j)}$ apart. The second term simplifies, since $\Delta_{x}^{(i)}$ for all $i \neq j$ are all

---

#### Page 83

> **Image description.** This image contains a set of nine scatter plots arranged in a 3x3 grid, labeled A through I. Each plot visualizes the relationship between a parameter and its estimate, with different methods used for estimation across the columns. The rows represent different parameters: eta (Î·), mu (Î¼), and gamma (Î³).
> 
> *   **General Layout:** The plots are organized as follows:
>     *   Top row (A, B, C): Plots for parameter Î· (eta).
>     *   Middle row (D, E, F): Plots for parameter Î¼ (mu).
>     *   Bottom row (G, H, I): Plots for parameter Î³ (gamma).
>     *   Left column (A, D, G): Plots using "fixed sampling" with varying numbers of samples.
>     *   Middle column (B, E, H): Plots using "IBS" (Iterative Bayesian Sequential sampling) with varying numbers of repeats.
>     *   Right column (C, F, I): Plots using the "exact" log-likelihood function.
> 
> *   **Plot Details:**
>     *   Each plot has a horizontal axis representing the true parameter value (Î·, Î¼, or Î³) and a vertical axis representing the estimated parameter value (denoted with a hat: á¿, Î¼Ì, or Î³Ì).
>     *   A black diagonal line (y=x) is present in each plot, representing perfect estimation (estimate equals true value).
>     *   Different colored lines represent different numbers of samples (left and middle columns) or the "exact" method (right column).
>     *   Colors range from light to dark, with lighter shades indicating fewer samples/repeats and darker shades indicating more.
>     *   The legends in plots A, B, E, and H indicate the number of samples/repeats associated with each colored line.
>     *   Plot A shows several lines ranging from light pink to dark red, representing 1, 2, 3, 5, 10, 15, 20, 35, 50, and 100 samples.
>     *   Plot B shows several lines ranging from light blue to dark blue, representing 2.22, 4.36, 6.48, 10.75, 21.38, 32.00, 42.59, 74.40, and 106.23 samples.
>     *   Plot C shows a single green line representing the "exact" method.
>     *   Plot D shows several lines ranging from light pink to dark red.
>     *   Plot E shows several lines ranging from light blue to dark blue, representing 2.22, 4.40, 6.56, 10.89, 21.67, 32.47, 43.22, 75.44, and 107.58 samples.
>     *   Plot F shows a single green line representing the "exact" method.
>     *   Plot G shows several lines ranging from light pink to dark red, representing 1, 2, 3, 5, 10, 15, 20, 35, 50, and 100 samples.
>     *   Plot H shows several lines ranging from light blue to dark blue, representing 2.20, 4.35, 6.49, 10.78, 21.51, 32.24, 42.95, 75.03, and 107.11 samples.
>     *   Plot I shows a single green line representing the "exact" method.
> 
> *   **Axes Labels:**
>     *   The horizontal axes are labeled with the parameter symbols: Î·, Î¼, and Î³.
>     *   The vertical axes are labeled with the estimated parameter symbols: á¿, Î¼Ì, and Î³Ì.

Figure S6: Full parameter recovery results for the orientation discrimination model. A.

Mean estimates recovered by fixed sampling with different number of samples. Error bars are omitted to avoid visual clutter. B. Mean estimates recovered by IBS with different numbers of repeats. The legend reports the average number of samples per trial that IBS uses to obtain these estimates. C. Mean estimate recovered using the 'exact' log-likelihood function (Equation 20). D-F Same, for the bias parameter $\mu$. G-I Same, for the lapse rate $\gamma$. Overall, fixed sampling produces highly biased estimates of $\eta$ and $\gamma$, while IBS is much more accurate. The bias parameter $\mu$ can be accurately estimated by either method regardless of the number of samples or repeats.

---

#### Page 84

independent and identically distributed. Therefore, we can rewrite this equation as

$$
P_{\text {correct }}\left(\Delta_{s}^{(c)} ; \boldsymbol{\theta}\right)=\frac{\gamma}{6}+(1-\gamma) \int_{0}^{2 \pi} \operatorname{Pr}\left(\Delta_{x}^{(c)} \mid \Delta_{s}^{(c)}\right) \operatorname{Pr}\left(\Delta \leq \Delta_{x}^{(c)}\right)^{5} \mathrm{~d} \Delta_{x}^{(c)}
$$

where $\Delta$ is an auxiliary variable generated by taking the absolute circular difference between two von Mises random variables that are centered at 0 with concentration parameter $\kappa$. The second term of the integrand, therefore, is the fifth power of the cumulative distribution function (cdf) of $\Delta$. We can compute the distribution of the circular distance between two von Mises random variables analytically, but the cdf is non-analytic. Moreover, the integral in Equation S11 is analytically intractable as well. We can, however, evaluate it numerically via trapezoidal integration (see Figure 6B).

We now describe the settings used for maximum-likelihood estimation. The model parameters are the sensory noise, represented as $\eta \equiv \log \sigma$ (with $\sigma=\frac{1}{\sqrt{\kappa}}$ ), and the lapse rate $\gamma$, with bounds defined in Table S2. We use the same procedure and settings for BADS as for the orientation discrimination task (see Section B.1). For IBS, we use an early-stopping threshold of $\mathcal{L}_{\text {lower }}=-N \log 6$, and we use repeats $R \in$ $\{1,2,3,5,10,15,20\}$ (since due to the larger response space IBS uses more samples per run). We run BADS 4 times, with starting values of $\eta \in\{-1.535,-0.767\}$ and $\gamma \in\{0.173,0.337\}$. For data generation, we select 40 parameter vectors with $\eta=$ $\log 0.3$ and $\gamma$ linearly spaced from 0.01 to 0.5 and 40 data sets with $\gamma=0.03$ and $\eta$ between $\log 0.1$ and $\log 1$. Again, we generate 100 data sets for each such parameter combination.

---

#### Page 85

Table S2: Parameters and bounds of the change localization model.

| Parameter | Description | LB | UB | PLB | PUB |
| :--: | :-- | :--: | :--: | :--: | :--: |
| $\eta \equiv \log \sigma$ | Sensory noise | $\log 0.05$ | $\log 2$ | $\log 0.1$ | $\log 1$ |
| $\gamma$ | Lapse | 0.01 | 1 | 0.01 | 0.5 |

# Complete parameter recovery results 

We report in Figure S7 the parameter recovery results for fixed sampling, inverse binomial sampling and 'exact' methods for the change localization task, for all tested number of samples $M$ and IBS repeats $R$. For this task, the exact method relies on numerical integration.

## B. 3 Four-in-a-row game

The four-in-a-row game model parameters are the value noise $\eta \equiv \log \sigma$, the pruning threshold $\xi$, and the feature dropping rate $\delta$, with bounds defined in Table S3. We use the same procedure and settings for BADS as for the orientation discrimination task (see Section B.1), unless noted otherwise. For IBS, we use an early-stopping threshold of $\mathcal{L}_{\text {lower }}=-N \log 20$, and due to computational cost we use only $R \in\{1,2,3\}$. For fixed sampling we consider $M \in\{1,2,3,5,10,15,20,35,50,100\}$. We have no expression for the likelihood of the four-in-a-row game model, not even in numerical form, so there is no 'exact' method.

We run BADS 8 times, with starting values of $\eta \in\{-0.707,0.196\}, \xi \in\{4,7\}$ and $\delta \in\{0.167,0.333\}$. For data generation, we set as baseline parameter vector $\eta=\log 1$, $\xi=5$ and $\delta=0.2$ and for each parameter we select 40 parameter vectors linearly

---

#### Page 86

> **Image description.** This image contains six scatter plots arranged in a 2x3 grid, labeled A through F. Each plot displays the relationship between two variables, with a diagonal black line serving as a reference.
> 
> *   **Plots A, B, and C:** These plots show the relationship between Î· (horizontal axis) and á¿ (vertical axis).
>     *   Plot A: Contains multiple lines in shades of red, from light to dark, representing different sample sizes (1, 2, 3, 5, 10, 15, 20, 35, 50, and 100 samples). The lines generally curve upwards, deviating from the diagonal black line, especially at higher values of Î·.
>     *   Plot B: Contains multiple lines in shades of blue, representing different sample sizes (6.19, 12.25, 18.36, 30.56, 61.12, and 122.52 samples). These lines are closer to the diagonal black line compared to plot A.
>     *   Plot C: Contains a single green line labeled "exact". This line closely follows the diagonal black line, indicating a more accurate relationship between Î· and á¿.
> 
> *   **Plots D, E, and F:** These plots show the relationship between Î³ (horizontal axis) and Å· (vertical axis).
>     *   Plot D: Contains multiple lines in shades of red, similar to plot A, representing different sample sizes. The lines curve upwards, deviating from the diagonal black line.
>     *   Plot E: Contains multiple lines in shades of blue, similar to plot B, representing different sample sizes (6.83, 13.42, 19.94, 32.85, 65.06, and 129.26 samples). These lines are closer to the diagonal black line compared to plot D.
>     *   Plot F: Contains a single green line labeled "exact". This line closely follows the diagonal black line, indicating a more accurate relationship between Î³ and Å·.
> 
> The axes in all plots are labeled with numerical values. The x-axes (Î· and Î³) range from approximately 1.5 to 4 (Plots A, B, C) and 0 to 0.5 (Plots D, E, F). The y-axes (á¿ and Å·) range from approximately 1 to 5 (Plots A, B, C) and 0 to 0.5 (Plots D, E, F).

Figure S7: Same as Figure S6, for the change localization model. Fixed sampling is substantially biased for both the measurement noise $\eta$ and the lapse rate $\gamma$, whereas IBS is accurate for $\eta$ and biased for $\gamma$, but still much less biased than fixed sampling.

---

#### Page 87

Table S3: Parameters and bounds of the four-in-a-row game model.

| Parameter | Description | LB | UB | PLB | PUB |
| :--: | :--: | :--: | :--: | :--: | :--: |
| $\eta \equiv \log \sigma$ | Value noise | $\log 0.01$ | $\log 5$ | $\log 0.2$ | $\log 3$ |
| $\xi$ | Pruning threshold | 0.01 | 10 | 1 | 10 |
| $\delta$ | Feature dropping rate | 0 | 1 | 0 | 0.5 |

spaced in the plausible range for that parameter (as per Table S3), while keeping the other two parameters at their baseline value. Again, we generate 100 data sets for each such parameter combination.

We fixed the other parameters of the model to typical values found in the previous study (van Opheusden et al., 2016), namely $w_{\text {center }}=0.60913, w_{\text {connected } 2 \text {-in-a-row }}=$ $0.90444, w_{\text {unconnected } 2 \text {-in-a-row }}=0.45076, w_{3 \text {-in-a-row }}=3.4272, w_{3 \text {-in-a-row }}=6.1728, C_{\text {act }}=$ $0.92498, \gamma_{\text {tree }}=0.02, \lambda=0.05$. The $w_{i}$ are the weights of features $f_{i}$ in the value function, briefly $f_{\text {center }}$ values pieces near the center of the board, the other features count the number of times certain patterns occur on the board (see van Opheusden et al., 2016 for the specific patterns). $C_{\text {act }}$ is a parameter which scales the value of features belonging to the active or passive player. The parameter $\gamma_{\text {tree }}$ is inversely proportional to the size of the tree built by the algorithm, and $\lambda$ is the lapse rate, that is the probability of a uniformly random move among the available squares (note that for the other models we denoted lapse rate as $\gamma$; here we use the variable naming from van Opheusden et al., 2016). See van Opheusden et al. (2016) for more details about the model and its parameters.

---

#### Page 88

# Complete parameter recovery results 

We report in Figure S8 the parameter recovery results for fixed sampling and inverse binomial sampling for the 4-in-a-row task, for all tested number of samples $M$ and IBS repeats $R$. For this task, there is no 'exact' method to evaluate the log-likelihood.

## C Improvements of IBS and further applications

## C. 1 Early stopping threshold

One downside of inverse binomial sampling is that the computational time it uses to estimate the log-likelihood is of the order of $\frac{1}{p}$, which is equal to $\exp (-\log p)=\exp (-\mathcal{L})$. In other words, IBS spends exponentially more time on estimating log-likelihoods of poorly-fitting models or bad parameters. This implies that an optimization algorithm that uses IBS allocates more computational resources to estimating the objective function $\mathcal{L}(\boldsymbol{\theta})$ for parameter vectors $\boldsymbol{\theta}$ where the objective is low. However, the value of the objective at such poor parameter vectors are unlikely to affect its estimate of the location or value of the maximum, so the optimizer (BADS in our case) is wasting time. It may be possible to develop optimization algorithms that take into account the exponentially large cost of probing points where the objective function is low, but we can circumvent the problem by amending IBS with a criterion that stops sampling when it realizes that $\dot{\mathcal{L}}(\boldsymbol{\theta})$ will be low.

In Section 4.1, we introduced a basic implementation of IBS for estimating the loglikelihood of multiple trials, by sequentially computing the log-likelihood of each trial. However, another way to implement multi-trial IBS (a 'parallel' implementation) is to

---

#### Page 89

> **Image description.** This image contains six scatter plots arranged in a 2x3 grid, labeled A through F. Each plot shows the relationship between an estimated parameter and the true parameter value.
> 
> *   **General Layout:** The plots are organized with A, B in the first row, C, D in the second row, and E, F in the third row. Each plot has axes labeled with parameter symbols. A black diagonal line is present in each plot, representing the ideal estimation (where estimated value equals true value).
> 
> *   **Plot A:** Shows the estimated parameter "$\hat{\eta}$" on the y-axis versus the true parameter "$\eta$" on the x-axis. Multiple lines are plotted, each representing a different number of samples. The lines are colored in shades of red, with darker red indicating a larger number of samples. A legend in the upper right corner indicates the number of samples for each line: 1, 2, 3, 5, 10, 15, 20, 35, 50, and 100 samples.
> 
> *   **Plot B:** Shows the estimated parameter "$\hat{\eta}$" on the y-axis versus the true parameter "$\eta$" on the x-axis. Three lines are plotted, colored in shades of blue. A legend in the upper left corner indicates the number of samples for each line: 25.70, 50.76, and 75.84 samples.
> 
> *   **Plot C:** Shows the estimated parameter "$\hat{\xi}$" on the y-axis versus the true parameter "$\xi$" on the x-axis. Multiple lines are plotted, each representing a different number of samples. The lines are colored in shades of red, with darker red indicating a larger number of samples.
> 
> *   **Plot D:** Shows the estimated parameter "$\hat{\xi}$" on the y-axis versus the true parameter "$\xi$" on the x-axis. Three lines are plotted, colored in shades of blue. A legend in the upper left corner indicates the number of samples for each line: 27.50, 53.80, and 80.20 samples.
> 
> *   **Plot E:** Shows the estimated parameter "$\hat{\delta}$" on the y-axis versus the true parameter "$\delta$" on the x-axis. Multiple lines are plotted, each representing a different number of samples. The lines are colored in shades of red, with darker red indicating a larger number of samples.
> 
> *   **Plot F:** Shows the estimated parameter "$\hat{\delta}$" on the y-axis versus the true parameter "$\delta$" on the x-axis. Three lines are plotted, colored in shades of blue. A legend in the upper left corner indicates the number of samples for each line: 27.92, 54.59, and 81.25 samples.
> 
> The plots on the left (A, C, E) use a red color scheme and show more sample number variations, while the plots on the right (B, D, F) use a blue color scheme and show fewer sample number variations. The x and y axes are scaled differently in each plot to accommodate the range of values for each parameter.

Figure S8: Same as Figure S6, for the four-in-a-row task. For this model, we do not have an exact log-likelihood formula or numerical approximation, so we only show fixed sampling and IBS. Overall, fixed sampling has substantial biases in its estimation of $\eta$ and $\delta$ and a smaller bias in estimating $\xi$. IBS has almost no bias for $\eta$ and only a small bias for $\xi$ and $\delta$.

---

#### Page 90

draw one sample from the simulator model for each trial, then set $K_{i}=1$ for each trial where the sample matches the participant's response. For all other trials, draw a second sample from the model, and if that matches the response, set $K_{i}=2$. Finally, repeat this process until no more trials remain. We illustrate this sampling scheme graphically in Figure S9.

After each iteration, we then compute

$$
\hat{\mathcal{L}}_{K}=-\sum_{i \in \mathcal{I}_{\text {match }}} \sum_{k=1}^{K_{i}-1} \frac{1}{k}-N_{\text {remaining }} \sum_{k=1}^{K-1} \frac{1}{k}
$$

where $K$ is the iteration number, $\mathcal{I}_{\text {match }}$ is the set of trials where we found a matching sample and $N_{\text {remaining }}$ is the number of remaining trials. This value $\hat{\mathcal{L}}_{K}$ is decreasing and by construction converges to $\sum_{i=1}^{N} \hat{\mathcal{L}}_{i, \text { IBS }}$ as $K \rightarrow \infty$. Therefore, whenever $\hat{\mathcal{L}}_{K}$ falls below a lower bound $\mathcal{L}_{\text {lower }}$, we are guaranteed that $\hat{\mathcal{L}}_{\text {IBS }}$ will be below that bound too. When it does, we stop sampling and return $\mathcal{L}_{\text {lower }}$ as estimate of $\mathcal{L}(\boldsymbol{\theta})$. This does introduce bias into the estimate, but since we bound the total log-likelihood, the bias will be exponentially small in $N$ as long as the true value $\mathcal{L}(\boldsymbol{\theta})$ is adequately larger than $\mathcal{L}_{\text {lower }}$.

In practice, we recommend using as lower bound the log-probability of the data under a 'chance' model, which assigns uniform probability to each possible response on each trial, and should be a poor model of the data. In the orientation discrimination and change localization examples from Sections 5.2 and 5.3, the log-likelihood of a chance model is $-N \log 2$ and $-N \log 6$, respectively. For the 4 -in-a-row game presented in Section 5.4 the log-likelihood of chance depends on the number of pieces on each board position; we chose an average value such that $\mathcal{L}_{\text {lower }}=-N \log 20$. This new sampling scheme has an additional advantage: since on each iteration we independently sample

---

#### Page 91

> **Image description.** This is a diagram illustrating a method for implementing IBS (Iterative Bayesian Sequential sampling) with multiple trials.
> 
> The diagram consists of a grid-like structure with six columns labeled "Trial" 1 through 6 along the bottom horizontal axis. The vertical axis is implicitly labeled with 'k' values from k=1 to k=6 on the left side. Each column represents a trial, and each row represents a successive sample from the model.
> 
> Each cell in the grid contains either a red "X" (representing a miss) or a green checkmark (representing a hit). Above each column is a "K" value representing the number of samples until a hit is achieved for that trial.
> 
> Specifically:
> *   Trial 1 has K=3, with a red X in the first two rows and a green checkmark in the third row.
> *   Trial 2 has K=1, with a green checkmark in the first row.
> *   Trial 3 has K=2, with a red X in the first row and a green checkmark in the second row.
> *   Trial 4 has K=1, with a green checkmark in the first row.
> *   Trial 5 has K=6, with red X's in the first five rows and a green checkmark in the sixth row.
> *   Trial 6 has K=4, with red X's in the first three rows and a green checkmark in the fourth row.

Figure S9: Graphical illustration of the two methods to implement IBS with multiple trials, in this case $N=6$. In this figure, each column represents a trial, each box above the trial number represents a successive sample from the model from that trial, with red crosses for samples that do not match the participant's response ('misses') and green checkmarks for ones that do ('hits'). Above each column, we indicate $K$, the number of samples until a hit. For trials 2 and $4, K=1$ so $\hat{\mathcal{L}}_{\text {IBS }}=0$. The most obvious implementation of multi-trial IBS is 'columns-first', to sample model responses for each trial until a hit, and only then move to the next trial. However, a more convenient sampling method is 'rows-first', and sample one response for each trial with $k=1$, then one response for each trial with $k=2$, excluding trials 2 and 4 since the first sample was a hit, and continue increasing $k$ until all trials reach a hit. This method allows for early stopping and a parallel processing.

---

#### Page 92

from the generative model on multiple trials, we can potentially run these computations in parallel.

# C. 2 Reducing variance by trial-dependent repeated sampling 

As we saw in Section 4.4, a simple method to improve the estimate of IBS is to run the estimator multiple times and average the results. Repeated sampling will preserve the zero bias but reduce variance inversely proportional to the number of repeats $R$.

We can further improve the estimator by varying the number of repeats $R_{i}$ between trials, for $1 \leq i \leq N$, and define

$$
\hat{\mathcal{L}}_{\text {IBS } \cdot \boldsymbol{R}}=\sum_{i=1}^{N} \frac{1}{R_{i}} \sum_{r=1}^{R_{i}} \hat{\mathcal{L}}_{i}^{(r)}
$$

where $\boldsymbol{R}$ is a vector of positive integers with elements $R_{i}$, and $\hat{\mathcal{L}}_{i}^{(r)}$ denotes the outcome of the $r$-th run of IBS on trial $i$. This estimator is unbiased regardless of the choice of $\boldsymbol{R}$ (as long as $R_{i}>0$ for all trials), and we can analytically compute both its variance and expected number of samples (see Equation S14).

We can then ask what is the best allocation of repeats $R_{i}$ that minimizes the variance of the estimator in Equation S13 such that the expected total number of samples does not exceed a fixed budget $S$. This defines the following constrained optimization problem,

$$
\boldsymbol{R}^{*}=\arg \min _{R_{i}, R_{2} \ldots, R_{N}}\left\{\left.\frac{1}{N} \sum_{i=1}^{N} \frac{\operatorname{Li}_{2}\left(1-p_{i}\right)}{R_{i}} \right\rvert\, \sum_{i=1}^{N} \frac{R_{i}}{p_{i}} \leq S\right\}
$$

where we used Equation 15 for the variance of the IBS estimator.

Assuming that the $R_{i}$ take continuous values, we can solve the optimization problem in Equation S14 exactly using a Lagrange multiplier, and find the following closed-form

---

#### Page 93

expression for the optimal number of repeats per trial,

$$
R_{i}^{*}=S\left(\sum_{j=1}^{N} \sqrt{\frac{\overline{\mathrm{Li}_{2}\left(1-p_{j}\right)}}{p_{j}}}\right)^{-1} \sqrt{p_{i} \overline{\mathrm{Li}_{2}\left(1-p_{i}\right)}}
$$

According to Equation S15, the optimal choice of repeats entails dividing the budget $S$ across trials, where trial $i$ is allocated repeats proportional to $\sqrt{p_{i} \overline{\mathrm{Li}_{2}\left(1-p_{i}\right)}}$. We plot this function in Figure S10 and see that, to minimize variance, we should allocate resources primarily to trials where $p_{i}$ is close to $\frac{1}{2}$ and avoid trials where $p_{i} \approx 1$ (since the variance of IBS is already small for those trials) or where $p_{i} \approx 0$ (since those utilize a larger share of the budget).

We can also calculate exactly the fractional increase in precision (inverse variance) when using the optimal choice of repeats vector $\boldsymbol{R}^{*}$, compared to a constant $R$ which divides the budget equally across trials,

$$
\frac{\operatorname{Var}\left[\hat{\mathcal{L}}_{\text {IBS-R }}\right]}{\operatorname{Var}\left[\hat{\mathcal{L}}_{\text {IBS-R }}\right]}=\left(\sum_{i=1}^{N} \sqrt{\frac{\overline{\mathrm{Li}_{2}\left(1-p_{i}\right)}}{p_{i}}}\right)^{2} \times\left(\sum_{i=1}^{N} \mathrm{Li}_{2}\left(1-p_{i}\right)\right)^{-1} \times\left(\sum_{i=1}^{N} \frac{1}{p_{i}}\right)^{-1}
$$

This equation implies that the gain in precision from this method depends on the distribution of $p_{i}$ across trials. If $p_{i} \sim$ Uniform[0,1] and $N=500$, the median precision gain is 1.584 and the inter-quartile range (IQR) is 1.375 to 2.090 . Note that the gain is always greater than 1 , unless $p_{i}$ is constant across trials.

# Practical implementation of trial-dependent repeated sampling 

In practice, Equation S15 cannot be applied directly, as we have treated $R_{i}$ as continuous variables, but the number of times to repeat IBS on a given trial has to be an integer. Additionally, the method is only unbiased if $R_{i}$ is at least 1 for each trial $i$. Therefore,

---

#### Page 94

> **Image description.** The image is a line graph showing the relationship between two variables.
> 
> *   **Axes:** The graph has two axes. The horizontal axis is labeled "p" and ranges from 0 to 1 in increments of 0.2. The vertical axis is labeled "$\sqrt{p \mathrm{Li}_{2}(1-p)}$" and ranges from 0 to 0.6 in increments of 0.2.
> *   **Data:** A single blue line is plotted on the graph. It starts at (0,0), rises to a peak around (0.5, 0.55), and then descends back to (1,0). The line is smooth and symmetrical.
> *   **Overall Shape:** The line forms a curve that resembles an inverted parabola or a bell curve that has been cut off at the x-axis.

Figure S10: Graph of $\sqrt{p \mathrm{Li}_{2}(1-p)}$, which is proportional to the optimal number of repeats for a trial with likelihood $p$ (see equation S15). We observe that the optimal allocation of computational resources entails repeated sampling for trials with $p \approx \frac{1}{2}$ and to avoid $p \approx 0$ or $p \approx 1$.

we can convert $R_{i}^{*}$ to integers by rounding up to the nearest integer. This method will make our solution approximate, and reduce the gain in precision, but it is still better than uniform repeats for uniformly distributed $p_{i}$ (median: 1.567, IQR: 1.374-2.002).

The derivation above has another, more fundamental problem. Computing $R_{i}^{*}$ requires knowledge of $p_{i}$ on each trial, which we do not have. While we could try and learn the allocation of $\boldsymbol{R}^{*}$ as a function of $\boldsymbol{\theta}$ in some adaptive way, in practice we recommend the following simple scheme:

1. Choose a default parameter vector $\boldsymbol{\theta}_{0}$, and run IBS with a large number of repeats (e.g, $R=100$ ) to estimate the (log)-likelihood of the model on each trial.
2. Compute the optimal repeats $R_{i}^{*}$ given the estimated likelihoods $\hat{p}_{i}$ and a total budget of expected samples $S$ per likelihood evaluation, and round up.
3. Run IBS with those fixed repeats per trial on each iteration of the optimization algorithm.

---

#### Page 95

This approach implicitly assumes that the log-likelihood will be correlated across trials between the generative model with parameter vector $\boldsymbol{\theta}_{0}$ and any other vector $\boldsymbol{\theta}$ probed by the optimization algorithm. This is usually the case, since low-probability trials are often those where something unexpected occurred (e.g., the participant of a behavioral experiment lapsed or otherwise made an error). In our experience, this scheme considerably reduces the variance of IBS for a given computational time budget.

# C. 3 Bayesian inference with IBS 

While the main text focused on maximum-likelihood estimation, the unbiased loglikelihood estimates provided by IBS can also be used to perform Bayesian inference of posterior distributions over model parameters. We describe here a few possible approaches to approximate Bayesian inference with IBS.

## Markov Chain Monte Carlo

Markov Chain Monte Carlo (MCMC; see e.g. Brooks et al., 2011) is a powerful class of algorithms that allows one to sequentially sample from a target probability density which is known up to a normalization constant (e.g., the joint distribution). A popular form of MCMC is known as Metropolis-Hastings (MH; Hastings, 1970), which explores the target distribution by drawing a sample from a 'proposal distribution' centered on the last sample (e.g., a multi-variate Gaussian). MH 'accepts' or 'rejects' the new sample with an acceptance probability that depends on the value of the target density at the proposed and at the last point. In case of acceptance, the new point is added the sequence of samples; otherwise, the last sample is repeated in the sequence. Under

---

#### Page 96

some conditions, the MH algorithm produces a (correlated) sequence of samples that are equivalent to draws from the target density. Crucially, and somewhat surprisingly, the MH algorithm is still valid (that is, produces a valid sequence) if one performs the comparison with a noisy but unbiased estimate of the target density as opposed to using the exact density (Andrieu et al., 2009).

One problem here is that IBS provides an unbiased estimate of the log-likelihood (and thus of the log target density); not of the likelihood. However, since the IBS estimates of the log-likelihood are nearly-exactly normally distributed (see Section 4.3), the distribution of the likelihood is log-normal. Thus, we can apply what is known as a 'convexity correction' and compute a (nearly) unbiased estimate of the likelihood $\hat{\ell}(\boldsymbol{\theta})$ by calculating the expected value of a log-normal variable, that is

$$
\hat{\ell}(\boldsymbol{\theta})=\exp \left(\hat{\mathcal{L}}(\boldsymbol{\theta})+\frac{1}{2} \operatorname{Var}[\hat{\mathcal{L}}(\boldsymbol{\theta})]\right)
$$

Equation S17 can be easily evaluated with IBS, using the expression for the variance of the IBS estimator (Equation 16).

# Variational inference 

An alternative class of approximate inference methods is based on variational inference (VI; Jordan et al., 1999). The goal of VI is to approximate the intractable posterior distribution with a simpler distribution $q(\boldsymbol{\theta})$ belonging to a chosen parametric family. A common choice is a multivariate normal with diagonal covariance (known as mean field approximation); but other choices are possible too. VI selects the 'best' approximation $q(\cdot)$ that minimizes the Kullback-Leibler divergence with the true posterior, or

---

#### Page 97

equivalently maximizes the following variational objective,

$$
\mathcal{E}[q]=\mathbb{E}_{\boldsymbol{\theta} \sim q(\cdot)}[\mathcal{L}(\boldsymbol{\theta})]+\mathcal{H}[q]
$$

where $\mathcal{H}[q]$ is the entropy of $q(\cdot)$, which we assume can be computed analytically or numerically. Crucially, we can obtain an unbiased estimate of the first term in Equation S18 (the expected log joint) with IBS, as we have seen in Section 6.1. The optimization of the variational objective can then be performed directly with derivative-free optimization methods (such as BADS), or via a technique that produces unbiased estimates of the gradient combined with variance-reduction tricks, called black-box variational inference (Ranganath et al., 2014).

# Gaussian process surrogate methods 

One issue with the approximate inference methods described above is that they require a large (possibly, very large) number of likelihood evaluations to converge. Thus, these approaches are unfeasible if the generative model is somewhat computationally expensive, as it is often the case. An alternative family of methods designed to deal with expensive likelihoods builds a Gaussian process approximation (a surrogate) of the log joint distribution, and uses it to actively acquire further points in a smart way, similarly to the approach of Bayesian optimization (Kandasamy et al., 2015; Acerbi, 2018; JÃ¤rvenpÃ¤Ã¤ et al., 2019). However, unlike Bayesian optimization, the goal here is not to optimize the target function, but instead to build an accurate approximation of the posterior distribution, with as few likelihood evaluations as possible.

IBS is particularly suited to be used in combination with Gaussian process surrogate methods as it provides both an unbiased estimate of the log-likelihood, and a

---

#### Page 98

calibrated estimate of the uncertainty in each measurement, which can be used to inform the Gaussian process observation model. The development of Gaussian process surrogate methods is an active and very promising area of research. A recent example is Variational Bayesian Monte Carlo (VBMC; Acerbi, 2018, 2019), a technique that naturally combines Gaussian process surrogate modeling with variational inference thanks to Bayesian quadrature (O'Hagan, 1991). Conveniently, VBMC returns both an approximate posterior distribution and an estimate of the model evidence, which can be used for model comparison. Acerbi (2020) showed that VBMC, combined with IBS and modified to deal with noisy log-likelihood evaluations, performs very well on a variety of models from computational and cognitive neuroscience.